title,abstract,year,source,keyword,class_name,middle_group,subject_area,cluster,year_raw,clean_text,topic
Preface to the 13th Workshop on Bibliometric-enhanced Information Retrieval at ECIR 2023,"{""This preface summarizes the 13th Workshop on Bibliometric-enhanced Information Retrieval (BIR). BIR 2023 was held as hybrid event at April 2nd, 2023, co-located with the 45th European Conference on Information Retrieval (ECIR 2023).""}",2023,CEUR Workshop Proceedings,,Computer Science (all),,,CS,2023,"this preface summarizes the 13th workshop on bibliometric-enhanced information retrieval (bir). bir 2023 was held as hybrid event at april 2nd, 2023, co-located with the 45th european conference on information retrieval (ecir 2023).",2
"GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets","{""Named Entity Recognition (NER) models play a crucial role in various NLP tasks, including information extraction (IE) and text understanding. In academic writing, references to machine learning models and datasets are fundamental components of various computer science publications and necessitate accurate models for identification. Despite the advancements in NER, existing ground truth datasets do not treat fine-grained types like ML model and model architecture as separate entity types, and consequently, baseline models cannot recognize them as such. In this paper, we release a corpus of 100 manually annotated full-text scientific publications and a first baseline model for 10 entity types centered around ML models and datasets. In order to provide a nuanced understanding of how ML models and datasets are mentioned and utilized, our dataset also contains annotations for informal mentions like “our BERT-based model” or “an image CNN”. You can find the ground truth dataset and code to replicate model training at https://data.gesis.org/gsap/gsap-ner.""}",2023,Findings of the Association for Computational Linguistics: EMNLP 2023,,Computer Science Applications,Computer Science,Physical Sciences,CS,2023,"named entity recognition (ner) models play a crucial role in various nlp tasks, including information extraction (ie) and text understanding. in academic writing, references to machine learning models and datasets are fundamental components of various computer science publications and necessitate accurate models for identification. despite the advancements in ner, existing ground truth datasets do not treat fine-grained types like ml model and model architecture as separate entity types, and consequently, baseline models cannot recognize them as such. in this paper, we release a corpus of 100 manually annotated full-text scientific publications and a first baseline model for 10 entity types centered around ml models and datasets. in order to provide a nuanced understanding of how ml models and datasets are mentioned and utilized, our dataset also contains annotations for informal mentions like “our bert-based model” or “an image cnn”. you can find the ground truth dataset and code to replicate model training at https://data.gesis.org/gsap/gsap-ner.",6
Harmonizing and synthesizing partnership histories from different German survey infrastructures,"{""The research project \""Harmonizing and Synthesizing Partnership Histories from Different Research Data Infrastructures\"" (HaSpaD) set out to harmonize large German surveys to provide a unique data source for investigating transitions and dynamics in partnerships. Our work was guided by the methodological framework of an individual participant data meta-analysis. Aiming toward complete coverage of survey data that contain information on partnership biographies in Germany, we introduce our eligibility criteria and describe the data retrieval process. Furthermore, we illustrate two different levels at which data harmonization took place, i.e. harmonizing biography data and respondents' and couples' characteristics. An important part of our data harmonization efforts is to document the entire process to allow future researchers and projects to build on our work. To assess data quality, we compare administrative statistics on divorce risks with those estimated with the cumulated HaSpaD data. All in all, the survey data on Germany cumulated in the HaSpaD project produce quite similar trends in marital stability compared to the administrative statistics. The deviations that occur are mostly plausible given the different data generation mechanisms. As harmonizing different data sources is not without limitations and challenges, we discuss such issues and highlight methodological challenges, e.g. regarding weighting and systematic missingness, and provide some recommendations on how to address them. Since data harmonization relies heavily on various software tools, we introduce the tools that were employed in the HaSpaD project. A brief introduction on how to get started with the cumulated HaSpaD data set completes our chapter.""}",2023,Survey Data Harmonization in the Social Sciences,"{""ex post"",harmonization,meta-analysis,""missing data"",""partnership biographies"",""relationship stability"",""survey weighting""}",Mathematics (all),,,CS,2023,"the research project \""harmonizing and synthesizing partnership histories from different research data infrastructures\"" (haspad) set out to harmonize large german surveys to provide a unique data source for investigating transitions and dynamics in partnerships. our work was guided by the methodological framework of an individual participant data meta-analysis. aiming toward complete coverage of survey data that contain information on partnership biographies in germany, we introduce our eligibility criteria and describe the data retrieval process. furthermore, we illustrate two different levels at which data harmonization took place, i.e. harmonizing biography data and respondents' and couples' characteristics. an important part of our data harmonization efforts is to document the entire process to allow future researchers and projects to build on our work. to assess data quality, we compare administrative statistics on divorce risks with those estimated with the cumulated haspad data. all in all, the survey data on germany cumulated in the haspad project produce quite similar trends in marital stability compared to the administrative statistics. the deviations that occur are mostly plausible given the different data generation mechanisms. as harmonizing different data sources is not without limitations and challenges, we discuss such issues and highlight methodological challenges, e.g. regarding weighting and systematic missingness, and provide some recommendations on how to address them. since data harmonization relies heavily on various software tools, we introduce the tools that were employed in the haspad project. a brief introduction on how to get started with the cumulated haspad data set completes our chapter.",2
NFDI4DS Shared Tasks,"{""Shared tasks have proven to be successful in proposing innovative solutions for challenging research problems. The NFDI4DS consortium plans to host various shared tasks to tackle problems under the umbrella of scholarly information processing. We discuss three shared tasks in detail: Field of Research Classification, Software Mention Detection, and Tracking State-of-The-Art in Empirical AI. We also briefly mention other shared tasks planned to be released in the future.""}",2023,"Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)","{nfdi,nfdi4ds,""shared tasks""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2023,"shared tasks have proven to be successful in proposing innovative solutions for challenging research problems. the nfdi4ds consortium plans to host various shared tasks to tackle problems under the umbrella of scholarly information processing. we discuss three shared tasks in detail: field of research classification, software mention detection, and tracking state-of-the-art in empirical ai. we also briefly mention other shared tasks planned to be released in the future.",2
Efficient computation of comprehensive statistical information of large OWL datasets: a scalable approach,"{""Computing dataset statistics is crucial for exploring their structure, however, it becomes challenging for large-scale datasets. This has several key benefits, such as link target identification, vocabulary reuse, quality analysis, big data analytics, and coverage analysis. In this paper, we present the first attempt of developing a distributed approach (OWLStats) for collecting comprehensive statistics over large-scale OWL datasets. OWLStats is a distributed in-memory approach for computing 50 statistical criteria for OWL datasets utilizing Apache Spark. We have successfully integrated OWLStats into the SANSA framework. Experiments results prove that OWLStats is linearly scalable in terms of both node and data scalability.""}",2023,Enterprise Information Systems,"{""distributed processing"",""in-memory approach"",""sansa framework"",""scalable architecture"",""semantic web"",""statistics computations""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2023,"computing dataset statistics is crucial for exploring their structure, however, it becomes challenging for large-scale datasets. this has several key benefits, such as link target identification, vocabulary reuse, quality analysis, big data analytics, and coverage analysis. in this paper, we present the first attempt of developing a distributed approach (owlstats) for collecting comprehensive statistics over large-scale owl datasets. owlstats is a distributed in-memory approach for computing 50 statistical criteria for owl datasets utilizing apache spark. we have successfully integrated owlstats into the sansa framework. experiments results prove that owlstats is linearly scalable in terms of both node and data scalability.",3
SimE4KG: Distributed and Explainable Multi-Modal Semantic Similarity Estimation for Knowledge Graphs,"{""In recent years, exciting sources of data have been modeled as knowledge graphs (KGs). This modeling represents both structural relationships and the entity-specific multi-modal data in KGs. In various data analytics pipelines and machine learning (ML), the task of semantic similarity estimation plays a significant role. Assigning similarity values to entity pairs is needed in recommendation systems, clustering, classification, entity matching/disambiguation and many others. Efficient and scalable frameworks are needed to handle the quadratic complexity of all-pair semantic similarity on Big Data KGs. Moreover, heterogeneous KGs demand multi-modal semantic similarity estimation to cover the versatile contents like categorical relations between classes or their attribute literals like strings, timestamps or numeric data. In this paper, we propose the SimE4KG framework as a resource providing generic open-source modules that perform semantic similarity estimation in multi-modal KGs. To justify the computational costs of similarity estimation, the SimE4KG generates reproducible, reusable and explainable results. The pipeline results are a native semantic RDF KG, including the experiment results, hyper-parameter setup and explanation of the results, like the most influential features. For fast and scalable execution in memory, we implemented the distributed approach using Apache Spark. The entire development of this framework is integrated into the holistic distributed Semantic ANalytics StAck (SANSA).""}",2023,International Journal of Semantic Computing,"{""apache spark"",""distributed computing"",""explainable artificial intelligence"",""knowledge graphs"",""machine learning"",rdf,""scalable semantic processing"",""semantic similarity""}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2023,"in recent years, exciting sources of data have been modeled as knowledge graphs (kgs). this modeling represents both structural relationships and the entity-specific multi-modal data in kgs. in various data analytics pipelines and machine learning (ml), the task of semantic similarity estimation plays a significant role. assigning similarity values to entity pairs is needed in recommendation systems, clustering, classification, entity matching/disambiguation and many others. efficient and scalable frameworks are needed to handle the quadratic complexity of all-pair semantic similarity on big data kgs. moreover, heterogeneous kgs demand multi-modal semantic similarity estimation to cover the versatile contents like categorical relations between classes or their attribute literals like strings, timestamps or numeric data. in this paper, we propose the sime4kg framework as a resource providing generic open-source modules that perform semantic similarity estimation in multi-modal kgs. to justify the computational costs of similarity estimation, the sime4kg generates reproducible, reusable and explainable results. the pipeline results are a native semantic rdf kg, including the experiment results, hyper-parameter setup and explanation of the results, like the most influential features. for fast and scalable execution in memory, we implemented the distributed approach using apache spark. the entire development of this framework is integrated into the holistic distributed semantic analytics stack (sansa).",3
Generating effective label description for label-aware sentiment classification,"{""Sentiment classification aims to predict the sentiment label for a given text. Recently, several research efforts have been devoted to incorporate matching clues between text words and class labels into the learning process of text representation. However, these methods heavily rely on the availability of label content. Moreover, they simply capture the label-specific signals to measure each word's contribution by either implicitly employing a learnable label representation or explicitly leveraging the interaction between text words and labels via the interaction mechanism. To deal with these issues, in this paper, we propose a novel framework called Label-Guided Dual-view Sentiment Classifier (LGDSC). We first introduce a new strategy for generating an effective label description and then design a novel Dual-Channel Label-guided Attention Network (DLAN) to learn a text representation via two different channels. DLAN will be further leveraged to learn label-guided text representations from two different views. Extensive experimental results on four real-world datasets demonstrate that LGDSC consistently outperforms the state-of-the-art baseline methods.""}",2023,Expert Systems with Applications,"{""attention network"",""sentiment analysis"",""sentiment classification"",""text summarization""}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2023,"sentiment classification aims to predict the sentiment label for a given text. recently, several research efforts have been devoted to incorporate matching clues between text words and class labels into the learning process of text representation. however, these methods heavily rely on the availability of label content. moreover, they simply capture the label-specific signals to measure each word's contribution by either implicitly employing a learnable label representation or explicitly leveraging the interaction between text words and labels via the interaction mechanism. to deal with these issues, in this paper, we propose a novel framework called label-guided dual-view sentiment classifier (lgdsc). we first introduce a new strategy for generating an effective label description and then design a novel dual-channel label-guided attention network (dlan) to learn a text representation via two different channels. dlan will be further leveraged to learn label-guided text representations from two different views. extensive experimental results on four real-world datasets demonstrate that lgdsc consistently outperforms the state-of-the-art baseline methods.",4
Broadening BERT vocabulary for Knowledge Graph Construction using Wikipedia2Vec,"{""Recent advancements in natural language processing (NLP) have been driven by the utilization of large language models like BERT. These models, pre-trained on extensive textual data, capture linguistic and relational knowledge. Therefore, cloze-style prompts, which involve filling in missing words in a sentence, can be used to solve knowledge-intensive NLP tasks with the help of a language model. The \""Knowledge Base Construction from Pre-trained Language Models (LM-KBC 2023)\"" challenge aims to harness language models’ potential for knowledge graph construction through prompts. In particular, contestants are challenged to infer the correct Wikidata ID of objects, given a prompt used to link subject, relation, and object. Automatically inferring the correct objects would help in reducing the need for an expensive manual graph population. Our proposed approach in Track 1 focuses on expanding BERT’s vocabulary with a task-specific one (i.e., Wikipedia2Vec) and facilitating its usage through prompt tuning with OPTIPROMPT.""}",2023,CEUR Workshop Proceedings,,Computer Science (all),,,CS,2023,"recent advancements in natural language processing (nlp) have been driven by the utilization of large language models like bert. these models, pre-trained on extensive textual data, capture linguistic and relational knowledge. therefore, cloze-style prompts, which involve filling in missing words in a sentence, can be used to solve knowledge-intensive nlp tasks with the help of a language model. the \""knowledge base construction from pre-trained language models (lm-kbc 2023)\"" challenge aims to harness language models’ potential for knowledge graph construction through prompts. in particular, contestants are challenged to infer the correct wikidata id of objects, given a prompt used to link subject, relation, and object. automatically inferring the correct objects would help in reducing the need for an expensive manual graph population. our proposed approach in track 1 focuses on expanding bert’s vocabulary with a task-specific one (i.e., wikipedia2vec) and facilitating its usage through prompt tuning with optiprompt.",0
SimE4KG: Distributed and Explainable Multi-Modal Semantic Similarity Estimation for Knowledge Graphs,"{""In recent years, exciting sources of data have been modeled as knowledge graphs (KGs). This modeling represents both structural relationships and the entity-specific multi-modal data in KGs. In various data analytics pipelines and machine learning (ML), the task of semantic similarity estimation plays a significant role. Assigning similarity values to entity pairs is needed in recommendation systems, clustering, classification, entity matching/disambiguation and many others. Efficient and scalable frameworks are needed to handle the quadratic complexity of all-pair semantic similarity on Big Data KGs. Moreover, heterogeneous KGs demand multi-modal semantic similarity estimation to cover the versatile contents like categorical relations between classes or their attribute literals like strings, timestamps or numeric data. In this paper, we propose the SimE4KG framework as a resource providing generic open-source modules that perform semantic similarity estimation in multi-modal KGs. To justify the computational costs of similarity estimation, the SimE4KG generates reproducible, reusable and explainable results. The pipeline results are a native semantic RDF KG, including the experiment results, hyper-parameter setup and explanation of the results, like the most influential features. For fast and scalable execution in memory, we implemented the distributed approach using Apache Spark. The entire development of this framework is integrated into the holistic distributed Semantic ANalytics StAck (SANSA).""}",2023,International Journal of Semantic Computing,"{""apache spark"",""distributed computing"",""explainable artificial intelligence"",""knowledge graphs"",""machine learning"",rdf,""scalable semantic processing"",""semantic similarity""}",Software,Computer Science,Physical Sciences,CS,2023,"in recent years, exciting sources of data have been modeled as knowledge graphs (kgs). this modeling represents both structural relationships and the entity-specific multi-modal data in kgs. in various data analytics pipelines and machine learning (ml), the task of semantic similarity estimation plays a significant role. assigning similarity values to entity pairs is needed in recommendation systems, clustering, classification, entity matching/disambiguation and many others. efficient and scalable frameworks are needed to handle the quadratic complexity of all-pair semantic similarity on big data kgs. moreover, heterogeneous kgs demand multi-modal semantic similarity estimation to cover the versatile contents like categorical relations between classes or their attribute literals like strings, timestamps or numeric data. in this paper, we propose the sime4kg framework as a resource providing generic open-source modules that perform semantic similarity estimation in multi-modal kgs. to justify the computational costs of similarity estimation, the sime4kg generates reproducible, reusable and explainable results. the pipeline results are a native semantic rdf kg, including the experiment results, hyper-parameter setup and explanation of the results, like the most influential features. for fast and scalable execution in memory, we implemented the distributed approach using apache spark. the entire development of this framework is integrated into the holistic distributed semantic analytics stack (sansa).",3
Exploring rich structure information for aspect-based sentiment classification,"{""Graph Convolutional Network (GCN) for aspect-based sentiment classification has attracted a lot of attention recently due to their promising performance in handling complex structure information. However, previous methods based on GCN focused mainly on examining the structure of syntactic dependency relationships, which were subject to the noise and sparsity problem. Furthermore, these methods tend to focus on one kind of structural information (namely syntactic dependency) while ignoring many other kinds of rich structures between words. To tackle these problems, we propose a novel GCN based model, named Structure-Enhanced Dual-Channel Graph Convolutional Network (SEDC-GCN). Specifically, we first exploit the rich structure information by constructing a text sequence graph and an enhanced dependency graph, then design a dual-channel graph encoder to model the structure information from the two graphs. After that, we propose two kinds of aspect-specific attention, i.e., aspect-specific semantic attention and aspect-specific structure attention, to learn sentence representation from two different perspectives, i.e., the semantic perspective based on the text encoder, and the structure perspective based on the dual-channel graph encoder. Finally, we merge the sentence representations from the above two perspectives and obtain the final sentence representation. We experimentally validate our proposed model SEDC-GCN by comparing with seven strong baseline methods. In terms of the metric accuracy, SEDC-GCN achieves performance gains of 74.42%, 77.74%, 83.30%, 81.73% and 90.75% on TWITTER, LAPTOP, REST14, REST15, and REST16, respectively, which are 0.35%, 4.22%, 1.62%, 0.70% and 2.01% better than the best performing baseline BiGCN. Similar performance improvements are also observed in terms of the metric macro-averaged F1 score. The ablation study further demonstrates the effectiveness of each component of SEDC-GCN.""}",2023,Journal of Intelligent Information Systems,"{""aspect-based sentiment classification"",""attention mechanism"",""graph convolutional networks"",""sentiment analysis""}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2023,"graph convolutional network (gcn) for aspect-based sentiment classification has attracted a lot of attention recently due to their promising performance in handling complex structure information. however, previous methods based on gcn focused mainly on examining the structure of syntactic dependency relationships, which were subject to the noise and sparsity problem. furthermore, these methods tend to focus on one kind of structural information (namely syntactic dependency) while ignoring many other kinds of rich structures between words. to tackle these problems, we propose a novel gcn based model, named structure-enhanced dual-channel graph convolutional network (sedc-gcn). specifically, we first exploit the rich structure information by constructing a text sequence graph and an enhanced dependency graph, then design a dual-channel graph encoder to model the structure information from the two graphs. after that, we propose two kinds of aspect-specific attention, i.e., aspect-specific semantic attention and aspect-specific structure attention, to learn sentence representation from two different perspectives, i.e., the semantic perspective based on the text encoder, and the structure perspective based on the dual-channel graph encoder. finally, we merge the sentence representations from the above two perspectives and obtain the final sentence representation. we experimentally validate our proposed model sedc-gcn by comparing with seven strong baseline methods. in terms of the metric accuracy, sedc-gcn achieves performance gains of 74.42%, 77.74%, 83.30%, 81.73% and 90.75% on twitter, laptop, rest14, rest15, and rest16, respectively, which are 0.35%, 4.22%, 1.62%, 0.70% and 2.01% better than the best performing baseline bigcn. similar performance improvements are also observed in terms of the metric macro-averaged f1 score. the ablation study further demonstrates the effectiveness of each component of sedc-gcn.",4
Decoding Prompt Syntax: Analysing its Impact on Knowledge Retrieval in Large Language Models,"{""Large Language Models (LLMs), with their advanced architectures and training on massive language datasets, contain unexplored knowledge. One method to infer this knowledge is through the use of cloze-style prompts. Typically, these prompts are manually designed because the phrasing of these prompts impacts the knowledge retrieval performance, even if the LLM encodes the desired information. In this paper, we study the impact of prompt syntax on the knowledge retrieval capacity of LLMs. We use a template-based approach to paraphrase simple prompts into prompts with a more complex grammatical structure. We then analyse the LLM performance for these structurally different but semantically equivalent prompts. Our study reveals that simple prompts work better than complex forms of sentences. The performance across the syntactical variations for simple relations (1:1) remains best, with a marginal decrease across different typologies. These results reinforce that simple prompt structures are more effective for knowledge retrieval in LLMs and motivate future research into the impact of prompt syntax on various tasks.""}",2023,"ACM Web Conference 2023 - Companion of the World Wide Web Conference, WWW 2023","{bert,""knowledge retrieval"",""large language models"",""syntax aware prompt""}",Computer Networks and Communications,Computer Science,Physical Sciences,CS,2023,"large language models (llms), with their advanced architectures and training on massive language datasets, contain unexplored knowledge. one method to infer this knowledge is through the use of cloze-style prompts. typically, these prompts are manually designed because the phrasing of these prompts impacts the knowledge retrieval performance, even if the llm encodes the desired information. in this paper, we study the impact of prompt syntax on the knowledge retrieval capacity of llms. we use a template-based approach to paraphrase simple prompts into prompts with a more complex grammatical structure. we then analyse the llm performance for these structurally different but semantically equivalent prompts. our study reveals that simple prompts work better than complex forms of sentences. the performance across the syntactical variations for simple relations (1:1) remains best, with a marginal decrease across different typologies. these results reinforce that simple prompt structures are more effective for knowledge retrieval in llms and motivate future research into the impact of prompt syntax on various tasks.",0
Dynamic global structure enhanced multi-channel graph neural network for session-based recommendation,"{""Session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. Most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. Meanwhile, previous works usually apply GNN to capture the transformation relationship between items, however the graph used in GNN is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. In this paper, we propose a novel method called Dynamic Global Structure Enhanced Multi-channel Graph Neural Network (DGS-MGNN) to learn accurate representations of items from multiple perspectives. In DGS-MGNN, we propose a novel GNN model named Multi-channel Graph Neural Network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. Meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. Finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. We conduct extensive experiments on three widely used datasets, and the results demonstrate that DGS-MGNN is consistently superior to the state-of-the-art baseline models.""}",2023,Information Sciences,"{""attention model"",""behavior modeling"",""graph neural network"",""recommendation system"",""representation learning"",""session-based recommendation""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2023,"session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. meanwhile, previous works usually apply gnn to capture the transformation relationship between items, however the graph used in gnn is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. in this paper, we propose a novel method called dynamic global structure enhanced multi-channel graph neural network (dgs-mgnn) to learn accurate representations of items from multiple perspectives. in dgs-mgnn, we propose a novel gnn model named multi-channel graph neural network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. we conduct extensive experiments on three widely used datasets, and the results demonstrate that dgs-mgnn is consistently superior to the state-of-the-art baseline models.",4
GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding,"{""Language models can serve as a valuable tool for software developers to increase productivity. Large generative models can be used for code generation and code completion, while smaller encoder-only models are capable of performing code search tasks using natural language queries. These capabilities are heavily influenced by the quality and diversity of the available training data. Source code datasets used for training usually focus on the most popular languages and testing is mostly conducted on the same distributions, often overlooking low-resource programming languages. Motivated by the NLP generalization taxonomy proposed by Hupkes et. al., we propose a new benchmark dataset called GenCodeSearchNet (GeCS) which builds upon existing natural language code search datasets to systemically evaluate the programming language understanding generalization capabilities of language models. As part of the full dataset, we introduce a new, manually curated subset StatCodeSearch that focuses on R, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science. For evaluation and comparison, we collect several baseline results using fine-tuned BERT-style models and GPT-style large language models in a zero-shot setting.""}",2023,"GenBench 2023 - GenBench: 1st Workshop on Generalisation (Benchmarking) in NLP, Proceedings",,Computer Science Applications,Computer Science,Physical Sciences,CS,2023,"language models can serve as a valuable tool for software developers to increase productivity. large generative models can be used for code generation and code completion, while smaller encoder-only models are capable of performing code search tasks using natural language queries. these capabilities are heavily influenced by the quality and diversity of the available training data. source code datasets used for training usually focus on the most popular languages and testing is mostly conducted on the same distributions, often overlooking low-resource programming languages. motivated by the nlp generalization taxonomy proposed by hupkes et. al., we propose a new benchmark dataset called gencodesearchnet (gecs) which builds upon existing natural language code search datasets to systemically evaluate the programming language understanding generalization capabilities of language models. as part of the full dataset, we introduce a new, manually curated subset statcodesearch that focuses on r, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science. for evaluation and comparison, we collect several baseline results using fine-tuned bert-style models and gpt-style large language models in a zero-shot setting.",0
EvoRecipes: A Generative Approach for Evolving Context-Aware Recipes,"{""Generative AI e.g. Large Language Models (LLMs) can be used to generate new recipes. However, LLMs struggle with more complex aspects like recipe semantics and process comprehension. Furthermore, LLMs have limited ability to account for user preferences since they are based on statistical patterns. As a result, these recipes may be invalid. Evolutionary algorithms inspired by the process of natural selection are optimization algorithms that use stochastic operators to generate new solutions. These algorithms can generate large number of solutions from the set of possible solution space. Moreover, these algorithms have the capability to incorporate user preferences in fitness function to generate novel recipes that are more aligned with the fitness objective. In this paper, we propose the EvoRecipes framework to generate novel recipes. The EvoRecipes framework utilizes both Genetic Algorithm and generative AI in addition to RecipeOn ontology, and RecipeKG knowledge graph. Genetic Algorithm explore the large solution space of encoded recipe solutions and are capable of incorporating user preferences, while LLMs are used to generate recipe text from encoded recipe solutions. EvoRecipes uses a population of context-aware recipe solutions from the RecipeKG knowledge graph. RecipeKG encodes recipes in RDF format using classes and properties as defined in the RecipeOn ontology. Moreover, to evaluate the alignment of EvoRecipe generated recipes with multiple intended objectives, we propose a fitness function that incorporates novelty, simplicity, visual appeal, and feasibility. Additionally, to evaluate the quality of the EvoRecipe generated recipes while considering the subjective nature of recipes, we conducted a survey using multi-dimensional metrics (i.e. contextual, procedural, and novelty). Results show that EvoRecipes generated recipes are novel, valid and incorporate user preferences.""}",2023,IEEE Access,"{""computational creativity"",food,""knowledge graph"",ontology,recipe,""recipe evolution""}",Engineering (all),,,CS,2023,"generative ai e.g. large language models (llms) can be used to generate new recipes. however, llms struggle with more complex aspects like recipe semantics and process comprehension. furthermore, llms have limited ability to account for user preferences since they are based on statistical patterns. as a result, these recipes may be invalid. evolutionary algorithms inspired by the process of natural selection are optimization algorithms that use stochastic operators to generate new solutions. these algorithms can generate large number of solutions from the set of possible solution space. moreover, these algorithms have the capability to incorporate user preferences in fitness function to generate novel recipes that are more aligned with the fitness objective. in this paper, we propose the evorecipes framework to generate novel recipes. the evorecipes framework utilizes both genetic algorithm and generative ai in addition to recipeon ontology, and recipekg knowledge graph. genetic algorithm explore the large solution space of encoded recipe solutions and are capable of incorporating user preferences, while llms are used to generate recipe text from encoded recipe solutions. evorecipes uses a population of context-aware recipe solutions from the recipekg knowledge graph. recipekg encodes recipes in rdf format using classes and properties as defined in the recipeon ontology. moreover, to evaluate the alignment of evorecipe generated recipes with multiple intended objectives, we propose a fitness function that incorporates novelty, simplicity, visual appeal, and feasibility. additionally, to evaluate the quality of the evorecipe generated recipes while considering the subjective nature of recipes, we conducted a survey using multi-dimensional metrics (i.e. contextual, procedural, and novelty). results show that evorecipes generated recipes are novel, valid and incorporate user preferences.",0
ExPAD: An Explainable Distributed Automatic Anomaly Detection Framework over Large KGs,"{""RDF data is playing an important role in publishing and integrating heterogeneous data in data lakes. However, since the data is generally created utilizing liberal curation approaches such as crowd-sourcing and automatic extraction tools with no cross-validation on input data, the data is prone to errors that can be hidden in several dimensions. The types of errors which can be considered as outliers may occur in any part of RDF statements, especially in literal objects. Although some scientific studies have revealed anomalies in knowledge graphs, none of the current approaches has the ability to explain the anomalies that have been discovered. In this paper, we present ExPAD, a scalable and distributed framework for explainable numeric anomaly detection on very large RDF knowledge graphs. Inspired by OutlierTree, ExPAD generates human-readable explanations for a given numeric result being an outlier by following and assessing supervised decision tree splits. The proposed framework ExPAD is open-source, well-documented, and fully integrated into the Semantic Analytics Stack (SANSA). Experiments on real-world use cases and synthetic datasets disclose that the framework can not only handle high volumes of RDF data but also efficiently generate explanations for hidden anomalies discovered in KGs.""}",2023,"Proceedings - 17th IEEE International Conference on Semantic Computing, ICSC 2023","{""big data"",""decision tree"",""distributed computing"",""explainable anomaly detection"",rdf,sansa}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2023,"rdf data is playing an important role in publishing and integrating heterogeneous data in data lakes. however, since the data is generally created utilizing liberal curation approaches such as crowd-sourcing and automatic extraction tools with no cross-validation on input data, the data is prone to errors that can be hidden in several dimensions. the types of errors which can be considered as outliers may occur in any part of rdf statements, especially in literal objects. although some scientific studies have revealed anomalies in knowledge graphs, none of the current approaches has the ability to explain the anomalies that have been discovered. in this paper, we present expad, a scalable and distributed framework for explainable numeric anomaly detection on very large rdf knowledge graphs. inspired by outliertree, expad generates human-readable explanations for a given numeric result being an outlier by following and assessing supervised decision tree splits. the proposed framework expad is open-source, well-documented, and fully integrated into the semantic analytics stack (sansa). experiments on real-world use cases and synthetic datasets disclose that the framework can not only handle high volumes of rdf data but also efficiently generate explanations for hidden anomalies discovered in kgs.",2
GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding,"{""Language models can serve as a valuable tool for software developers to increase productivity. Large generative models can be used for code generation and code completion, while smaller encoder-only models are capable of performing code search tasks using natural language queries. These capabilities are heavily influenced by the quality and diversity of the available training data. Source code datasets used for training usually focus on the most popular languages and testing is mostly conducted on the same distributions, often overlooking low-resource programming languages. Motivated by the NLP generalization taxonomy proposed by Hupkes et. al., we propose a new benchmark dataset called GenCodeSearchNet (GeCS) which builds upon existing natural language code search datasets to systemically evaluate the programming language understanding generalization capabilities of language models. As part of the full dataset, we introduce a new, manually curated subset StatCodeSearch that focuses on R, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science. For evaluation and comparison, we collect several baseline results using fine-tuned BERT-style models and GPT-style large language models in a zero-shot setting.""}",2023,"GenBench 2023 - GenBench: 1st Workshop on Generalisation (Benchmarking) in NLP, Proceedings",,Computational Theory and Mathematics,Computer Science,Physical Sciences,CS,2023,"language models can serve as a valuable tool for software developers to increase productivity. large generative models can be used for code generation and code completion, while smaller encoder-only models are capable of performing code search tasks using natural language queries. these capabilities are heavily influenced by the quality and diversity of the available training data. source code datasets used for training usually focus on the most popular languages and testing is mostly conducted on the same distributions, often overlooking low-resource programming languages. motivated by the nlp generalization taxonomy proposed by hupkes et. al., we propose a new benchmark dataset called gencodesearchnet (gecs) which builds upon existing natural language code search datasets to systemically evaluate the programming language understanding generalization capabilities of language models. as part of the full dataset, we introduce a new, manually curated subset statcodesearch that focuses on r, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science. for evaluation and comparison, we collect several baseline results using fine-tuned bert-style models and gpt-style large language models in a zero-shot setting.",0
Anomaly Detection for Numerical Literals in Knowledge Graphs: A Short Review of Approaches,"{""Anomaly Detection is an important problem that has been well-studied within diverse research areas and application domains. However, within the field of Semantic Web and Knowledge Graphs, anomaly detection has been relatively overlooked. Additionally, the existing literature on anomaly detection over Knowledge Graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. In light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over Knowledge Graphs. In this overview, we review the quality metrics of KGs and discuss the possible errors which may occur in different parts of the RDF data. Additionally, we outline a generic conceptual framework for the execution pipeline of Anomaly Detection over KGs. Moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. Finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for KGs.""}",2023,"Proceedings - 2023 IEEE 6th International Conference on Artificial Intelligence and Knowledge Engineering, AIKE 2023","{""anomaly detection"",""knowledge graphs"",""linked open data"",""outlier detection"",""rdf data"",""semantic web""}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2023,"anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. however, within the field of semantic web and knowledge graphs, anomaly detection has been relatively overlooked. additionally, the existing literature on anomaly detection over knowledge graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. in light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over knowledge graphs. in this overview, we review the quality metrics of kgs and discuss the possible errors which may occur in different parts of the rdf data. additionally, we outline a generic conceptual framework for the execution pipeline of anomaly detection over kgs. moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for kgs.",5
Shall androids dream of genocides? How generative AI can change the future of memorialization of mass atrocities,"{""The memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. Digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. At the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. The emergence of generative forms of artificial intelligence (AI), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. AI can identify patterns in training data to create new narratives for representing and interpreting mass atrocities—and do so in a fraction of the time it takes for humans. The use of generative AI in this context raises numerous questions: For example, can the paucity of training data on mass atrocities distort how AI interprets some atrocity-related inquiries? How important is the ability to differentiate between human- and AI-made content concerning mass atrocities? Can AI-made content be used to promote false information concerning atrocities? This article addresses these and other questions by examining the opportunities and risks associated with using generative AIs for memorializing mass atrocities. It also discusses recommendations for AIs integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.""}",2023,Discover Artificial Intelligence,,Human-Computer Interaction,Computer Science,Physical Sciences,CS,2023,"the memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. at the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. the emergence of generative forms of artificial intelligence (ai), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. ai can identify patterns in training data to create new narratives for representing and interpreting mass atrocities—and do so in a fraction of the time it takes for humans. the use of generative ai in this context raises numerous questions: for example, can the paucity of training data on mass atrocities distort how ai interprets some atrocity-related inquiries? how important is the ability to differentiate between human- and ai-made content concerning mass atrocities? can ai-made content be used to promote false information concerning atrocities? this article addresses these and other questions by examining the opportunities and risks associated with using generative ais for memorializing mass atrocities. it also discusses recommendations for ais integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.",1
EvoRecipes: A Generative Approach for Evolving Context-Aware Recipes,"{""Generative AI e.g. Large Language Models (LLMs) can be used to generate new recipes. However, LLMs struggle with more complex aspects like recipe semantics and process comprehension. Furthermore, LLMs have limited ability to account for user preferences since they are based on statistical patterns. As a result, these recipes may be invalid. Evolutionary algorithms inspired by the process of natural selection are optimization algorithms that use stochastic operators to generate new solutions. These algorithms can generate large number of solutions from the set of possible solution space. Moreover, these algorithms have the capability to incorporate user preferences in fitness function to generate novel recipes that are more aligned with the fitness objective. In this paper, we propose the EvoRecipes framework to generate novel recipes. The EvoRecipes framework utilizes both Genetic Algorithm and generative AI in addition to RecipeOn ontology, and RecipeKG knowledge graph. Genetic Algorithm explore the large solution space of encoded recipe solutions and are capable of incorporating user preferences, while LLMs are used to generate recipe text from encoded recipe solutions. EvoRecipes uses a population of context-aware recipe solutions from the RecipeKG knowledge graph. RecipeKG encodes recipes in RDF format using classes and properties as defined in the RecipeOn ontology. Moreover, to evaluate the alignment of EvoRecipe generated recipes with multiple intended objectives, we propose a fitness function that incorporates novelty, simplicity, visual appeal, and feasibility. Additionally, to evaluate the quality of the EvoRecipe generated recipes while considering the subjective nature of recipes, we conducted a survey using multi-dimensional metrics (i.e. contextual, procedural, and novelty). Results show that EvoRecipes generated recipes are novel, valid and incorporate user preferences.""}",2023,IEEE Access,"{""computational creativity"",food,""knowledge graph"",ontology,recipe,""recipe evolution""}",Computer Science (all),,,CS,2023,"generative ai e.g. large language models (llms) can be used to generate new recipes. however, llms struggle with more complex aspects like recipe semantics and process comprehension. furthermore, llms have limited ability to account for user preferences since they are based on statistical patterns. as a result, these recipes may be invalid. evolutionary algorithms inspired by the process of natural selection are optimization algorithms that use stochastic operators to generate new solutions. these algorithms can generate large number of solutions from the set of possible solution space. moreover, these algorithms have the capability to incorporate user preferences in fitness function to generate novel recipes that are more aligned with the fitness objective. in this paper, we propose the evorecipes framework to generate novel recipes. the evorecipes framework utilizes both genetic algorithm and generative ai in addition to recipeon ontology, and recipekg knowledge graph. genetic algorithm explore the large solution space of encoded recipe solutions and are capable of incorporating user preferences, while llms are used to generate recipe text from encoded recipe solutions. evorecipes uses a population of context-aware recipe solutions from the recipekg knowledge graph. recipekg encodes recipes in rdf format using classes and properties as defined in the recipeon ontology. moreover, to evaluate the alignment of evorecipe generated recipes with multiple intended objectives, we propose a fitness function that incorporates novelty, simplicity, visual appeal, and feasibility. additionally, to evaluate the quality of the evorecipe generated recipes while considering the subjective nature of recipes, we conducted a survey using multi-dimensional metrics (i.e. contextual, procedural, and novelty). results show that evorecipes generated recipes are novel, valid and incorporate user preferences.",0
A General and NLP-based Architecture to perform Recommendation: A Use Case for Online Job Search and Skills Acquisition,"{""Natural Language Processing (NLP) is crucial to perform recommendations of items that can be only described by natural language. However, NLP usage within recommendation modules is difficult and usually requires a relevant initial effort, thus limiting its widespread adoption. To overcome this limitation, we introduce FORESEE, a novel architecture that can be instantiated with NLP and Machine Learning (ML) modules to perform recommendations of items that are described by natural language features. Furthermore, we describe an instantiation of such architecture to provide a service for the job market where applicants can verify whether their curriculum vitae (CV) is eligible for a given job position, can receive suggestions about which skills and abilities they should obtain, and finally, can obtain recommendations about online resources which might strengthen their CVs.""}",2023,Proceedings of the ACM Symposium on Applied Computing,"{e-recruitment,""natural language processing"",recommendation,transformers}",Software,Computer Science,Physical Sciences,CS,2023,"natural language processing (nlp) is crucial to perform recommendations of items that can be only described by natural language. however, nlp usage within recommendation modules is difficult and usually requires a relevant initial effort, thus limiting its widespread adoption. to overcome this limitation, we introduce foresee, a novel architecture that can be instantiated with nlp and machine learning (ml) modules to perform recommendations of items that are described by natural language features. furthermore, we describe an instantiation of such architecture to provide a service for the job market where applicants can verify whether their curriculum vitae (cv) is eligible for a given job position, can receive suggestions about which skills and abilities they should obtain, and finally, can obtain recommendations about online resources which might strengthen their cvs.",0
Anomaly Detection for Numerical Literals in Knowledge Graphs: A Short Review of Approaches,"{""Anomaly Detection is an important problem that has been well-studied within diverse research areas and application domains. However, within the field of Semantic Web and Knowledge Graphs, anomaly detection has been relatively overlooked. Additionally, the existing literature on anomaly detection over Knowledge Graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. In light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over Knowledge Graphs. In this overview, we review the quality metrics of KGs and discuss the possible errors which may occur in different parts of the RDF data. Additionally, we outline a generic conceptual framework for the execution pipeline of Anomaly Detection over KGs. Moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. Finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for KGs.""}",2023,"Proceedings - 2023 IEEE 6th International Conference on Artificial Intelligence and Knowledge Engineering, AIKE 2023","{""anomaly detection"",""knowledge graphs"",""linked open data"",""outlier detection"",""rdf data"",""semantic web""}","Statistics, Probability and Uncertainty",Decision Sciences,Social Sciences & Humanities,CS,2023,"anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. however, within the field of semantic web and knowledge graphs, anomaly detection has been relatively overlooked. additionally, the existing literature on anomaly detection over knowledge graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. in light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over knowledge graphs. in this overview, we review the quality metrics of kgs and discuss the possible errors which may occur in different parts of the rdf data. additionally, we outline a generic conceptual framework for the execution pipeline of anomaly detection over kgs. moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for kgs.",5
Perceptions on and attitudes towards lifelong learning in the educational system,"{""Lifelong learning (LLL) is of international concern and schools are an important, however, a neglected context to foster LLL. The aim of the thematic analysis was to explore how teachers define LLL, how they perceive their own role in fostering LLL, which strategies they use to foster LLL competencies in their students, and if and where they can learn about LLL themselves. The sample consisted of 15 teachers and headmasters. Even though teachers already apply a lot of strategies to foster LLL competencies in their students, the results indicate the need to increase awareness of the influence teachers have.""}",2023,Social Sciences and Humanities Open,"{""lifelong learning"",""self-directed learning"",""teacher education"",""teacher role"",""thematic analysis""}",Decision Sciences (miscellaneous),Decision Sciences,Social Sciences & Humanities,CS,2023,"lifelong learning (lll) is of international concern and schools are an important, however, a neglected context to foster lll. the aim of the thematic analysis was to explore how teachers define lll, how they perceive their own role in fostering lll, which strategies they use to foster lll competencies in their students, and if and where they can learn about lll themselves. the sample consisted of 15 teachers and headmasters. even though teachers already apply a lot of strategies to foster lll competencies in their students, the results indicate the need to increase awareness of the influence teachers have.",0
Subject Classification of Software Repository,"{""Software categorization involves organizing software into groups based on their behavior or domain. Traditionally, categorization has been crucial for software maintenance, aiding programmers in locating programs, identifying features, and finding similar ones within extensive code repositories. Manual categorization is expensive, tedious, and labor-intensive, leading to the growing importance of automatic categorization approaches. However, existing datasets primarily focus on technical categorization for the most common programming language, leaving a gap in other areas. This paper addresses the research problem of classifying software repositories that contain R code. The objective is to develop a classification model capable of accurately and efficiently categorizing these repositories into predefined classes with less data. The contribution of this research is twofold. Firstly, we propose a model that enables the categorization of software repositories focusing on R programming, even with a limited amount of training data. Secondly, we conduct a comprehensive empirical evaluation to assess the impact of repository features and data augmentation on automatic repository categorization. This research endeavors to advance the field of software categorization and facilitate better utilization of software repositories in the context of diverse domains research.""}",2023,"International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K - Proceedings","{""data augmentation"",""few-shot learning"",llm,""r code"",""software categorization""}",Strategy and Management,"Business, Management and Accounting",Social Sciences & Humanities,CS,2023,"software categorization involves organizing software into groups based on their behavior or domain. traditionally, categorization has been crucial for software maintenance, aiding programmers in locating programs, identifying features, and finding similar ones within extensive code repositories. manual categorization is expensive, tedious, and labor-intensive, leading to the growing importance of automatic categorization approaches. however, existing datasets primarily focus on technical categorization for the most common programming language, leaving a gap in other areas. this paper addresses the research problem of classifying software repositories that contain r code. the objective is to develop a classification model capable of accurately and efficiently categorizing these repositories into predefined classes with less data. the contribution of this research is twofold. firstly, we propose a model that enables the categorization of software repositories focusing on r programming, even with a limited amount of training data. secondly, we conduct a comprehensive empirical evaluation to assess the impact of repository features and data augmentation on automatic repository categorization. this research endeavors to advance the field of software categorization and facilitate better utilization of software repositories in the context of diverse domains research.",0
No deal: German researchers’ publishing and citing behaviors after Big Deal negotiations with Elsevier,"{""In 2014, a union of German research organizations established Projekt DEAL, a national-level project to negotiate licensing agreements with large scientific publishers. Negotiations between DEAL and Elsevier began in 2016, and broke down without a successful agreement in 2018; during this time, around 200 German research institutions canceled their license agreements with Elsevier, leading Elsevier to restrict journal access at those institutions.We investigated the effect on researchers’ publishing and citing behaviors from a bibliometric perspective, using a data set of ~400,000 articles published by researchers at DEAL institutions during 2012–2020. We further investigated these effects with respect to the timing of contract cancellations, research disciplines, collaboration patterns, and article open-access status. We find evidence for a decrease in Elsevier’s market share of articles from DEAL institutions, with the largest year-on-year market share decreases occurring from 2018 to 2020 following the implementation of access restrictions. We also observe year-on-year decreases in the proportion of citations, although the decrease is smaller. We conclude that negotiations with Elsevier and access restrictions have led to some reduced willingness to publish in Elsevier journals, but that researchers are not strongly affected in their ability to cite Elsevier articles, implying that researchers use other methods to access scientific literature.""}",2023,Quantitative Science Studies,"{bibliometrics,""big deals"",germany,""open access"",""scholarly publishing""}",Analysis,Mathematics,Physical Sciences,CS,2023,"in 2014, a union of german research organizations established projekt deal, a national-level project to negotiate licensing agreements with large scientific publishers. negotiations between deal and elsevier began in 2016, and broke down without a successful agreement in 2018; during this time, around 200 german research institutions canceled their license agreements with elsevier, leading elsevier to restrict journal access at those institutions.we investigated the effect on researchers’ publishing and citing behaviors from a bibliometric perspective, using a data set of ~400,000 articles published by researchers at deal institutions during 2012–2020. we further investigated these effects with respect to the timing of contract cancellations, research disciplines, collaboration patterns, and article open-access status. we find evidence for a decrease in elsevier’s market share of articles from deal institutions, with the largest year-on-year market share decreases occurring from 2018 to 2020 following the implementation of access restrictions. we also observe year-on-year decreases in the proportion of citations, although the decrease is smaller. we conclude that negotiations with elsevier and access restrictions have led to some reduced willingness to publish in elsevier journals, but that researchers are not strongly affected in their ability to cite elsevier articles, implying that researchers use other methods to access scientific literature.",2
Dynamic global structure enhanced multi-channel graph neural network for session-based recommendation,"{""Session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. Most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. Meanwhile, previous works usually apply GNN to capture the transformation relationship between items, however the graph used in GNN is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. In this paper, we propose a novel method called Dynamic Global Structure Enhanced Multi-channel Graph Neural Network (DGS-MGNN) to learn accurate representations of items from multiple perspectives. In DGS-MGNN, we propose a novel GNN model named Multi-channel Graph Neural Network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. Meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. Finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. We conduct extensive experiments on three widely used datasets, and the results demonstrate that DGS-MGNN is consistently superior to the state-of-the-art baseline models.""}",2023,Information Sciences,"{""attention model"",""behavior modeling"",""graph neural network"",""recommendation system"",""representation learning"",""session-based recommendation""}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2023,"session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. meanwhile, previous works usually apply gnn to capture the transformation relationship between items, however the graph used in gnn is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. in this paper, we propose a novel method called dynamic global structure enhanced multi-channel graph neural network (dgs-mgnn) to learn accurate representations of items from multiple perspectives. in dgs-mgnn, we propose a novel gnn model named multi-channel graph neural network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. we conduct extensive experiments on three widely used datasets, and the results demonstrate that dgs-mgnn is consistently superior to the state-of-the-art baseline models.",4
Shall androids dream of genocides? How generative AI can change the future of memorialization of mass atrocities,"{""The memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. Digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. At the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. The emergence of generative forms of artificial intelligence (AI), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. AI can identify patterns in training data to create new narratives for representing and interpreting mass atrocities—and do so in a fraction of the time it takes for humans. The use of generative AI in this context raises numerous questions: For example, can the paucity of training data on mass atrocities distort how AI interprets some atrocity-related inquiries? How important is the ability to differentiate between human- and AI-made content concerning mass atrocities? Can AI-made content be used to promote false information concerning atrocities? This article addresses these and other questions by examining the opportunities and risks associated with using generative AIs for memorializing mass atrocities. It also discusses recommendations for AIs integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.""}",2023,Discover Artificial Intelligence,,Information Systems,Computer Science,Physical Sciences,CS,2023,"the memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. at the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. the emergence of generative forms of artificial intelligence (ai), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. ai can identify patterns in training data to create new narratives for representing and interpreting mass atrocities—and do so in a fraction of the time it takes for humans. the use of generative ai in this context raises numerous questions: for example, can the paucity of training data on mass atrocities distort how ai interprets some atrocity-related inquiries? how important is the ability to differentiate between human- and ai-made content concerning mass atrocities? can ai-made content be used to promote false information concerning atrocities? this article addresses these and other questions by examining the opportunities and risks associated with using generative ais for memorializing mass atrocities. it also discusses recommendations for ais integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.",1
Generating effective label description for label-aware sentiment classification,"{""Sentiment classification aims to predict the sentiment label for a given text. Recently, several research efforts have been devoted to incorporate matching clues between text words and class labels into the learning process of text representation. However, these methods heavily rely on the availability of label content. Moreover, they simply capture the label-specific signals to measure each word's contribution by either implicitly employing a learnable label representation or explicitly leveraging the interaction between text words and labels via the interaction mechanism. To deal with these issues, in this paper, we propose a novel framework called Label-Guided Dual-view Sentiment Classifier (LGDSC). We first introduce a new strategy for generating an effective label description and then design a novel Dual-Channel Label-guided Attention Network (DLAN) to learn a text representation via two different channels. DLAN will be further leveraged to learn label-guided text representations from two different views. Extensive experimental results on four real-world datasets demonstrate that LGDSC consistently outperforms the state-of-the-art baseline methods.""}",2023,Expert Systems with Applications,"{""attention network"",""sentiment analysis"",""sentiment classification"",""text summarization""}",Engineering (all),,,CS,2023,"sentiment classification aims to predict the sentiment label for a given text. recently, several research efforts have been devoted to incorporate matching clues between text words and class labels into the learning process of text representation. however, these methods heavily rely on the availability of label content. moreover, they simply capture the label-specific signals to measure each word's contribution by either implicitly employing a learnable label representation or explicitly leveraging the interaction between text words and labels via the interaction mechanism. to deal with these issues, in this paper, we propose a novel framework called label-guided dual-view sentiment classifier (lgdsc). we first introduce a new strategy for generating an effective label description and then design a novel dual-channel label-guided attention network (dlan) to learn a text representation via two different channels. dlan will be further leveraged to learn label-guided text representations from two different views. extensive experimental results on four real-world datasets demonstrate that lgdsc consistently outperforms the state-of-the-art baseline methods.",4
Research Knowledge Graphs in NFDI4DS,"{""The ever-increasing amount of research output through scientific articles requires means to enable transparency and a better understanding of key entities of the research lifecycle, referred to as research artifacts, such as methods, software, datasets, etc. Research Knowledge Graphs (RKG) make research artifacts findable, accessible, interoperable, and reusable (FAIR) and facilitate their interpretability. In this article, we describe the role of RKGs, from their construction to the expected benefits, including an overview and a vision of their use within the German National Research Data Infrastructure (NFDI) consortium NFDI4DataScience (NFDI4DS). This paper includes insights about the existing RKGs, how to formally represent research artifacts, and how this supports better transparency and reproducibility in data science and artificial intelligence. We also discuss key challenges, such as RKG construction, and integration, and give an outlook on future work.""}",2023,"Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)","{""data science"",""knowledge graph federation"",""knowledge graph integration"",nfdi,nfdi4ds,""research knowledge graph"",""scholarly data""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2023,"the ever-increasing amount of research output through scientific articles requires means to enable transparency and a better understanding of key entities of the research lifecycle, referred to as research artifacts, such as methods, software, datasets, etc. research knowledge graphs (rkg) make research artifacts findable, accessible, interoperable, and reusable (fair) and facilitate their interpretability. in this article, we describe the role of rkgs, from their construction to the expected benefits, including an overview and a vision of their use within the german national research data infrastructure (nfdi) consortium nfdi4datascience (nfdi4ds). this paper includes insights about the existing rkgs, how to formally represent research artifacts, and how this supports better transparency and reproducibility in data science and artificial intelligence. we also discuss key challenges, such as rkg construction, and integration, and give an outlook on future work.",2
Investigating the contribution of author- and publication-specific features to scholars’ h-index prediction,"{""Evaluation of researchers’ output is vital for hiring committees and funding bodies, and it is usually measured via their scientific productivity, citations, or a combined metric such as the h-index. Assessing young researchers is more critical because it takes a while to get citations and increment of h-index. Hence, predicting the h-index can help to discover the researchers’ scientific impact. In addition, identifying the influential factors to predict the scientific impact is helpful for researchers and their organizations seeking solutions to improve it. This study investigates the effect of the author, paper/venue-specific features on the future h-index. For this purpose, we used a machine learning approach to predict the h-index and feature analysis techniques to advance the understanding of feature impact. Utilizing the bibliometric data in Scopus, we defined and extracted two main groups of features. The first relates to prior scientific impact, and we name it ‘prior impact-based features’ and includes the number of publications, received citations, and h-index. The second group is ‘non-prior impact-based features’ and contains the features related to author, co-authorship, paper, and venue characteristics. We explored their importance in predicting researchers’ h-index in three career phases. Also, we examined the temporal dimension of predicting performance for different feature categories to find out which features are more reliable for long- and short-term prediction. We referred to the gender of the authors to examine the role of this author’s characteristics in the prediction task. Our findings showed that gender has a very slight effect in predicting the h-index. Although the results demonstrate better performance for the models containing prior impact-based features for all researchers’ groups in the near future, we found that non-prior impact-based features are more robust predictors for younger scholars in the long term. Also, prior impact-based features lose their power to predict more than other features in the long term.""}",2023,EPJ Data Science,"{""academic mobility"",""feature importance"",""h-index prediction"",""machine learning"",""open access publishing""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2023,"evaluation of researchers’ output is vital for hiring committees and funding bodies, and it is usually measured via their scientific productivity, citations, or a combined metric such as the h-index. assessing young researchers is more critical because it takes a while to get citations and increment of h-index. hence, predicting the h-index can help to discover the researchers’ scientific impact. in addition, identifying the influential factors to predict the scientific impact is helpful for researchers and their organizations seeking solutions to improve it. this study investigates the effect of the author, paper/venue-specific features on the future h-index. for this purpose, we used a machine learning approach to predict the h-index and feature analysis techniques to advance the understanding of feature impact. utilizing the bibliometric data in scopus, we defined and extracted two main groups of features. the first relates to prior scientific impact, and we name it ‘prior impact-based features’ and includes the number of publications, received citations, and h-index. the second group is ‘non-prior impact-based features’ and contains the features related to author, co-authorship, paper, and venue characteristics. we explored their importance in predicting researchers’ h-index in three career phases. also, we examined the temporal dimension of predicting performance for different feature categories to find out which features are more reliable for long- and short-term prediction. we referred to the gender of the authors to examine the role of this author’s characteristics in the prediction task. our findings showed that gender has a very slight effect in predicting the h-index. although the results demonstrate better performance for the models containing prior impact-based features for all researchers’ groups in the near future, we found that non-prior impact-based features are more robust predictors for younger scholars in the long term. also, prior impact-based features lose their power to predict more than other features in the long term.",8
Subject Classification of Software Repository,"{""Software categorization involves organizing software into groups based on their behavior or domain. Traditionally, categorization has been crucial for software maintenance, aiding programmers in locating programs, identifying features, and finding similar ones within extensive code repositories. Manual categorization is expensive, tedious, and labor-intensive, leading to the growing importance of automatic categorization approaches. However, existing datasets primarily focus on technical categorization for the most common programming language, leaving a gap in other areas. This paper addresses the research problem of classifying software repositories that contain R code. The objective is to develop a classification model capable of accurately and efficiently categorizing these repositories into predefined classes with less data. The contribution of this research is twofold. Firstly, we propose a model that enables the categorization of software repositories focusing on R programming, even with a limited amount of training data. Secondly, we conduct a comprehensive empirical evaluation to assess the impact of repository features and data augmentation on automatic repository categorization. This research endeavors to advance the field of software categorization and facilitate better utilization of software repositories in the context of diverse domains research.""}",2023,"International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K - Proceedings","{""data augmentation"",""few-shot learning"",llm,""r code"",""software categorization""}",Management of Technology and Innovation,"Business, Management and Accounting",Social Sciences & Humanities,CS,2023,"software categorization involves organizing software into groups based on their behavior or domain. traditionally, categorization has been crucial for software maintenance, aiding programmers in locating programs, identifying features, and finding similar ones within extensive code repositories. manual categorization is expensive, tedious, and labor-intensive, leading to the growing importance of automatic categorization approaches. however, existing datasets primarily focus on technical categorization for the most common programming language, leaving a gap in other areas. this paper addresses the research problem of classifying software repositories that contain r code. the objective is to develop a classification model capable of accurately and efficiently categorizing these repositories into predefined classes with less data. the contribution of this research is twofold. firstly, we propose a model that enables the categorization of software repositories focusing on r programming, even with a limited amount of training data. secondly, we conduct a comprehensive empirical evaluation to assess the impact of repository features and data augmentation on automatic repository categorization. this research endeavors to advance the field of software categorization and facilitate better utilization of software repositories in the context of diverse domains research.",0
CNIM-GCN: Consensus Neighbor Interaction-based Multi-channel Graph Convolutional Networks,"{""Node classification plays a critical role in numerous network applications, and has attracted increasing attention in recent years. Existing state-of-the-art studies aim at maintaining common information between the topology graph and the feature graph in an implicit way, i.e., adopting a common convolution with parameter sharing strategy to preserve common information between the two graphs. Despite their effectiveness, these studies are still far from satisfactory due to the complex correlation information between the two spaces. To address this issue, we present a novel method named Consensus Neighbor Interaction-based Multi-channel Graph Convolutional Networks (CNIM-GCN). CNIM-GCN preserves the common information between the feature space and topology space in an explicit way by introducing a consensus graph for information propagation. A multi-channel graph convolutional networks is developed for effectively fusing information from different graphs. In addition, we further incorporate two types of consistency constraints, i.e., structural consistency constraint and reconstruction consistency constraint, to maintain the consistency between different channels. The former is leveraged to keep the consistency between different spaces at the structural relationship level, while the latter is used to preserve a consistency between the final node representation and the original node feature representation. We carry out extensive experiments on five real-world datasets, including ACM, BlogCatalog, CiteSeer, Flickr and UAI2010. Experimental results show that our proposed approach CNIM-GCN is superior to the state-of-the-art baselines.""}",2023,Expert Systems with Applications,"{""deep learning"",""graph convolutional networks"",""network representation learning"",""node classification""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2023,"node classification plays a critical role in numerous network applications, and has attracted increasing attention in recent years. existing state-of-the-art studies aim at maintaining common information between the topology graph and the feature graph in an implicit way, i.e., adopting a common convolution with parameter sharing strategy to preserve common information between the two graphs. despite their effectiveness, these studies are still far from satisfactory due to the complex correlation information between the two spaces. to address this issue, we present a novel method named consensus neighbor interaction-based multi-channel graph convolutional networks (cnim-gcn). cnim-gcn preserves the common information between the feature space and topology space in an explicit way by introducing a consensus graph for information propagation. a multi-channel graph convolutional networks is developed for effectively fusing information from different graphs. in addition, we further incorporate two types of consistency constraints, i.e., structural consistency constraint and reconstruction consistency constraint, to maintain the consistency between different channels. the former is leveraged to keep the consistency between different spaces at the structural relationship level, while the latter is used to preserve a consistency between the final node representation and the original node feature representation. we carry out extensive experiments on five real-world datasets, including acm, blogcatalog, citeseer, flickr and uai2010. experimental results show that our proposed approach cnim-gcn is superior to the state-of-the-art baselines.",4
Generating effective label description for label-aware sentiment classification,"{""Sentiment classification aims to predict the sentiment label for a given text. Recently, several research efforts have been devoted to incorporate matching clues between text words and class labels into the learning process of text representation. However, these methods heavily rely on the availability of label content. Moreover, they simply capture the label-specific signals to measure each word's contribution by either implicitly employing a learnable label representation or explicitly leveraging the interaction between text words and labels via the interaction mechanism. To deal with these issues, in this paper, we propose a novel framework called Label-Guided Dual-view Sentiment Classifier (LGDSC). We first introduce a new strategy for generating an effective label description and then design a novel Dual-Channel Label-guided Attention Network (DLAN) to learn a text representation via two different channels. DLAN will be further leveraged to learn label-guided text representations from two different views. Extensive experimental results on four real-world datasets demonstrate that LGDSC consistently outperforms the state-of-the-art baseline methods.""}",2023,Expert Systems with Applications,"{""attention network"",""sentiment analysis"",""sentiment classification"",""text summarization""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2023,"sentiment classification aims to predict the sentiment label for a given text. recently, several research efforts have been devoted to incorporate matching clues between text words and class labels into the learning process of text representation. however, these methods heavily rely on the availability of label content. moreover, they simply capture the label-specific signals to measure each word's contribution by either implicitly employing a learnable label representation or explicitly leveraging the interaction between text words and labels via the interaction mechanism. to deal with these issues, in this paper, we propose a novel framework called label-guided dual-view sentiment classifier (lgdsc). we first introduce a new strategy for generating an effective label description and then design a novel dual-channel label-guided attention network (dlan) to learn a text representation via two different channels. dlan will be further leveraged to learn label-guided text representations from two different views. extensive experimental results on four real-world datasets demonstrate that lgdsc consistently outperforms the state-of-the-art baseline methods.",4
Baby’s CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models,"{""Large Language Models (LLMs) demonstrate remarkable performance on a variety of natural language understanding (NLU) tasks, primarily due to their in-context learning ability. This ability could be applied to building baby-like models, i.e. models at small scales, improving training efficiency. In this paper, we propose a “CoThought” pipeline, which efficiently trains smaller “baby” language models (BabyLMs) by leveraging the Chain of Thought prompting of LLMs. Our pipeline restructures a dataset of less than 100M in size using GPT-3.5-turbo, transforming it into task-oriented, human-readable texts that are comparable to the school texts for language learners. The BabyLM is then pretrained on this restructured dataset in a RoBERTa fashion. In evaluations across 4 benchmarks, our BabyLM outperforms the vanilla RoBERTa in 10 linguistic, NLU, and question-answering tasks by more than 3 points, showing a superior ability to extract contextual information. These results suggest that compact LMs pretrained on small, LLM-restructured data can better understand tasks and achieve improved performance.""}",2023,"CoNLL 2023 - BabyLM Challenge at the 27th Conference on Computational Natural Language Learning, Proceedings",,Human-Computer Interaction,Computer Science,Physical Sciences,CS,2023,"large language models (llms) demonstrate remarkable performance on a variety of natural language understanding (nlu) tasks, primarily due to their in-context learning ability. this ability could be applied to building baby-like models, i.e. models at small scales, improving training efficiency. in this paper, we propose a “cothought” pipeline, which efficiently trains smaller “baby” language models (babylms) by leveraging the chain of thought prompting of llms. our pipeline restructures a dataset of less than 100m in size using gpt-3.5-turbo, transforming it into task-oriented, human-readable texts that are comparable to the school texts for language learners. the babylm is then pretrained on this restructured dataset in a roberta fashion. in evaluations across 4 benchmarks, our babylm outperforms the vanilla roberta in 10 linguistic, nlu, and question-answering tasks by more than 3 points, showing a superior ability to extract contextual information. these results suggest that compact lms pretrained on small, llm-restructured data can better understand tasks and achieve improved performance.",0
NFDI4DS Transfer and Application,"{""Due to the ever increasing importance of Data Science and Artificial Intelligence methods for a wide range of scientific disciplines, ensuring transparency and reproducibility of DS and AI methods and research findings have become essential. The NFDI4DS project promotes the findability, accessibility, interoperability, and reusability in DS and AI by developing an open integrated research data infrastructure in which all artefacts (e. g., papers, code, models, datasets) will be interlinked in a FAIR and transparent way. One of the key aspects is to build a bridge between NFDI4DS and other research communities which actively apply DS and AI methods. This paper describes the main actions taken to engage with the relevant (sub)communities.""}",2023,"Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)","{""artificial intelligence"",""data science"",nfdi,nfdi4ds,""research data infrastructures""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2023,"due to the ever increasing importance of data science and artificial intelligence methods for a wide range of scientific disciplines, ensuring transparency and reproducibility of ds and ai methods and research findings have become essential. the nfdi4ds project promotes the findability, accessibility, interoperability, and reusability in ds and ai by developing an open integrated research data infrastructure in which all artefacts (e. g., papers, code, models, datasets) will be interlinked in a fair and transparent way. one of the key aspects is to build a bridge between nfdi4ds and other research communities which actively apply ds and ai methods. this paper describes the main actions taken to engage with the relevant (sub)communities.",2
Investigating the contribution of author- and publication-specific features to scholars’ h-index prediction,"{""Evaluation of researchers’ output is vital for hiring committees and funding bodies, and it is usually measured via their scientific productivity, citations, or a combined metric such as the h-index. Assessing young researchers is more critical because it takes a while to get citations and increment of h-index. Hence, predicting the h-index can help to discover the researchers’ scientific impact. In addition, identifying the influential factors to predict the scientific impact is helpful for researchers and their organizations seeking solutions to improve it. This study investigates the effect of the author, paper/venue-specific features on the future h-index. For this purpose, we used a machine learning approach to predict the h-index and feature analysis techniques to advance the understanding of feature impact. Utilizing the bibliometric data in Scopus, we defined and extracted two main groups of features. The first relates to prior scientific impact, and we name it ‘prior impact-based features’ and includes the number of publications, received citations, and h-index. The second group is ‘non-prior impact-based features’ and contains the features related to author, co-authorship, paper, and venue characteristics. We explored their importance in predicting researchers’ h-index in three career phases. Also, we examined the temporal dimension of predicting performance for different feature categories to find out which features are more reliable for long- and short-term prediction. We referred to the gender of the authors to examine the role of this author’s characteristics in the prediction task. Our findings showed that gender has a very slight effect in predicting the h-index. Although the results demonstrate better performance for the models containing prior impact-based features for all researchers’ groups in the near future, we found that non-prior impact-based features are more robust predictors for younger scholars in the long term. Also, prior impact-based features lose their power to predict more than other features in the long term.""}",2023,EPJ Data Science,"{""academic mobility"",""feature importance"",""h-index prediction"",""machine learning"",""open access publishing""}",Modeling and Simulation,Mathematics,Physical Sciences,CS,2023,"evaluation of researchers’ output is vital for hiring committees and funding bodies, and it is usually measured via their scientific productivity, citations, or a combined metric such as the h-index. assessing young researchers is more critical because it takes a while to get citations and increment of h-index. hence, predicting the h-index can help to discover the researchers’ scientific impact. in addition, identifying the influential factors to predict the scientific impact is helpful for researchers and their organizations seeking solutions to improve it. this study investigates the effect of the author, paper/venue-specific features on the future h-index. for this purpose, we used a machine learning approach to predict the h-index and feature analysis techniques to advance the understanding of feature impact. utilizing the bibliometric data in scopus, we defined and extracted two main groups of features. the first relates to prior scientific impact, and we name it ‘prior impact-based features’ and includes the number of publications, received citations, and h-index. the second group is ‘non-prior impact-based features’ and contains the features related to author, co-authorship, paper, and venue characteristics. we explored their importance in predicting researchers’ h-index in three career phases. also, we examined the temporal dimension of predicting performance for different feature categories to find out which features are more reliable for long- and short-term prediction. we referred to the gender of the authors to examine the role of this author’s characteristics in the prediction task. our findings showed that gender has a very slight effect in predicting the h-index. although the results demonstrate better performance for the models containing prior impact-based features for all researchers’ groups in the near future, we found that non-prior impact-based features are more robust predictors for younger scholars in the long term. also, prior impact-based features lose their power to predict more than other features in the long term.",8
Anomaly Detection for Numerical Literals in Knowledge Graphs: A Short Review of Approaches,"{""Anomaly Detection is an important problem that has been well-studied within diverse research areas and application domains. However, within the field of Semantic Web and Knowledge Graphs, anomaly detection has been relatively overlooked. Additionally, the existing literature on anomaly detection over Knowledge Graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. In light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over Knowledge Graphs. In this overview, we review the quality metrics of KGs and discuss the possible errors which may occur in different parts of the RDF data. Additionally, we outline a generic conceptual framework for the execution pipeline of Anomaly Detection over KGs. Moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. Finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for KGs.""}",2023,"Proceedings - 2023 IEEE 6th International Conference on Artificial Intelligence and Knowledge Engineering, AIKE 2023","{""anomaly detection"",""knowledge graphs"",""linked open data"",""outlier detection"",""rdf data"",""semantic web""}",Information Systems,Computer Science,Physical Sciences,CS,2023,"anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. however, within the field of semantic web and knowledge graphs, anomaly detection has been relatively overlooked. additionally, the existing literature on anomaly detection over knowledge graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. in light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over knowledge graphs. in this overview, we review the quality metrics of kgs and discuss the possible errors which may occur in different parts of the rdf data. additionally, we outline a generic conceptual framework for the execution pipeline of anomaly detection over kgs. moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for kgs.",5
Which factors are associated with Open Access publishing? A Springer Nature case study,"{""Open Access (OA) facilitates access to research articles. However, authors or funders often must pay the publishing costs, preventing authors who do not receive financial support from participating in OA publishing and gaining citation advantage for OA articles. OA may exacerbate existing inequalities in the publication system rather than overcome them. To investigate this, we studied 522,411 articles published by Springer Nature. Employing correlation and regression analyses, we describe the relationship between authors affiliated with countries from different income levels, their choice of publishing model, and the citation impact of their papers. A machine learning classification method helped us to explore the importance of different features in predicting the publishing model. The results show that authors eligible for article processing charge (APC) waivers publish more in gold OA journals than others. In contrast, authors eligible for an APC discount have the lowest ratio of OA publications, leading to the assumption that this discount insufficiently motivates authors to publish in gold OA journals. We found a strong correlation between the journal rank and the publishing model in gold OA journals, whereas the OA option is mostly avoided in hybrid journals. Also, results show that the countries’ income level, seniority, and experience with OA publications are the most predictive factors for OA publishing in hybrid journals.""}",2023,Quantitative Science Studies,"{""apc policies"",bibliometrics,""citation impact"",""machine learning"",""open access""}",Analysis,Mathematics,Physical Sciences,CS,2023,"open access (oa) facilitates access to research articles. however, authors or funders often must pay the publishing costs, preventing authors who do not receive financial support from participating in oa publishing and gaining citation advantage for oa articles. oa may exacerbate existing inequalities in the publication system rather than overcome them. to investigate this, we studied 522,411 articles published by springer nature. employing correlation and regression analyses, we describe the relationship between authors affiliated with countries from different income levels, their choice of publishing model, and the citation impact of their papers. a machine learning classification method helped us to explore the importance of different features in predicting the publishing model. the results show that authors eligible for article processing charge (apc) waivers publish more in gold oa journals than others. in contrast, authors eligible for an apc discount have the lowest ratio of oa publications, leading to the assumption that this discount insufficiently motivates authors to publish in gold oa journals. we found a strong correlation between the journal rank and the publishing model in gold oa journals, whereas the oa option is mostly avoided in hybrid journals. also, results show that the countries’ income level, seniority, and experience with oa publications are the most predictive factors for oa publishing in hybrid journals.",2
Subject Classification of Software Repository,"{""Software categorization involves organizing software into groups based on their behavior or domain. Traditionally, categorization has been crucial for software maintenance, aiding programmers in locating programs, identifying features, and finding similar ones within extensive code repositories. Manual categorization is expensive, tedious, and labor-intensive, leading to the growing importance of automatic categorization approaches. However, existing datasets primarily focus on technical categorization for the most common programming language, leaving a gap in other areas. This paper addresses the research problem of classifying software repositories that contain R code. The objective is to develop a classification model capable of accurately and efficiently categorizing these repositories into predefined classes with less data. The contribution of this research is twofold. Firstly, we propose a model that enables the categorization of software repositories focusing on R programming, even with a limited amount of training data. Secondly, we conduct a comprehensive empirical evaluation to assess the impact of repository features and data augmentation on automatic repository categorization. This research endeavors to advance the field of software categorization and facilitate better utilization of software repositories in the context of diverse domains research.""}",2023,"International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K - Proceedings","{""data augmentation"",""few-shot learning"",llm,""r code"",""software categorization""}",Software,Computer Science,Physical Sciences,CS,2023,"software categorization involves organizing software into groups based on their behavior or domain. traditionally, categorization has been crucial for software maintenance, aiding programmers in locating programs, identifying features, and finding similar ones within extensive code repositories. manual categorization is expensive, tedious, and labor-intensive, leading to the growing importance of automatic categorization approaches. however, existing datasets primarily focus on technical categorization for the most common programming language, leaving a gap in other areas. this paper addresses the research problem of classifying software repositories that contain r code. the objective is to develop a classification model capable of accurately and efficiently categorizing these repositories into predefined classes with less data. the contribution of this research is twofold. firstly, we propose a model that enables the categorization of software repositories focusing on r programming, even with a limited amount of training data. secondly, we conduct a comprehensive empirical evaluation to assess the impact of repository features and data augmentation on automatic repository categorization. this research endeavors to advance the field of software categorization and facilitate better utilization of software repositories in the context of diverse domains research.",0
SimE4KG: Distributed and Explainable Multi-Modal Semantic Similarity Estimation for Knowledge Graphs,"{""In recent years, exciting sources of data have been modeled as knowledge graphs (KGs). This modeling represents both structural relationships and the entity-specific multi-modal data in KGs. In various data analytics pipelines and machine learning (ML), the task of semantic similarity estimation plays a significant role. Assigning similarity values to entity pairs is needed in recommendation systems, clustering, classification, entity matching/disambiguation and many others. Efficient and scalable frameworks are needed to handle the quadratic complexity of all-pair semantic similarity on Big Data KGs. Moreover, heterogeneous KGs demand multi-modal semantic similarity estimation to cover the versatile contents like categorical relations between classes or their attribute literals like strings, timestamps or numeric data. In this paper, we propose the SimE4KG framework as a resource providing generic open-source modules that perform semantic similarity estimation in multi-modal KGs. To justify the computational costs of similarity estimation, the SimE4KG generates reproducible, reusable and explainable results. The pipeline results are a native semantic RDF KG, including the experiment results, hyper-parameter setup and explanation of the results, like the most influential features. For fast and scalable execution in memory, we implemented the distributed approach using Apache Spark. The entire development of this framework is integrated into the holistic distributed Semantic ANalytics StAck (SANSA).""}",2023,International Journal of Semantic Computing,"{""apache spark"",""distributed computing"",""explainable artificial intelligence"",""knowledge graphs"",""machine learning"",rdf,""scalable semantic processing"",""semantic similarity""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2023,"in recent years, exciting sources of data have been modeled as knowledge graphs (kgs). this modeling represents both structural relationships and the entity-specific multi-modal data in kgs. in various data analytics pipelines and machine learning (ml), the task of semantic similarity estimation plays a significant role. assigning similarity values to entity pairs is needed in recommendation systems, clustering, classification, entity matching/disambiguation and many others. efficient and scalable frameworks are needed to handle the quadratic complexity of all-pair semantic similarity on big data kgs. moreover, heterogeneous kgs demand multi-modal semantic similarity estimation to cover the versatile contents like categorical relations between classes or their attribute literals like strings, timestamps or numeric data. in this paper, we propose the sime4kg framework as a resource providing generic open-source modules that perform semantic similarity estimation in multi-modal kgs. to justify the computational costs of similarity estimation, the sime4kg generates reproducible, reusable and explainable results. the pipeline results are a native semantic rdf kg, including the experiment results, hyper-parameter setup and explanation of the results, like the most influential features. for fast and scalable execution in memory, we implemented the distributed approach using apache spark. the entire development of this framework is integrated into the holistic distributed semantic analytics stack (sansa).",3
Exploring rich structure information for aspect-based sentiment classification,"{""Graph Convolutional Network (GCN) for aspect-based sentiment classification has attracted a lot of attention recently due to their promising performance in handling complex structure information. However, previous methods based on GCN focused mainly on examining the structure of syntactic dependency relationships, which were subject to the noise and sparsity problem. Furthermore, these methods tend to focus on one kind of structural information (namely syntactic dependency) while ignoring many other kinds of rich structures between words. To tackle these problems, we propose a novel GCN based model, named Structure-Enhanced Dual-Channel Graph Convolutional Network (SEDC-GCN). Specifically, we first exploit the rich structure information by constructing a text sequence graph and an enhanced dependency graph, then design a dual-channel graph encoder to model the structure information from the two graphs. After that, we propose two kinds of aspect-specific attention, i.e., aspect-specific semantic attention and aspect-specific structure attention, to learn sentence representation from two different perspectives, i.e., the semantic perspective based on the text encoder, and the structure perspective based on the dual-channel graph encoder. Finally, we merge the sentence representations from the above two perspectives and obtain the final sentence representation. We experimentally validate our proposed model SEDC-GCN by comparing with seven strong baseline methods. In terms of the metric accuracy, SEDC-GCN achieves performance gains of 74.42%, 77.74%, 83.30%, 81.73% and 90.75% on TWITTER, LAPTOP, REST14, REST15, and REST16, respectively, which are 0.35%, 4.22%, 1.62%, 0.70% and 2.01% better than the best performing baseline BiGCN. Similar performance improvements are also observed in terms of the metric macro-averaged F1 score. The ablation study further demonstrates the effectiveness of each component of SEDC-GCN.""}",2023,Journal of Intelligent Information Systems,"{""aspect-based sentiment classification"",""attention mechanism"",""graph convolutional networks"",""sentiment analysis""}",Computer Networks and Communications,Computer Science,Physical Sciences,CS,2023,"graph convolutional network (gcn) for aspect-based sentiment classification has attracted a lot of attention recently due to their promising performance in handling complex structure information. however, previous methods based on gcn focused mainly on examining the structure of syntactic dependency relationships, which were subject to the noise and sparsity problem. furthermore, these methods tend to focus on one kind of structural information (namely syntactic dependency) while ignoring many other kinds of rich structures between words. to tackle these problems, we propose a novel gcn based model, named structure-enhanced dual-channel graph convolutional network (sedc-gcn). specifically, we first exploit the rich structure information by constructing a text sequence graph and an enhanced dependency graph, then design a dual-channel graph encoder to model the structure information from the two graphs. after that, we propose two kinds of aspect-specific attention, i.e., aspect-specific semantic attention and aspect-specific structure attention, to learn sentence representation from two different perspectives, i.e., the semantic perspective based on the text encoder, and the structure perspective based on the dual-channel graph encoder. finally, we merge the sentence representations from the above two perspectives and obtain the final sentence representation. we experimentally validate our proposed model sedc-gcn by comparing with seven strong baseline methods. in terms of the metric accuracy, sedc-gcn achieves performance gains of 74.42%, 77.74%, 83.30%, 81.73% and 90.75% on twitter, laptop, rest14, rest15, and rest16, respectively, which are 0.35%, 4.22%, 1.62%, 0.70% and 2.01% better than the best performing baseline bigcn. similar performance improvements are also observed in terms of the metric macro-averaged f1 score. the ablation study further demonstrates the effectiveness of each component of sedc-gcn.",4
Anomaly Detection for Numerical Literals in Knowledge Graphs: A Short Review of Approaches,"{""Anomaly Detection is an important problem that has been well-studied within diverse research areas and application domains. However, within the field of Semantic Web and Knowledge Graphs, anomaly detection has been relatively overlooked. Additionally, the existing literature on anomaly detection over Knowledge Graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. In light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over Knowledge Graphs. In this overview, we review the quality metrics of KGs and discuss the possible errors which may occur in different parts of the RDF data. Additionally, we outline a generic conceptual framework for the execution pipeline of Anomaly Detection over KGs. Moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. Finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for KGs.""}",2023,"Proceedings - 2023 IEEE 6th International Conference on Artificial Intelligence and Knowledge Engineering, AIKE 2023","{""anomaly detection"",""knowledge graphs"",""linked open data"",""outlier detection"",""rdf data"",""semantic web""}",Computer Vision and Pattern Recognition,Computer Science,Physical Sciences,CS,2023,"anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. however, within the field of semantic web and knowledge graphs, anomaly detection has been relatively overlooked. additionally, the existing literature on anomaly detection over knowledge graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. in light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over knowledge graphs. in this overview, we review the quality metrics of kgs and discuss the possible errors which may occur in different parts of the rdf data. additionally, we outline a generic conceptual framework for the execution pipeline of anomaly detection over kgs. moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for kgs.",5
Baby’s CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models,"{""Large Language Models (LLMs) demonstrate remarkable performance on a variety of natural language understanding (NLU) tasks, primarily due to their in-context learning ability. This ability could be applied to building baby-like models, i.e. models at small scales, improving training efficiency. In this paper, we propose a “CoThought” pipeline, which efficiently trains smaller “baby” language models (BabyLMs) by leveraging the Chain of Thought prompting of LLMs. Our pipeline restructures a dataset of less than 100M in size using GPT-3.5-turbo, transforming it into task-oriented, human-readable texts that are comparable to the school texts for language learners. The BabyLM is then pretrained on this restructured dataset in a RoBERTa fashion. In evaluations across 4 benchmarks, our BabyLM outperforms the vanilla RoBERTa in 10 linguistic, NLU, and question-answering tasks by more than 3 points, showing a superior ability to extract contextual information. These results suggest that compact LMs pretrained on small, LLM-restructured data can better understand tasks and achieve improved performance.""}",2023,"CoNLL 2023 - BabyLM Challenge at the 27th Conference on Computational Natural Language Learning, Proceedings",,Artificial Intelligence,Computer Science,Physical Sciences,CS,2023,"large language models (llms) demonstrate remarkable performance on a variety of natural language understanding (nlu) tasks, primarily due to their in-context learning ability. this ability could be applied to building baby-like models, i.e. models at small scales, improving training efficiency. in this paper, we propose a “cothought” pipeline, which efficiently trains smaller “baby” language models (babylms) by leveraging the chain of thought prompting of llms. our pipeline restructures a dataset of less than 100m in size using gpt-3.5-turbo, transforming it into task-oriented, human-readable texts that are comparable to the school texts for language learners. the babylm is then pretrained on this restructured dataset in a roberta fashion. in evaluations across 4 benchmarks, our babylm outperforms the vanilla roberta in 10 linguistic, nlu, and question-answering tasks by more than 3 points, showing a superior ability to extract contextual information. these results suggest that compact lms pretrained on small, llm-restructured data can better understand tasks and achieve improved performance.",0
Exploring rich structure information for aspect-based sentiment classification,"{""Graph Convolutional Network (GCN) for aspect-based sentiment classification has attracted a lot of attention recently due to their promising performance in handling complex structure information. However, previous methods based on GCN focused mainly on examining the structure of syntactic dependency relationships, which were subject to the noise and sparsity problem. Furthermore, these methods tend to focus on one kind of structural information (namely syntactic dependency) while ignoring many other kinds of rich structures between words. To tackle these problems, we propose a novel GCN based model, named Structure-Enhanced Dual-Channel Graph Convolutional Network (SEDC-GCN). Specifically, we first exploit the rich structure information by constructing a text sequence graph and an enhanced dependency graph, then design a dual-channel graph encoder to model the structure information from the two graphs. After that, we propose two kinds of aspect-specific attention, i.e., aspect-specific semantic attention and aspect-specific structure attention, to learn sentence representation from two different perspectives, i.e., the semantic perspective based on the text encoder, and the structure perspective based on the dual-channel graph encoder. Finally, we merge the sentence representations from the above two perspectives and obtain the final sentence representation. We experimentally validate our proposed model SEDC-GCN by comparing with seven strong baseline methods. In terms of the metric accuracy, SEDC-GCN achieves performance gains of 74.42%, 77.74%, 83.30%, 81.73% and 90.75% on TWITTER, LAPTOP, REST14, REST15, and REST16, respectively, which are 0.35%, 4.22%, 1.62%, 0.70% and 2.01% better than the best performing baseline BiGCN. Similar performance improvements are also observed in terms of the metric macro-averaged F1 score. The ablation study further demonstrates the effectiveness of each component of SEDC-GCN.""}",2023,Journal of Intelligent Information Systems,"{""aspect-based sentiment classification"",""attention mechanism"",""graph convolutional networks"",""sentiment analysis""}",Hardware and Architecture,Computer Science,Physical Sciences,CS,2023,"graph convolutional network (gcn) for aspect-based sentiment classification has attracted a lot of attention recently due to their promising performance in handling complex structure information. however, previous methods based on gcn focused mainly on examining the structure of syntactic dependency relationships, which were subject to the noise and sparsity problem. furthermore, these methods tend to focus on one kind of structural information (namely syntactic dependency) while ignoring many other kinds of rich structures between words. to tackle these problems, we propose a novel gcn based model, named structure-enhanced dual-channel graph convolutional network (sedc-gcn). specifically, we first exploit the rich structure information by constructing a text sequence graph and an enhanced dependency graph, then design a dual-channel graph encoder to model the structure information from the two graphs. after that, we propose two kinds of aspect-specific attention, i.e., aspect-specific semantic attention and aspect-specific structure attention, to learn sentence representation from two different perspectives, i.e., the semantic perspective based on the text encoder, and the structure perspective based on the dual-channel graph encoder. finally, we merge the sentence representations from the above two perspectives and obtain the final sentence representation. we experimentally validate our proposed model sedc-gcn by comparing with seven strong baseline methods. in terms of the metric accuracy, sedc-gcn achieves performance gains of 74.42%, 77.74%, 83.30%, 81.73% and 90.75% on twitter, laptop, rest14, rest15, and rest16, respectively, which are 0.35%, 4.22%, 1.62%, 0.70% and 2.01% better than the best performing baseline bigcn. similar performance improvements are also observed in terms of the metric macro-averaged f1 score. the ablation study further demonstrates the effectiveness of each component of sedc-gcn.",4
Decoding Prompt Syntax: Analysing its Impact on Knowledge Retrieval in Large Language Models,"{""Large Language Models (LLMs), with their advanced architectures and training on massive language datasets, contain unexplored knowledge. One method to infer this knowledge is through the use of cloze-style prompts. Typically, these prompts are manually designed because the phrasing of these prompts impacts the knowledge retrieval performance, even if the LLM encodes the desired information. In this paper, we study the impact of prompt syntax on the knowledge retrieval capacity of LLMs. We use a template-based approach to paraphrase simple prompts into prompts with a more complex grammatical structure. We then analyse the LLM performance for these structurally different but semantically equivalent prompts. Our study reveals that simple prompts work better than complex forms of sentences. The performance across the syntactical variations for simple relations (1:1) remains best, with a marginal decrease across different typologies. These results reinforce that simple prompt structures are more effective for knowledge retrieval in LLMs and motivate future research into the impact of prompt syntax on various tasks.""}",2023,"ACM Web Conference 2023 - Companion of the World Wide Web Conference, WWW 2023","{bert,""knowledge retrieval"",""large language models"",""syntax aware prompt""}",Software,Computer Science,Physical Sciences,CS,2023,"large language models (llms), with their advanced architectures and training on massive language datasets, contain unexplored knowledge. one method to infer this knowledge is through the use of cloze-style prompts. typically, these prompts are manually designed because the phrasing of these prompts impacts the knowledge retrieval performance, even if the llm encodes the desired information. in this paper, we study the impact of prompt syntax on the knowledge retrieval capacity of llms. we use a template-based approach to paraphrase simple prompts into prompts with a more complex grammatical structure. we then analyse the llm performance for these structurally different but semantically equivalent prompts. our study reveals that simple prompts work better than complex forms of sentences. the performance across the syntactical variations for simple relations (1:1) remains best, with a marginal decrease across different typologies. these results reinforce that simple prompt structures are more effective for knowledge retrieval in llms and motivate future research into the impact of prompt syntax on various tasks.",0
Exploring rich structure information for aspect-based sentiment classification,"{""Graph Convolutional Network (GCN) for aspect-based sentiment classification has attracted a lot of attention recently due to their promising performance in handling complex structure information. However, previous methods based on GCN focused mainly on examining the structure of syntactic dependency relationships, which were subject to the noise and sparsity problem. Furthermore, these methods tend to focus on one kind of structural information (namely syntactic dependency) while ignoring many other kinds of rich structures between words. To tackle these problems, we propose a novel GCN based model, named Structure-Enhanced Dual-Channel Graph Convolutional Network (SEDC-GCN). Specifically, we first exploit the rich structure information by constructing a text sequence graph and an enhanced dependency graph, then design a dual-channel graph encoder to model the structure information from the two graphs. After that, we propose two kinds of aspect-specific attention, i.e., aspect-specific semantic attention and aspect-specific structure attention, to learn sentence representation from two different perspectives, i.e., the semantic perspective based on the text encoder, and the structure perspective based on the dual-channel graph encoder. Finally, we merge the sentence representations from the above two perspectives and obtain the final sentence representation. We experimentally validate our proposed model SEDC-GCN by comparing with seven strong baseline methods. In terms of the metric accuracy, SEDC-GCN achieves performance gains of 74.42%, 77.74%, 83.30%, 81.73% and 90.75% on TWITTER, LAPTOP, REST14, REST15, and REST16, respectively, which are 0.35%, 4.22%, 1.62%, 0.70% and 2.01% better than the best performing baseline BiGCN. Similar performance improvements are also observed in terms of the metric macro-averaged F1 score. The ablation study further demonstrates the effectiveness of each component of SEDC-GCN.""}",2023,Journal of Intelligent Information Systems,"{""aspect-based sentiment classification"",""attention mechanism"",""graph convolutional networks"",""sentiment analysis""}",Information Systems,Computer Science,Physical Sciences,CS,2023,"graph convolutional network (gcn) for aspect-based sentiment classification has attracted a lot of attention recently due to their promising performance in handling complex structure information. however, previous methods based on gcn focused mainly on examining the structure of syntactic dependency relationships, which were subject to the noise and sparsity problem. furthermore, these methods tend to focus on one kind of structural information (namely syntactic dependency) while ignoring many other kinds of rich structures between words. to tackle these problems, we propose a novel gcn based model, named structure-enhanced dual-channel graph convolutional network (sedc-gcn). specifically, we first exploit the rich structure information by constructing a text sequence graph and an enhanced dependency graph, then design a dual-channel graph encoder to model the structure information from the two graphs. after that, we propose two kinds of aspect-specific attention, i.e., aspect-specific semantic attention and aspect-specific structure attention, to learn sentence representation from two different perspectives, i.e., the semantic perspective based on the text encoder, and the structure perspective based on the dual-channel graph encoder. finally, we merge the sentence representations from the above two perspectives and obtain the final sentence representation. we experimentally validate our proposed model sedc-gcn by comparing with seven strong baseline methods. in terms of the metric accuracy, sedc-gcn achieves performance gains of 74.42%, 77.74%, 83.30%, 81.73% and 90.75% on twitter, laptop, rest14, rest15, and rest16, respectively, which are 0.35%, 4.22%, 1.62%, 0.70% and 2.01% better than the best performing baseline bigcn. similar performance improvements are also observed in terms of the metric macro-averaged f1 score. the ablation study further demonstrates the effectiveness of each component of sedc-gcn.",4
Exploring rich structure information for aspect-based sentiment classification,"{""Graph Convolutional Network (GCN) for aspect-based sentiment classification has attracted a lot of attention recently due to their promising performance in handling complex structure information. However, previous methods based on GCN focused mainly on examining the structure of syntactic dependency relationships, which were subject to the noise and sparsity problem. Furthermore, these methods tend to focus on one kind of structural information (namely syntactic dependency) while ignoring many other kinds of rich structures between words. To tackle these problems, we propose a novel GCN based model, named Structure-Enhanced Dual-Channel Graph Convolutional Network (SEDC-GCN). Specifically, we first exploit the rich structure information by constructing a text sequence graph and an enhanced dependency graph, then design a dual-channel graph encoder to model the structure information from the two graphs. After that, we propose two kinds of aspect-specific attention, i.e., aspect-specific semantic attention and aspect-specific structure attention, to learn sentence representation from two different perspectives, i.e., the semantic perspective based on the text encoder, and the structure perspective based on the dual-channel graph encoder. Finally, we merge the sentence representations from the above two perspectives and obtain the final sentence representation. We experimentally validate our proposed model SEDC-GCN by comparing with seven strong baseline methods. In terms of the metric accuracy, SEDC-GCN achieves performance gains of 74.42%, 77.74%, 83.30%, 81.73% and 90.75% on TWITTER, LAPTOP, REST14, REST15, and REST16, respectively, which are 0.35%, 4.22%, 1.62%, 0.70% and 2.01% better than the best performing baseline BiGCN. Similar performance improvements are also observed in terms of the metric macro-averaged F1 score. The ablation study further demonstrates the effectiveness of each component of SEDC-GCN.""}",2023,Journal of Intelligent Information Systems,"{""aspect-based sentiment classification"",""attention mechanism"",""graph convolutional networks"",""sentiment analysis""}",Software,Computer Science,Physical Sciences,CS,2023,"graph convolutional network (gcn) for aspect-based sentiment classification has attracted a lot of attention recently due to their promising performance in handling complex structure information. however, previous methods based on gcn focused mainly on examining the structure of syntactic dependency relationships, which were subject to the noise and sparsity problem. furthermore, these methods tend to focus on one kind of structural information (namely syntactic dependency) while ignoring many other kinds of rich structures between words. to tackle these problems, we propose a novel gcn based model, named structure-enhanced dual-channel graph convolutional network (sedc-gcn). specifically, we first exploit the rich structure information by constructing a text sequence graph and an enhanced dependency graph, then design a dual-channel graph encoder to model the structure information from the two graphs. after that, we propose two kinds of aspect-specific attention, i.e., aspect-specific semantic attention and aspect-specific structure attention, to learn sentence representation from two different perspectives, i.e., the semantic perspective based on the text encoder, and the structure perspective based on the dual-channel graph encoder. finally, we merge the sentence representations from the above two perspectives and obtain the final sentence representation. we experimentally validate our proposed model sedc-gcn by comparing with seven strong baseline methods. in terms of the metric accuracy, sedc-gcn achieves performance gains of 74.42%, 77.74%, 83.30%, 81.73% and 90.75% on twitter, laptop, rest14, rest15, and rest16, respectively, which are 0.35%, 4.22%, 1.62%, 0.70% and 2.01% better than the best performing baseline bigcn. similar performance improvements are also observed in terms of the metric macro-averaged f1 score. the ablation study further demonstrates the effectiveness of each component of sedc-gcn.",4
CNIM-GCN: Consensus Neighbor Interaction-based Multi-channel Graph Convolutional Networks,"{""Node classification plays a critical role in numerous network applications, and has attracted increasing attention in recent years. Existing state-of-the-art studies aim at maintaining common information between the topology graph and the feature graph in an implicit way, i.e., adopting a common convolution with parameter sharing strategy to preserve common information between the two graphs. Despite their effectiveness, these studies are still far from satisfactory due to the complex correlation information between the two spaces. To address this issue, we present a novel method named Consensus Neighbor Interaction-based Multi-channel Graph Convolutional Networks (CNIM-GCN). CNIM-GCN preserves the common information between the feature space and topology space in an explicit way by introducing a consensus graph for information propagation. A multi-channel graph convolutional networks is developed for effectively fusing information from different graphs. In addition, we further incorporate two types of consistency constraints, i.e., structural consistency constraint and reconstruction consistency constraint, to maintain the consistency between different channels. The former is leveraged to keep the consistency between different spaces at the structural relationship level, while the latter is used to preserve a consistency between the final node representation and the original node feature representation. We carry out extensive experiments on five real-world datasets, including ACM, BlogCatalog, CiteSeer, Flickr and UAI2010. Experimental results show that our proposed approach CNIM-GCN is superior to the state-of-the-art baselines.""}",2023,Expert Systems with Applications,"{""deep learning"",""graph convolutional networks"",""network representation learning"",""node classification""}",Engineering (all),,,CS,2023,"node classification plays a critical role in numerous network applications, and has attracted increasing attention in recent years. existing state-of-the-art studies aim at maintaining common information between the topology graph and the feature graph in an implicit way, i.e., adopting a common convolution with parameter sharing strategy to preserve common information between the two graphs. despite their effectiveness, these studies are still far from satisfactory due to the complex correlation information between the two spaces. to address this issue, we present a novel method named consensus neighbor interaction-based multi-channel graph convolutional networks (cnim-gcn). cnim-gcn preserves the common information between the feature space and topology space in an explicit way by introducing a consensus graph for information propagation. a multi-channel graph convolutional networks is developed for effectively fusing information from different graphs. in addition, we further incorporate two types of consistency constraints, i.e., structural consistency constraint and reconstruction consistency constraint, to maintain the consistency between different channels. the former is leveraged to keep the consistency between different spaces at the structural relationship level, while the latter is used to preserve a consistency between the final node representation and the original node feature representation. we carry out extensive experiments on five real-world datasets, including acm, blogcatalog, citeseer, flickr and uai2010. experimental results show that our proposed approach cnim-gcn is superior to the state-of-the-art baselines.",4
Question Answering Versus Named Entity Recognition for Extracting Unknown Datasets,"{""Dataset mention extraction is a difficult problem due to the unstructured nature of text, the sparsity of dataset mentions, and the various ways the same dataset can be mentioned. Extracting unknown dataset mentions which are not part of the training data of the model is even harder. We address this challenge in two ways. First, we consider a two-step approach where a binary classifier filters out positive contexts, i.e., detects sentences with a dataset mention. We consider multiple transformer-based models and strong baselines for this task. Subsequently, the dataset is extracted from the positive context. Second, we consider a one-step approach and directly aim to detect and extract a possible dataset mention. For the extraction of datasets, we consider transformer models in named entity recognition (NER) mode. We contrast NER with the transformers' capabilities for question answering (QA). We use the Coleridge Initiative 'Show US the Data' dataset consisting of 14.3k scientific papers with about 35k mentions of datasets. We found that using transformers in QA mode is a better choice than NER for extracting unknown datasets. The rationale is that detecting new datasets is an out-of-vocabulary task, i.e., the dataset name has not been seen once during training. Comparing the two-step versus the one-step approach, we found contrasting strengths. A two-step dataset extraction using an MLP for filtering and RoBERTa in QA mode extracts more dataset mentions than a one-step system, but at the cost of a lower F1-score of 62.7%. A one-step extraction with DeBERTa in QA achieves the highest F1-score of 92.88% at the cost of missing dataset mentions. We recommend the one-step approach for the case when accuracy is more important, and the two-step approach when there is a postprocessing mechanism for the extracted dataset mentions, e.g., a manual check. The source code is available at https://github.com/yousef-younes/dataset-mention-extraction.""}",2023,IEEE Access,"{""binary text classification"",""dataset mentions"",""named entity recognition"",""question answering""}",Materials Science (all),,,CS,2023,"dataset mention extraction is a difficult problem due to the unstructured nature of text, the sparsity of dataset mentions, and the various ways the same dataset can be mentioned. extracting unknown dataset mentions which are not part of the training data of the model is even harder. we address this challenge in two ways. first, we consider a two-step approach where a binary classifier filters out positive contexts, i.e., detects sentences with a dataset mention. we consider multiple transformer-based models and strong baselines for this task. subsequently, the dataset is extracted from the positive context. second, we consider a one-step approach and directly aim to detect and extract a possible dataset mention. for the extraction of datasets, we consider transformer models in named entity recognition (ner) mode. we contrast ner with the transformers' capabilities for question answering (qa). we use the coleridge initiative 'show us the data' dataset consisting of 14.3k scientific papers with about 35k mentions of datasets. we found that using transformers in qa mode is a better choice than ner for extracting unknown datasets. the rationale is that detecting new datasets is an out-of-vocabulary task, i.e., the dataset name has not been seen once during training. comparing the two-step versus the one-step approach, we found contrasting strengths. a two-step dataset extraction using an mlp for filtering and roberta in qa mode extracts more dataset mentions than a one-step system, but at the cost of a lower f1-score of 62.7%. a one-step extraction with deberta in qa achieves the highest f1-score of 92.88% at the cost of missing dataset mentions. we recommend the one-step approach for the case when accuracy is more important, and the two-step approach when there is a postprocessing mechanism for the extracted dataset mentions, e.g., a manual check. the source code is available at https://github.com/yousef-younes/dataset-mention-extraction.",0
Towards syntax-aware pretraining and prompt engineering for knowledge retrieval from large language models,"{""The ability to access relational knowledge from LLM parameters, known as relational knowledge retrieval (rKR), is considered a critical factor in their capacity to comprehend and interpret natural language. However, the role of syntax in this context has not been adequately explored. In this position paper, we hypothesize a close link between the accessibility of relational knowledge and syntax. We discuss related works and lay out a research agenda focused on rKR from self-supervised LLMs without or with minimal fine-tuning and aiming at understanding the impact of syntax on rKR. This involves examining biases, factors affecting result reliability and robustness, and analyzing the effect of syntactic features in training corpora on rKR. We argue that rKR can be improved through syntax-aware pretraining and prompt engineering, and propose a dedicated research agenda geared toward exploring the impact of syntax on knowledge retrieval.""}",2023,CEUR Workshop Proceedings,,Computer Science (all),,,CS,2023,"the ability to access relational knowledge from llm parameters, known as relational knowledge retrieval (rkr), is considered a critical factor in their capacity to comprehend and interpret natural language. however, the role of syntax in this context has not been adequately explored. in this position paper, we hypothesize a close link between the accessibility of relational knowledge and syntax. we discuss related works and lay out a research agenda focused on rkr from self-supervised llms without or with minimal fine-tuning and aiming at understanding the impact of syntax on rkr. this involves examining biases, factors affecting result reliability and robustness, and analyzing the effect of syntactic features in training corpora on rkr. we argue that rkr can be improved through syntax-aware pretraining and prompt engineering, and propose a dedicated research agenda geared toward exploring the impact of syntax on knowledge retrieval.",0
SimE4KG: Distributed and Explainable Multi-Modal Semantic Similarity Estimation for Knowledge Graphs,"{""In recent years, exciting sources of data have been modeled as knowledge graphs (KGs). This modeling represents both structural relationships and the entity-specific multi-modal data in KGs. In various data analytics pipelines and machine learning (ML), the task of semantic similarity estimation plays a significant role. Assigning similarity values to entity pairs is needed in recommendation systems, clustering, classification, entity matching/disambiguation and many others. Efficient and scalable frameworks are needed to handle the quadratic complexity of all-pair semantic similarity on Big Data KGs. Moreover, heterogeneous KGs demand multi-modal semantic similarity estimation to cover the versatile contents like categorical relations between classes or their attribute literals like strings, timestamps or numeric data. In this paper, we propose the SimE4KG framework as a resource providing generic open-source modules that perform semantic similarity estimation in multi-modal KGs. To justify the computational costs of similarity estimation, the SimE4KG generates reproducible, reusable and explainable results. The pipeline results are a native semantic RDF KG, including the experiment results, hyper-parameter setup and explanation of the results, like the most influential features. For fast and scalable execution in memory, we implemented the distributed approach using Apache Spark. The entire development of this framework is integrated into the holistic distributed Semantic ANalytics StAck (SANSA).""}",2023,International Journal of Semantic Computing,"{""apache spark"",""distributed computing"",""explainable artificial intelligence"",""knowledge graphs"",""machine learning"",rdf,""scalable semantic processing"",""semantic similarity""}",Computer Networks and Communications,Computer Science,Physical Sciences,CS,2023,"in recent years, exciting sources of data have been modeled as knowledge graphs (kgs). this modeling represents both structural relationships and the entity-specific multi-modal data in kgs. in various data analytics pipelines and machine learning (ml), the task of semantic similarity estimation plays a significant role. assigning similarity values to entity pairs is needed in recommendation systems, clustering, classification, entity matching/disambiguation and many others. efficient and scalable frameworks are needed to handle the quadratic complexity of all-pair semantic similarity on big data kgs. moreover, heterogeneous kgs demand multi-modal semantic similarity estimation to cover the versatile contents like categorical relations between classes or their attribute literals like strings, timestamps or numeric data. in this paper, we propose the sime4kg framework as a resource providing generic open-source modules that perform semantic similarity estimation in multi-modal kgs. to justify the computational costs of similarity estimation, the sime4kg generates reproducible, reusable and explainable results. the pipeline results are a native semantic rdf kg, including the experiment results, hyper-parameter setup and explanation of the results, like the most influential features. for fast and scalable execution in memory, we implemented the distributed approach using apache spark. the entire development of this framework is integrated into the holistic distributed semantic analytics stack (sansa).",3
Dynamic global structure enhanced multi-channel graph neural network for session-based recommendation,"{""Session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. Most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. Meanwhile, previous works usually apply GNN to capture the transformation relationship between items, however the graph used in GNN is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. In this paper, we propose a novel method called Dynamic Global Structure Enhanced Multi-channel Graph Neural Network (DGS-MGNN) to learn accurate representations of items from multiple perspectives. In DGS-MGNN, we propose a novel GNN model named Multi-channel Graph Neural Network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. Meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. Finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. We conduct extensive experiments on three widely used datasets, and the results demonstrate that DGS-MGNN is consistently superior to the state-of-the-art baseline models.""}",2023,Information Sciences,"{""attention model"",""behavior modeling"",""graph neural network"",""recommendation system"",""representation learning"",""session-based recommendation""}",Software,Computer Science,Physical Sciences,CS,2023,"session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. meanwhile, previous works usually apply gnn to capture the transformation relationship between items, however the graph used in gnn is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. in this paper, we propose a novel method called dynamic global structure enhanced multi-channel graph neural network (dgs-mgnn) to learn accurate representations of items from multiple perspectives. in dgs-mgnn, we propose a novel gnn model named multi-channel graph neural network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. we conduct extensive experiments on three widely used datasets, and the results demonstrate that dgs-mgnn is consistently superior to the state-of-the-art baseline models.",4
People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection,"{""NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data augmentation, including Counterfactually Augmented Data (CADs). CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. However, manually generating CADs can be time-consuming and expensive. Hence in this work, we assess if this task can be automated using generative NLP models. We automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate their usefulness in improving model robustness compared to manually-generated CADs. By testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual CADs are still the most effective, CADs generated by ChatGPT come a close second. One key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label.""}",2023,"EMNLP 2023 - 2023 Conference on Empirical Methods in Natural Language Processing, Proceedings",,Information Systems,Computer Science,Physical Sciences,CS,2023,"nlp models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. therefore, it is imperative that these models are robust to spurious features. past work has attempted to tackle such spurious features using training data augmentation, including counterfactually augmented data (cads). cads introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. however, manually generating cads can be time-consuming and expensive. hence in this work, we assess if this task can be automated using generative nlp models. we automatically generate cads using polyjuice, chatgpt, and flan-t5, and evaluate their usefulness in improving model robustness compared to manually-generated cads. by testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual cads are still the most effective, cads generated by chatgpt come a close second. one key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label.",0
Question Answering Versus Named Entity Recognition for Extracting Unknown Datasets,"{""Dataset mention extraction is a difficult problem due to the unstructured nature of text, the sparsity of dataset mentions, and the various ways the same dataset can be mentioned. Extracting unknown dataset mentions which are not part of the training data of the model is even harder. We address this challenge in two ways. First, we consider a two-step approach where a binary classifier filters out positive contexts, i.e., detects sentences with a dataset mention. We consider multiple transformer-based models and strong baselines for this task. Subsequently, the dataset is extracted from the positive context. Second, we consider a one-step approach and directly aim to detect and extract a possible dataset mention. For the extraction of datasets, we consider transformer models in named entity recognition (NER) mode. We contrast NER with the transformers' capabilities for question answering (QA). We use the Coleridge Initiative 'Show US the Data' dataset consisting of 14.3k scientific papers with about 35k mentions of datasets. We found that using transformers in QA mode is a better choice than NER for extracting unknown datasets. The rationale is that detecting new datasets is an out-of-vocabulary task, i.e., the dataset name has not been seen once during training. Comparing the two-step versus the one-step approach, we found contrasting strengths. A two-step dataset extraction using an MLP for filtering and RoBERTa in QA mode extracts more dataset mentions than a one-step system, but at the cost of a lower F1-score of 62.7%. A one-step extraction with DeBERTa in QA achieves the highest F1-score of 92.88% at the cost of missing dataset mentions. We recommend the one-step approach for the case when accuracy is more important, and the two-step approach when there is a postprocessing mechanism for the extracted dataset mentions, e.g., a manual check. The source code is available at https://github.com/yousef-younes/dataset-mention-extraction.""}",2023,IEEE Access,"{""binary text classification"",""dataset mentions"",""named entity recognition"",""question answering""}",Engineering (all),,,CS,2023,"dataset mention extraction is a difficult problem due to the unstructured nature of text, the sparsity of dataset mentions, and the various ways the same dataset can be mentioned. extracting unknown dataset mentions which are not part of the training data of the model is even harder. we address this challenge in two ways. first, we consider a two-step approach where a binary classifier filters out positive contexts, i.e., detects sentences with a dataset mention. we consider multiple transformer-based models and strong baselines for this task. subsequently, the dataset is extracted from the positive context. second, we consider a one-step approach and directly aim to detect and extract a possible dataset mention. for the extraction of datasets, we consider transformer models in named entity recognition (ner) mode. we contrast ner with the transformers' capabilities for question answering (qa). we use the coleridge initiative 'show us the data' dataset consisting of 14.3k scientific papers with about 35k mentions of datasets. we found that using transformers in qa mode is a better choice than ner for extracting unknown datasets. the rationale is that detecting new datasets is an out-of-vocabulary task, i.e., the dataset name has not been seen once during training. comparing the two-step versus the one-step approach, we found contrasting strengths. a two-step dataset extraction using an mlp for filtering and roberta in qa mode extracts more dataset mentions than a one-step system, but at the cost of a lower f1-score of 62.7%. a one-step extraction with deberta in qa achieves the highest f1-score of 92.88% at the cost of missing dataset mentions. we recommend the one-step approach for the case when accuracy is more important, and the two-step approach when there is a postprocessing mechanism for the extracted dataset mentions, e.g., a manual check. the source code is available at https://github.com/yousef-younes/dataset-mention-extraction.",0
ExPAD: An Explainable Distributed Automatic Anomaly Detection Framework over Large KGs,"{""RDF data is playing an important role in publishing and integrating heterogeneous data in data lakes. However, since the data is generally created utilizing liberal curation approaches such as crowd-sourcing and automatic extraction tools with no cross-validation on input data, the data is prone to errors that can be hidden in several dimensions. The types of errors which can be considered as outliers may occur in any part of RDF statements, especially in literal objects. Although some scientific studies have revealed anomalies in knowledge graphs, none of the current approaches has the ability to explain the anomalies that have been discovered. In this paper, we present ExPAD, a scalable and distributed framework for explainable numeric anomaly detection on very large RDF knowledge graphs. Inspired by OutlierTree, ExPAD generates human-readable explanations for a given numeric result being an outlier by following and assessing supervised decision tree splits. The proposed framework ExPAD is open-source, well-documented, and fully integrated into the Semantic Analytics Stack (SANSA). Experiments on real-world use cases and synthetic datasets disclose that the framework can not only handle high volumes of RDF data but also efficiently generate explanations for hidden anomalies discovered in KGs.""}",2023,"Proceedings - 17th IEEE International Conference on Semantic Computing, ICSC 2023","{""big data"",""decision tree"",""distributed computing"",""explainable anomaly detection"",rdf,sansa}",Computer Networks and Communications,Computer Science,Physical Sciences,CS,2023,"rdf data is playing an important role in publishing and integrating heterogeneous data in data lakes. however, since the data is generally created utilizing liberal curation approaches such as crowd-sourcing and automatic extraction tools with no cross-validation on input data, the data is prone to errors that can be hidden in several dimensions. the types of errors which can be considered as outliers may occur in any part of rdf statements, especially in literal objects. although some scientific studies have revealed anomalies in knowledge graphs, none of the current approaches has the ability to explain the anomalies that have been discovered. in this paper, we present expad, a scalable and distributed framework for explainable numeric anomaly detection on very large rdf knowledge graphs. inspired by outliertree, expad generates human-readable explanations for a given numeric result being an outlier by following and assessing supervised decision tree splits. the proposed framework expad is open-source, well-documented, and fully integrated into the semantic analytics stack (sansa). experiments on real-world use cases and synthetic datasets disclose that the framework can not only handle high volumes of rdf data but also efficiently generate explanations for hidden anomalies discovered in kgs.",2
Shall androids dream of genocides? How generative AI can change the future of memorialization of mass atrocities,"{""The memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. Digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. At the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. The emergence of generative forms of artificial intelligence (AI), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. AI can identify patterns in training data to create new narratives for representing and interpreting mass atrocities—and do so in a fraction of the time it takes for humans. The use of generative AI in this context raises numerous questions: For example, can the paucity of training data on mass atrocities distort how AI interprets some atrocity-related inquiries? How important is the ability to differentiate between human- and AI-made content concerning mass atrocities? Can AI-made content be used to promote false information concerning atrocities? This article addresses these and other questions by examining the opportunities and risks associated with using generative AIs for memorializing mass atrocities. It also discusses recommendations for AIs integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.""}",2023,Discover Artificial Intelligence,,Computer Vision and Pattern Recognition,Computer Science,Physical Sciences,CS,2023,"the memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. at the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. the emergence of generative forms of artificial intelligence (ai), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. ai can identify patterns in training data to create new narratives for representing and interpreting mass atrocities—and do so in a fraction of the time it takes for humans. the use of generative ai in this context raises numerous questions: for example, can the paucity of training data on mass atrocities distort how ai interprets some atrocity-related inquiries? how important is the ability to differentiate between human- and ai-made content concerning mass atrocities? can ai-made content be used to promote false information concerning atrocities? this article addresses these and other questions by examining the opportunities and risks associated with using generative ais for memorializing mass atrocities. it also discusses recommendations for ais integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.",1
People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection,"{""NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data augmentation, including Counterfactually Augmented Data (CADs). CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. However, manually generating CADs can be time-consuming and expensive. Hence in this work, we assess if this task can be automated using generative NLP models. We automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate their usefulness in improving model robustness compared to manually-generated CADs. By testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual CADs are still the most effective, CADs generated by ChatGPT come a close second. One key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label.""}",2023,"EMNLP 2023 - 2023 Conference on Empirical Methods in Natural Language Processing, Proceedings",,Computer Science Applications,Computer Science,Physical Sciences,CS,2023,"nlp models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. therefore, it is imperative that these models are robust to spurious features. past work has attempted to tackle such spurious features using training data augmentation, including counterfactually augmented data (cads). cads introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. however, manually generating cads can be time-consuming and expensive. hence in this work, we assess if this task can be automated using generative nlp models. we automatically generate cads using polyjuice, chatgpt, and flan-t5, and evaluate their usefulness in improving model robustness compared to manually-generated cads. by testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual cads are still the most effective, cads generated by chatgpt come a close second. one key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label.",0
Anomaly Detection for Numerical Literals in Knowledge Graphs: A Short Review of Approaches,"{""Anomaly Detection is an important problem that has been well-studied within diverse research areas and application domains. However, within the field of Semantic Web and Knowledge Graphs, anomaly detection has been relatively overlooked. Additionally, the existing literature on anomaly detection over Knowledge Graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. In light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over Knowledge Graphs. In this overview, we review the quality metrics of KGs and discuss the possible errors which may occur in different parts of the RDF data. Additionally, we outline a generic conceptual framework for the execution pipeline of Anomaly Detection over KGs. Moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. Finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for KGs.""}",2023,"Proceedings - 2023 IEEE 6th International Conference on Artificial Intelligence and Knowledge Engineering, AIKE 2023","{""anomaly detection"",""knowledge graphs"",""linked open data"",""outlier detection"",""rdf data"",""semantic web""}",Information Systems and Management,Decision Sciences,Social Sciences & Humanities,CS,2023,"anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. however, within the field of semantic web and knowledge graphs, anomaly detection has been relatively overlooked. additionally, the existing literature on anomaly detection over knowledge graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. in light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over knowledge graphs. in this overview, we review the quality metrics of kgs and discuss the possible errors which may occur in different parts of the rdf data. additionally, we outline a generic conceptual framework for the execution pipeline of anomaly detection over kgs. moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for kgs.",5
GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding,"{""Language models can serve as a valuable tool for software developers to increase productivity. Large generative models can be used for code generation and code completion, while smaller encoder-only models are capable of performing code search tasks using natural language queries. These capabilities are heavily influenced by the quality and diversity of the available training data. Source code datasets used for training usually focus on the most popular languages and testing is mostly conducted on the same distributions, often overlooking low-resource programming languages. Motivated by the NLP generalization taxonomy proposed by Hupkes et. al., we propose a new benchmark dataset called GenCodeSearchNet (GeCS) which builds upon existing natural language code search datasets to systemically evaluate the programming language understanding generalization capabilities of language models. As part of the full dataset, we introduce a new, manually curated subset StatCodeSearch that focuses on R, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science. For evaluation and comparison, we collect several baseline results using fine-tuned BERT-style models and GPT-style large language models in a zero-shot setting.""}",2023,"GenBench 2023 - GenBench: 1st Workshop on Generalisation (Benchmarking) in NLP, Proceedings",,Information Systems,Computer Science,Physical Sciences,CS,2023,"language models can serve as a valuable tool for software developers to increase productivity. large generative models can be used for code generation and code completion, while smaller encoder-only models are capable of performing code search tasks using natural language queries. these capabilities are heavily influenced by the quality and diversity of the available training data. source code datasets used for training usually focus on the most popular languages and testing is mostly conducted on the same distributions, often overlooking low-resource programming languages. motivated by the nlp generalization taxonomy proposed by hupkes et. al., we propose a new benchmark dataset called gencodesearchnet (gecs) which builds upon existing natural language code search datasets to systemically evaluate the programming language understanding generalization capabilities of language models. as part of the full dataset, we introduce a new, manually curated subset statcodesearch that focuses on r, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science. for evaluation and comparison, we collect several baseline results using fine-tuned bert-style models and gpt-style large language models in a zero-shot setting.",0
SimE4KG: Distributed and Explainable Multi-Modal Semantic Similarity Estimation for Knowledge Graphs,"{""In recent years, exciting sources of data have been modeled as knowledge graphs (KGs). This modeling represents both structural relationships and the entity-specific multi-modal data in KGs. In various data analytics pipelines and machine learning (ML), the task of semantic similarity estimation plays a significant role. Assigning similarity values to entity pairs is needed in recommendation systems, clustering, classification, entity matching/disambiguation and many others. Efficient and scalable frameworks are needed to handle the quadratic complexity of all-pair semantic similarity on Big Data KGs. Moreover, heterogeneous KGs demand multi-modal semantic similarity estimation to cover the versatile contents like categorical relations between classes or their attribute literals like strings, timestamps or numeric data. In this paper, we propose the SimE4KG framework as a resource providing generic open-source modules that perform semantic similarity estimation in multi-modal KGs. To justify the computational costs of similarity estimation, the SimE4KG generates reproducible, reusable and explainable results. The pipeline results are a native semantic RDF KG, including the experiment results, hyper-parameter setup and explanation of the results, like the most influential features. For fast and scalable execution in memory, we implemented the distributed approach using Apache Spark. The entire development of this framework is integrated into the holistic distributed Semantic ANalytics StAck (SANSA).""}",2023,International Journal of Semantic Computing,"{""apache spark"",""distributed computing"",""explainable artificial intelligence"",""knowledge graphs"",""machine learning"",rdf,""scalable semantic processing"",""semantic similarity""}",Information Systems,Computer Science,Physical Sciences,CS,2023,"in recent years, exciting sources of data have been modeled as knowledge graphs (kgs). this modeling represents both structural relationships and the entity-specific multi-modal data in kgs. in various data analytics pipelines and machine learning (ml), the task of semantic similarity estimation plays a significant role. assigning similarity values to entity pairs is needed in recommendation systems, clustering, classification, entity matching/disambiguation and many others. efficient and scalable frameworks are needed to handle the quadratic complexity of all-pair semantic similarity on big data kgs. moreover, heterogeneous kgs demand multi-modal semantic similarity estimation to cover the versatile contents like categorical relations between classes or their attribute literals like strings, timestamps or numeric data. in this paper, we propose the sime4kg framework as a resource providing generic open-source modules that perform semantic similarity estimation in multi-modal kgs. to justify the computational costs of similarity estimation, the sime4kg generates reproducible, reusable and explainable results. the pipeline results are a native semantic rdf kg, including the experiment results, hyper-parameter setup and explanation of the results, like the most influential features. for fast and scalable execution in memory, we implemented the distributed approach using apache spark. the entire development of this framework is integrated into the holistic distributed semantic analytics stack (sansa).",3
Which factors are associated with Open Access publishing? A Springer Nature case study,"{""Open Access (OA) facilitates access to research articles. However, authors or funders often must pay the publishing costs, preventing authors who do not receive financial support from participating in OA publishing and gaining citation advantage for OA articles. OA may exacerbate existing inequalities in the publication system rather than overcome them. To investigate this, we studied 522,411 articles published by Springer Nature. Employing correlation and regression analyses, we describe the relationship between authors affiliated with countries from different income levels, their choice of publishing model, and the citation impact of their papers. A machine learning classification method helped us to explore the importance of different features in predicting the publishing model. The results show that authors eligible for article processing charge (APC) waivers publish more in gold OA journals than others. In contrast, authors eligible for an APC discount have the lowest ratio of OA publications, leading to the assumption that this discount insufficiently motivates authors to publish in gold OA journals. We found a strong correlation between the journal rank and the publishing model in gold OA journals, whereas the OA option is mostly avoided in hybrid journals. Also, results show that the countries’ income level, seniority, and experience with OA publications are the most predictive factors for OA publishing in hybrid journals.""}",2023,Quantitative Science Studies,"{""apc policies"",bibliometrics,""citation impact"",""machine learning"",""open access""}",Numerical Analysis,Mathematics,Physical Sciences,CS,2023,"open access (oa) facilitates access to research articles. however, authors or funders often must pay the publishing costs, preventing authors who do not receive financial support from participating in oa publishing and gaining citation advantage for oa articles. oa may exacerbate existing inequalities in the publication system rather than overcome them. to investigate this, we studied 522,411 articles published by springer nature. employing correlation and regression analyses, we describe the relationship between authors affiliated with countries from different income levels, their choice of publishing model, and the citation impact of their papers. a machine learning classification method helped us to explore the importance of different features in predicting the publishing model. the results show that authors eligible for article processing charge (apc) waivers publish more in gold oa journals than others. in contrast, authors eligible for an apc discount have the lowest ratio of oa publications, leading to the assumption that this discount insufficiently motivates authors to publish in gold oa journals. we found a strong correlation between the journal rank and the publishing model in gold oa journals, whereas the oa option is mostly avoided in hybrid journals. also, results show that the countries’ income level, seniority, and experience with oa publications are the most predictive factors for oa publishing in hybrid journals.",2
ExPAD: An Explainable Distributed Automatic Anomaly Detection Framework over Large KGs,"{""RDF data is playing an important role in publishing and integrating heterogeneous data in data lakes. However, since the data is generally created utilizing liberal curation approaches such as crowd-sourcing and automatic extraction tools with no cross-validation on input data, the data is prone to errors that can be hidden in several dimensions. The types of errors which can be considered as outliers may occur in any part of RDF statements, especially in literal objects. Although some scientific studies have revealed anomalies in knowledge graphs, none of the current approaches has the ability to explain the anomalies that have been discovered. In this paper, we present ExPAD, a scalable and distributed framework for explainable numeric anomaly detection on very large RDF knowledge graphs. Inspired by OutlierTree, ExPAD generates human-readable explanations for a given numeric result being an outlier by following and assessing supervised decision tree splits. The proposed framework ExPAD is open-source, well-documented, and fully integrated into the Semantic Analytics Stack (SANSA). Experiments on real-world use cases and synthetic datasets disclose that the framework can not only handle high volumes of RDF data but also efficiently generate explanations for hidden anomalies discovered in KGs.""}",2023,"Proceedings - 17th IEEE International Conference on Semantic Computing, ICSC 2023","{""big data"",""decision tree"",""distributed computing"",""explainable anomaly detection"",rdf,sansa}",Information Systems,Computer Science,Physical Sciences,CS,2023,"rdf data is playing an important role in publishing and integrating heterogeneous data in data lakes. however, since the data is generally created utilizing liberal curation approaches such as crowd-sourcing and automatic extraction tools with no cross-validation on input data, the data is prone to errors that can be hidden in several dimensions. the types of errors which can be considered as outliers may occur in any part of rdf statements, especially in literal objects. although some scientific studies have revealed anomalies in knowledge graphs, none of the current approaches has the ability to explain the anomalies that have been discovered. in this paper, we present expad, a scalable and distributed framework for explainable numeric anomaly detection on very large rdf knowledge graphs. inspired by outliertree, expad generates human-readable explanations for a given numeric result being an outlier by following and assessing supervised decision tree splits. the proposed framework expad is open-source, well-documented, and fully integrated into the semantic analytics stack (sansa). experiments on real-world use cases and synthetic datasets disclose that the framework can not only handle high volumes of rdf data but also efficiently generate explanations for hidden anomalies discovered in kgs.",2
"GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets","{""Named Entity Recognition (NER) models play a crucial role in various NLP tasks, including information extraction (IE) and text understanding. In academic writing, references to machine learning models and datasets are fundamental components of various computer science publications and necessitate accurate models for identification. Despite the advancements in NER, existing ground truth datasets do not treat fine-grained types like ML model and model architecture as separate entity types, and consequently, baseline models cannot recognize them as such. In this paper, we release a corpus of 100 manually annotated full-text scientific publications and a first baseline model for 10 entity types centered around ML models and datasets. In order to provide a nuanced understanding of how ML models and datasets are mentioned and utilized, our dataset also contains annotations for informal mentions like “our BERT-based model” or “an image CNN”. You can find the ground truth dataset and code to replicate model training at https://data.gesis.org/gsap/gsap-ner.""}",2023,Findings of the Association for Computational Linguistics: EMNLP 2023,,Information Systems,Computer Science,Physical Sciences,CS,2023,"named entity recognition (ner) models play a crucial role in various nlp tasks, including information extraction (ie) and text understanding. in academic writing, references to machine learning models and datasets are fundamental components of various computer science publications and necessitate accurate models for identification. despite the advancements in ner, existing ground truth datasets do not treat fine-grained types like ml model and model architecture as separate entity types, and consequently, baseline models cannot recognize them as such. in this paper, we release a corpus of 100 manually annotated full-text scientific publications and a first baseline model for 10 entity types centered around ml models and datasets. in order to provide a nuanced understanding of how ml models and datasets are mentioned and utilized, our dataset also contains annotations for informal mentions like “our bert-based model” or “an image cnn”. you can find the ground truth dataset and code to replicate model training at https://data.gesis.org/gsap/gsap-ner.",6
Why Net Worth Misrepresents Wealth Effects and What to Do About It,"{""Wealth plays an important role in social stratification but the results that can be obtained when analyzing wealth as a predictor variable depend on modeling decisions. Although wealth consists of multiple components it is often operationalized as net worth. Moreover, wealth effects are likely non-linear, but the functional form is often unknown. To overcome these problems, we propose to 1) split up net worth into gross wealth and debt and evaluate their joint effect and 2) use non-parametric Generalized Additive Models. We show in a simulation study that this approach describes systematic wealth differences in more detail and overfits less to random variation in the data than standard approaches. We then apply the approach to re-analyze wealth gaps in educational attainment in the US. We find that the operationalization of wealth as net worth results in a misclassification of which children have the best and the worst educational prospects. Not negative net worth is associated with the worst educational prospects but only the combination of low gross wealth and low debt. The most advantaged group are not only children with high net worth but all children with high gross wealth independent of the households’ amount of debt.""}",2023,Sociological Science,"{education,""generalized additive models"",""net worth"",""simulation study"",wealth}",Social Sciences (all),,,CS,2023,"wealth plays an important role in social stratification but the results that can be obtained when analyzing wealth as a predictor variable depend on modeling decisions. although wealth consists of multiple components it is often operationalized as net worth. moreover, wealth effects are likely non-linear, but the functional form is often unknown. to overcome these problems, we propose to 1) split up net worth into gross wealth and debt and evaluate their joint effect and 2) use non-parametric generalized additive models. we show in a simulation study that this approach describes systematic wealth differences in more detail and overfits less to random variation in the data than standard approaches. we then apply the approach to re-analyze wealth gaps in educational attainment in the us. we find that the operationalization of wealth as net worth results in a misclassification of which children have the best and the worst educational prospects. not negative net worth is associated with the worst educational prospects but only the combination of low gross wealth and low debt. the most advantaged group are not only children with high net worth but all children with high gross wealth independent of the households’ amount of debt.",3
"GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets","{""Named Entity Recognition (NER) models play a crucial role in various NLP tasks, including information extraction (IE) and text understanding. In academic writing, references to machine learning models and datasets are fundamental components of various computer science publications and necessitate accurate models for identification. Despite the advancements in NER, existing ground truth datasets do not treat fine-grained types like ML model and model architecture as separate entity types, and consequently, baseline models cannot recognize them as such. In this paper, we release a corpus of 100 manually annotated full-text scientific publications and a first baseline model for 10 entity types centered around ML models and datasets. In order to provide a nuanced understanding of how ML models and datasets are mentioned and utilized, our dataset also contains annotations for informal mentions like “our BERT-based model” or “an image CNN”. You can find the ground truth dataset and code to replicate model training at https://data.gesis.org/gsap/gsap-ner.""}",2023,Findings of the Association for Computational Linguistics: EMNLP 2023,,Computational Theory and Mathematics,Computer Science,Physical Sciences,CS,2023,"named entity recognition (ner) models play a crucial role in various nlp tasks, including information extraction (ie) and text understanding. in academic writing, references to machine learning models and datasets are fundamental components of various computer science publications and necessitate accurate models for identification. despite the advancements in ner, existing ground truth datasets do not treat fine-grained types like ml model and model architecture as separate entity types, and consequently, baseline models cannot recognize them as such. in this paper, we release a corpus of 100 manually annotated full-text scientific publications and a first baseline model for 10 entity types centered around ml models and datasets. in order to provide a nuanced understanding of how ml models and datasets are mentioned and utilized, our dataset also contains annotations for informal mentions like “our bert-based model” or “an image cnn”. you can find the ground truth dataset and code to replicate model training at https://data.gesis.org/gsap/gsap-ner.",6
Bibliometric-Enhanced Information Retrieval: 13th International BIR Workshop (BIR 2023),"{""The 13th iteration of the Bibliometric-enhanced Information Retrieval (BIR) workshop series will take place at ECIR 2023 as a full-day workshop. BIR tackles issues related to, for instance, academic search and recommendation, at the intersection of Information Retrieval, Natural Language Processing, and Bibliometrics. As an interdisciplinary scientific event, BIR brings together researchers and practitioners from the Scientometrics/Bibliometrics community on the one hand, and the Information Retrieval community on the other hand. BIR is an ever-growing topic investigated by both academia and the industry.""}",2023,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"{""academic search"",bibliometrics,""digital libraries"",""information retrieval"",scientometrics}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2023,"the 13th iteration of the bibliometric-enhanced information retrieval (bir) workshop series will take place at ecir 2023 as a full-day workshop. bir tackles issues related to, for instance, academic search and recommendation, at the intersection of information retrieval, natural language processing, and bibliometrics. as an interdisciplinary scientific event, bir brings together researchers and practitioners from the scientometrics/bibliometrics community on the one hand, and the information retrieval community on the other hand. bir is an ever-growing topic investigated by both academia and the industry.",2
Goodness of fit tests for random multigraph models,"{""Goodness of fit tests for two probabilistic multigraph models are presented. The first model is random stub matching given fixed degrees (RSM) so that edge assignments to vertex pair sites are dependent, and the second is independent edge assignments (IEA) according to a common probability distribution. Tests are performed using goodness of fit measures between the edge multiplicity sequence of an observed multigraph, and the expected one according to a simple or composite hypothesis. Test statistics of Pearson type and of likelihood ratio type are used, and the expected values of the Pearson statistic under the different models are derived. Test performances based on simulations indicate that even for small number of edges, the null distributions of both statistics are well approximated by their asymptotic (Formula presented.) -distribution. The non-null distributions of the test statistics can be well approximated by proposed adjusted (Formula presented.) -distributions used for power approximations. The influence of RSM on both test statistics is substantial for small number of edges and implies a shift of their distributions towards smaller values compared to what holds true for the null distributions under IEA. Two applications on social networks are included to illustrate how the tests can guide in the analysis of social structure.""}",2023,Journal of Applied Statistics,"{""data aggregation"",""goodness of fit"",""multivariate networks"",""network model"",""random multigraphs"",""random stub matching""}",Statistics and Probability,Mathematics,Physical Sciences,CS,2023,"goodness of fit tests for two probabilistic multigraph models are presented. the first model is random stub matching given fixed degrees (rsm) so that edge assignments to vertex pair sites are dependent, and the second is independent edge assignments (iea) according to a common probability distribution. tests are performed using goodness of fit measures between the edge multiplicity sequence of an observed multigraph, and the expected one according to a simple or composite hypothesis. test statistics of pearson type and of likelihood ratio type are used, and the expected values of the pearson statistic under the different models are derived. test performances based on simulations indicate that even for small number of edges, the null distributions of both statistics are well approximated by their asymptotic (formula presented.) -distribution. the non-null distributions of the test statistics can be well approximated by proposed adjusted (formula presented.) -distributions used for power approximations. the influence of rsm on both test statistics is substantial for small number of edges and implies a shift of their distributions towards smaller values compared to what holds true for the null distributions under iea. two applications on social networks are included to illustrate how the tests can guide in the analysis of social structure.",3
People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection,"{""NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data augmentation, including Counterfactually Augmented Data (CADs). CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. However, manually generating CADs can be time-consuming and expensive. Hence in this work, we assess if this task can be automated using generative NLP models. We automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate their usefulness in improving model robustness compared to manually-generated CADs. By testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual CADs are still the most effective, CADs generated by ChatGPT come a close second. One key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label.""}",2023,"EMNLP 2023 - 2023 Conference on Empirical Methods in Natural Language Processing, Proceedings",,Computational Theory and Mathematics,Computer Science,Physical Sciences,CS,2023,"nlp models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. therefore, it is imperative that these models are robust to spurious features. past work has attempted to tackle such spurious features using training data augmentation, including counterfactually augmented data (cads). cads introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. however, manually generating cads can be time-consuming and expensive. hence in this work, we assess if this task can be automated using generative nlp models. we automatically generate cads using polyjuice, chatgpt, and flan-t5, and evaluate their usefulness in improving model robustness compared to manually-generated cads. by testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual cads are still the most effective, cads generated by chatgpt come a close second. one key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label.",0
Preface to Joint Workshop of the 4th Extraction and Evaluation of Knowledge Entities from Scientific Documents (EEKE2023) and the 3rd AI + Informetrics (AII2023) at JCDL 2023,"{""The Joint Workshop of the 4th Extraction and Evaluation of Knowledge Entities from Scientific Documents (EEKE2023; https://eeke-workshop.github.io/) and the 3rd AI + Informetrics (AII2023; https://ai-informetrics.github.io/) was held at Santa Fe, New Mexico, USA and online, co-located with the ACM/IEEE Joint Conference on Digital Libraries (JCDL) 2023. The two workshop series aim to engage the communities in open problems in the extraction and evaluation of knowledge entities from scientific documents and the modeling and applications of AI + Informetrics for broad interests in science of science, science, technology, & innovation, etc. This joint workshop comprises keynote speeches, oral presentations, and poster sessions. The main topics of the proceedings include entity extraction and its applications, along with the integration of Artificial Intelligence + Informetrics.""}",2023,CEUR Workshop Proceedings,"{""artificial intelligence"",informetrics,""knowledge entity evaluation"",""knowledge entity extraction"",""scientific document""}",Computer Science (all),,,CS,2023,"the joint workshop of the 4th extraction and evaluation of knowledge entities from scientific documents (eeke2023; https://eeke-workshop.github.io/) and the 3rd ai + informetrics (aii2023; https://ai-informetrics.github.io/) was held at santa fe, new mexico, usa and online, co-located with the acm/ieee joint conference on digital libraries (jcdl) 2023. the two workshop series aim to engage the communities in open problems in the extraction and evaluation of knowledge entities from scientific documents and the modeling and applications of ai + informetrics for broad interests in science of science, science, technology, & innovation, etc. this joint workshop comprises keynote speeches, oral presentations, and poster sessions. the main topics of the proceedings include entity extraction and its applications, along with the integration of artificial intelligence + informetrics.",2
NFDI4DS Infrastructure and Services,"{""NFDI4DataScience (NFDI4DS) is a consortium founded to support researchers in all stages of the research data lifecycle in order to conduct their research in line with the FAIR principles. The infrastructure developed targets researchers from a wide range of disciplines working in the field of data science and artificial intelligence. NFDI4DS contributes to systematically understanding the needs and challenges of researchers in various disciplines regarding data science and artificial intelligence, keeping in mind ethical, legal and social aspects. The identified needs will be addressed by support structures such as educational videos and challenges. Transparency, reproducibility and FAIRness will be improved by integrating existing and newly developed services into the NFDI4DS infrastructure, and by systematically adding all digital objects (articles, data, machine learning models, workflows, scripts/code, etc.) to the NFDI4DS research knowledge graph. This paper presents the goals of NFDI4DS, and gives an overview on what the consortium is going to contribute to the data science and artificial intelligence communities. It focuses on existing and newly developed services and their integration.""}",2023,"Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)","{nfdi,nfdi4ds,""research data infrastructures""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2023,"nfdi4datascience (nfdi4ds) is a consortium founded to support researchers in all stages of the research data lifecycle in order to conduct their research in line with the fair principles. the infrastructure developed targets researchers from a wide range of disciplines working in the field of data science and artificial intelligence. nfdi4ds contributes to systematically understanding the needs and challenges of researchers in various disciplines regarding data science and artificial intelligence, keeping in mind ethical, legal and social aspects. the identified needs will be addressed by support structures such as educational videos and challenges. transparency, reproducibility and fairness will be improved by integrating existing and newly developed services into the nfdi4ds infrastructure, and by systematically adding all digital objects (articles, data, machine learning models, workflows, scripts/code, etc.) to the nfdi4ds research knowledge graph. this paper presents the goals of nfdi4ds, and gives an overview on what the consortium is going to contribute to the data science and artificial intelligence communities. it focuses on existing and newly developed services and their integration.",2
grafzahl: fine-tuning Transformers for text data from within R,"{""This paper introduces grafzahl, an R package for fine-tuning Transformers for text data from within R. The package is used in this paper to reproduce the analyses in other papers. Very significant improvement in model accuracy over traditional machine learning approaches such as convolutional Neural Network is observed.""}",2023,Computational Communication Research,"{""analysis machine learning"",""automated content"",r.python,transformers}",Computational Theory and Mathematics,Computer Science,Physical Sciences,CS,2023,"this paper introduces grafzahl, an r package for fine-tuning transformers for text data from within r. the package is used in this paper to reproduce the analyses in other papers. very significant improvement in model accuracy over traditional machine learning approaches such as convolutional neural network is observed.",8
Efficient computation of comprehensive statistical information of large OWL datasets: a scalable approach,"{""Computing dataset statistics is crucial for exploring their structure, however, it becomes challenging for large-scale datasets. This has several key benefits, such as link target identification, vocabulary reuse, quality analysis, big data analytics, and coverage analysis. In this paper, we present the first attempt of developing a distributed approach (OWLStats) for collecting comprehensive statistics over large-scale OWL datasets. OWLStats is a distributed in-memory approach for computing 50 statistical criteria for OWL datasets utilizing Apache Spark. We have successfully integrated OWLStats into the SANSA framework. Experiments results prove that OWLStats is linearly scalable in terms of both node and data scalability.""}",2023,Enterprise Information Systems,"{""distributed processing"",""in-memory approach"",""sansa framework"",""scalable architecture"",""semantic web"",""statistics computations""}",Information Systems and Management,Decision Sciences,Social Sciences & Humanities,CS,2023,"computing dataset statistics is crucial for exploring their structure, however, it becomes challenging for large-scale datasets. this has several key benefits, such as link target identification, vocabulary reuse, quality analysis, big data analytics, and coverage analysis. in this paper, we present the first attempt of developing a distributed approach (owlstats) for collecting comprehensive statistics over large-scale owl datasets. owlstats is a distributed in-memory approach for computing 50 statistical criteria for owl datasets utilizing apache spark. we have successfully integrated owlstats into the sansa framework. experiments results prove that owlstats is linearly scalable in terms of both node and data scalability.",3
Investigating the contribution of author- and publication-specific features to scholars’ h-index prediction,"{""Evaluation of researchers’ output is vital for hiring committees and funding bodies, and it is usually measured via their scientific productivity, citations, or a combined metric such as the h-index. Assessing young researchers is more critical because it takes a while to get citations and increment of h-index. Hence, predicting the h-index can help to discover the researchers’ scientific impact. In addition, identifying the influential factors to predict the scientific impact is helpful for researchers and their organizations seeking solutions to improve it. This study investigates the effect of the author, paper/venue-specific features on the future h-index. For this purpose, we used a machine learning approach to predict the h-index and feature analysis techniques to advance the understanding of feature impact. Utilizing the bibliometric data in Scopus, we defined and extracted two main groups of features. The first relates to prior scientific impact, and we name it ‘prior impact-based features’ and includes the number of publications, received citations, and h-index. The second group is ‘non-prior impact-based features’ and contains the features related to author, co-authorship, paper, and venue characteristics. We explored their importance in predicting researchers’ h-index in three career phases. Also, we examined the temporal dimension of predicting performance for different feature categories to find out which features are more reliable for long- and short-term prediction. We referred to the gender of the authors to examine the role of this author’s characteristics in the prediction task. Our findings showed that gender has a very slight effect in predicting the h-index. Although the results demonstrate better performance for the models containing prior impact-based features for all researchers’ groups in the near future, we found that non-prior impact-based features are more robust predictors for younger scholars in the long term. Also, prior impact-based features lose their power to predict more than other features in the long term.""}",2023,EPJ Data Science,"{""academic mobility"",""feature importance"",""h-index prediction"",""machine learning"",""open access publishing""}",Computational Mathematics,Mathematics,Physical Sciences,CS,2023,"evaluation of researchers’ output is vital for hiring committees and funding bodies, and it is usually measured via their scientific productivity, citations, or a combined metric such as the h-index. assessing young researchers is more critical because it takes a while to get citations and increment of h-index. hence, predicting the h-index can help to discover the researchers’ scientific impact. in addition, identifying the influential factors to predict the scientific impact is helpful for researchers and their organizations seeking solutions to improve it. this study investigates the effect of the author, paper/venue-specific features on the future h-index. for this purpose, we used a machine learning approach to predict the h-index and feature analysis techniques to advance the understanding of feature impact. utilizing the bibliometric data in scopus, we defined and extracted two main groups of features. the first relates to prior scientific impact, and we name it ‘prior impact-based features’ and includes the number of publications, received citations, and h-index. the second group is ‘non-prior impact-based features’ and contains the features related to author, co-authorship, paper, and venue characteristics. we explored their importance in predicting researchers’ h-index in three career phases. also, we examined the temporal dimension of predicting performance for different feature categories to find out which features are more reliable for long- and short-term prediction. we referred to the gender of the authors to examine the role of this author’s characteristics in the prediction task. our findings showed that gender has a very slight effect in predicting the h-index. although the results demonstrate better performance for the models containing prior impact-based features for all researchers’ groups in the near future, we found that non-prior impact-based features are more robust predictors for younger scholars in the long term. also, prior impact-based features lose their power to predict more than other features in the long term.",8
Question Answering Versus Named Entity Recognition for Extracting Unknown Datasets,"{""Dataset mention extraction is a difficult problem due to the unstructured nature of text, the sparsity of dataset mentions, and the various ways the same dataset can be mentioned. Extracting unknown dataset mentions which are not part of the training data of the model is even harder. We address this challenge in two ways. First, we consider a two-step approach where a binary classifier filters out positive contexts, i.e., detects sentences with a dataset mention. We consider multiple transformer-based models and strong baselines for this task. Subsequently, the dataset is extracted from the positive context. Second, we consider a one-step approach and directly aim to detect and extract a possible dataset mention. For the extraction of datasets, we consider transformer models in named entity recognition (NER) mode. We contrast NER with the transformers' capabilities for question answering (QA). We use the Coleridge Initiative 'Show US the Data' dataset consisting of 14.3k scientific papers with about 35k mentions of datasets. We found that using transformers in QA mode is a better choice than NER for extracting unknown datasets. The rationale is that detecting new datasets is an out-of-vocabulary task, i.e., the dataset name has not been seen once during training. Comparing the two-step versus the one-step approach, we found contrasting strengths. A two-step dataset extraction using an MLP for filtering and RoBERTa in QA mode extracts more dataset mentions than a one-step system, but at the cost of a lower F1-score of 62.7%. A one-step extraction with DeBERTa in QA achieves the highest F1-score of 92.88% at the cost of missing dataset mentions. We recommend the one-step approach for the case when accuracy is more important, and the two-step approach when there is a postprocessing mechanism for the extracted dataset mentions, e.g., a manual check. The source code is available at https://github.com/yousef-younes/dataset-mention-extraction.""}",2023,IEEE Access,"{""binary text classification"",""dataset mentions"",""named entity recognition"",""question answering""}",Computer Science (all),,,CS,2023,"dataset mention extraction is a difficult problem due to the unstructured nature of text, the sparsity of dataset mentions, and the various ways the same dataset can be mentioned. extracting unknown dataset mentions which are not part of the training data of the model is even harder. we address this challenge in two ways. first, we consider a two-step approach where a binary classifier filters out positive contexts, i.e., detects sentences with a dataset mention. we consider multiple transformer-based models and strong baselines for this task. subsequently, the dataset is extracted from the positive context. second, we consider a one-step approach and directly aim to detect and extract a possible dataset mention. for the extraction of datasets, we consider transformer models in named entity recognition (ner) mode. we contrast ner with the transformers' capabilities for question answering (qa). we use the coleridge initiative 'show us the data' dataset consisting of 14.3k scientific papers with about 35k mentions of datasets. we found that using transformers in qa mode is a better choice than ner for extracting unknown datasets. the rationale is that detecting new datasets is an out-of-vocabulary task, i.e., the dataset name has not been seen once during training. comparing the two-step versus the one-step approach, we found contrasting strengths. a two-step dataset extraction using an mlp for filtering and roberta in qa mode extracts more dataset mentions than a one-step system, but at the cost of a lower f1-score of 62.7%. a one-step extraction with deberta in qa achieves the highest f1-score of 92.88% at the cost of missing dataset mentions. we recommend the one-step approach for the case when accuracy is more important, and the two-step approach when there is a postprocessing mechanism for the extracted dataset mentions, e.g., a manual check. the source code is available at https://github.com/yousef-younes/dataset-mention-extraction.",0
Dynamic global structure enhanced multi-channel graph neural network for session-based recommendation,"{""Session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. Most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. Meanwhile, previous works usually apply GNN to capture the transformation relationship between items, however the graph used in GNN is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. In this paper, we propose a novel method called Dynamic Global Structure Enhanced Multi-channel Graph Neural Network (DGS-MGNN) to learn accurate representations of items from multiple perspectives. In DGS-MGNN, we propose a novel GNN model named Multi-channel Graph Neural Network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. Meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. Finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. We conduct extensive experiments on three widely used datasets, and the results demonstrate that DGS-MGNN is consistently superior to the state-of-the-art baseline models.""}",2023,Information Sciences,"{""attention model"",""behavior modeling"",""graph neural network"",""recommendation system"",""representation learning"",""session-based recommendation""}",Information Systems and Management,Decision Sciences,Social Sciences & Humanities,CS,2023,"session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. meanwhile, previous works usually apply gnn to capture the transformation relationship between items, however the graph used in gnn is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. in this paper, we propose a novel method called dynamic global structure enhanced multi-channel graph neural network (dgs-mgnn) to learn accurate representations of items from multiple perspectives. in dgs-mgnn, we propose a novel gnn model named multi-channel graph neural network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. we conduct extensive experiments on three widely used datasets, and the results demonstrate that dgs-mgnn is consistently superior to the state-of-the-art baseline models.",4
Dynamic global structure enhanced multi-channel graph neural network for session-based recommendation,"{""Session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. Most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. Meanwhile, previous works usually apply GNN to capture the transformation relationship between items, however the graph used in GNN is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. In this paper, we propose a novel method called Dynamic Global Structure Enhanced Multi-channel Graph Neural Network (DGS-MGNN) to learn accurate representations of items from multiple perspectives. In DGS-MGNN, we propose a novel GNN model named Multi-channel Graph Neural Network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. Meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. Finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. We conduct extensive experiments on three widely used datasets, and the results demonstrate that DGS-MGNN is consistently superior to the state-of-the-art baseline models.""}",2023,Information Sciences,"{""attention model"",""behavior modeling"",""graph neural network"",""recommendation system"",""representation learning"",""session-based recommendation""}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2023,"session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. meanwhile, previous works usually apply gnn to capture the transformation relationship between items, however the graph used in gnn is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. in this paper, we propose a novel method called dynamic global structure enhanced multi-channel graph neural network (dgs-mgnn) to learn accurate representations of items from multiple perspectives. in dgs-mgnn, we propose a novel gnn model named multi-channel graph neural network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. we conduct extensive experiments on three widely used datasets, and the results demonstrate that dgs-mgnn is consistently superior to the state-of-the-art baseline models.",4
Using Double Machine Learning to Understand Nonresponse in the Recruitment of a Mixed-Mode Online Panel,"{""Survey scientists increasingly face the problem of high-dimensionality in their research as digitization makes it much easier to construct high-dimensional (or “big”) data sets through tools such as online surveys and mobile applications. Machine learning methods are able to handle such data, and they have been successfully applied to solve predictive problems. However, in many situations, survey statisticians want to learn about causal relationships to draw conclusions and be able to transfer the findings of one survey to another. Standard machine learning methods provide biased estimates of such relationships. We introduce into survey statistics the double machine learning approach, which gives approximately unbiased estimators of parameters of interest, and show how it can be used to analyze survey nonresponse in a high-dimensional panel setting. The double machine learning approach here assumes unconfoundedness of variables as its identification strategy. In high-dimensional settings, where the number of potential confounders to include in the model is too large, the double machine learning approach secures valid inference by selecting the relevant confounding variables.""}",2023,Social Science Computer Review,"{""causal inference"",""gesis panel"",""machine learning"",""panel dropout"",""survey nonresponse""}",Social Sciences (all),,,CS,2023,"survey scientists increasingly face the problem of high-dimensionality in their research as digitization makes it much easier to construct high-dimensional (or “big”) data sets through tools such as online surveys and mobile applications. machine learning methods are able to handle such data, and they have been successfully applied to solve predictive problems. however, in many situations, survey statisticians want to learn about causal relationships to draw conclusions and be able to transfer the findings of one survey to another. standard machine learning methods provide biased estimates of such relationships. we introduce into survey statistics the double machine learning approach, which gives approximately unbiased estimators of parameters of interest, and show how it can be used to analyze survey nonresponse in a high-dimensional panel setting. the double machine learning approach here assumes unconfoundedness of variables as its identification strategy. in high-dimensional settings, where the number of potential confounders to include in the model is too large, the double machine learning approach secures valid inference by selecting the relevant confounding variables.",5
Bibliometric-Enhanced Information Retrieval: 13th International BIR Workshop (BIR 2023),"{""The 13th iteration of the Bibliometric-enhanced Information Retrieval (BIR) workshop series will take place at ECIR 2023 as a full-day workshop. BIR tackles issues related to, for instance, academic search and recommendation, at the intersection of Information Retrieval, Natural Language Processing, and Bibliometrics. As an interdisciplinary scientific event, BIR brings together researchers and practitioners from the Scientometrics/Bibliometrics community on the one hand, and the Information Retrieval community on the other hand. BIR is an ever-growing topic investigated by both academia and the industry.""}",2023,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"{""academic search"",bibliometrics,""digital libraries"",""information retrieval"",scientometrics}",Computer Science (all),,,CS,2023,"the 13th iteration of the bibliometric-enhanced information retrieval (bir) workshop series will take place at ecir 2023 as a full-day workshop. bir tackles issues related to, for instance, academic search and recommendation, at the intersection of information retrieval, natural language processing, and bibliometrics. as an interdisciplinary scientific event, bir brings together researchers and practitioners from the scientometrics/bibliometrics community on the one hand, and the information retrieval community on the other hand. bir is an ever-growing topic investigated by both academia and the industry.",2
Goodness of fit tests for random multigraph models,"{""Goodness of fit tests for two probabilistic multigraph models are presented. The first model is random stub matching given fixed degrees (RSM) so that edge assignments to vertex pair sites are dependent, and the second is independent edge assignments (IEA) according to a common probability distribution. Tests are performed using goodness of fit measures between the edge multiplicity sequence of an observed multigraph, and the expected one according to a simple or composite hypothesis. Test statistics of Pearson type and of likelihood ratio type are used, and the expected values of the Pearson statistic under the different models are derived. Test performances based on simulations indicate that even for small number of edges, the null distributions of both statistics are well approximated by their asymptotic (Formula presented.) -distribution. The non-null distributions of the test statistics can be well approximated by proposed adjusted (Formula presented.) -distributions used for power approximations. The influence of RSM on both test statistics is substantial for small number of edges and implies a shift of their distributions towards smaller values compared to what holds true for the null distributions under IEA. Two applications on social networks are included to illustrate how the tests can guide in the analysis of social structure.""}",2023,Journal of Applied Statistics,"{""data aggregation"",""goodness of fit"",""multivariate networks"",""network model"",""random multigraphs"",""random stub matching""}","Statistics, Probability and Uncertainty",Decision Sciences,Social Sciences & Humanities,CS,2023,"goodness of fit tests for two probabilistic multigraph models are presented. the first model is random stub matching given fixed degrees (rsm) so that edge assignments to vertex pair sites are dependent, and the second is independent edge assignments (iea) according to a common probability distribution. tests are performed using goodness of fit measures between the edge multiplicity sequence of an observed multigraph, and the expected one according to a simple or composite hypothesis. test statistics of pearson type and of likelihood ratio type are used, and the expected values of the pearson statistic under the different models are derived. test performances based on simulations indicate that even for small number of edges, the null distributions of both statistics are well approximated by their asymptotic (formula presented.) -distribution. the non-null distributions of the test statistics can be well approximated by proposed adjusted (formula presented.) -distributions used for power approximations. the influence of rsm on both test statistics is substantial for small number of edges and implies a shift of their distributions towards smaller values compared to what holds true for the null distributions under iea. two applications on social networks are included to illustrate how the tests can guide in the analysis of social structure.",3
Dynamic global structure enhanced multi-channel graph neural network for session-based recommendation,"{""Session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. Most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. Meanwhile, previous works usually apply GNN to capture the transformation relationship between items, however the graph used in GNN is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. In this paper, we propose a novel method called Dynamic Global Structure Enhanced Multi-channel Graph Neural Network (DGS-MGNN) to learn accurate representations of items from multiple perspectives. In DGS-MGNN, we propose a novel GNN model named Multi-channel Graph Neural Network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. Meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. Finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. We conduct extensive experiments on three widely used datasets, and the results demonstrate that DGS-MGNN is consistently superior to the state-of-the-art baseline models.""}",2023,Information Sciences,"{""attention model"",""behavior modeling"",""graph neural network"",""recommendation system"",""representation learning"",""session-based recommendation""}",Control and Systems Engineering,Engineering,Physical Sciences,CS,2023,"session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. meanwhile, previous works usually apply gnn to capture the transformation relationship between items, however the graph used in gnn is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. in this paper, we propose a novel method called dynamic global structure enhanced multi-channel graph neural network (dgs-mgnn) to learn accurate representations of items from multiple perspectives. in dgs-mgnn, we propose a novel gnn model named multi-channel graph neural network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. we conduct extensive experiments on three widely used datasets, and the results demonstrate that dgs-mgnn is consistently superior to the state-of-the-art baseline models.",4
No deal: German researchers’ publishing and citing behaviors after Big Deal negotiations with Elsevier,"{""In 2014, a union of German research organizations established Projekt DEAL, a national-level project to negotiate licensing agreements with large scientific publishers. Negotiations between DEAL and Elsevier began in 2016, and broke down without a successful agreement in 2018; during this time, around 200 German research institutions canceled their license agreements with Elsevier, leading Elsevier to restrict journal access at those institutions.We investigated the effect on researchers’ publishing and citing behaviors from a bibliometric perspective, using a data set of ~400,000 articles published by researchers at DEAL institutions during 2012–2020. We further investigated these effects with respect to the timing of contract cancellations, research disciplines, collaboration patterns, and article open-access status. We find evidence for a decrease in Elsevier’s market share of articles from DEAL institutions, with the largest year-on-year market share decreases occurring from 2018 to 2020 following the implementation of access restrictions. We also observe year-on-year decreases in the proportion of citations, although the decrease is smaller. We conclude that negotiations with Elsevier and access restrictions have led to some reduced willingness to publish in Elsevier journals, but that researchers are not strongly affected in their ability to cite Elsevier articles, implying that researchers use other methods to access scientific literature.""}",2023,Quantitative Science Studies,"{bibliometrics,""big deals"",germany,""open access"",""scholarly publishing""}",Numerical Analysis,Mathematics,Physical Sciences,CS,2023,"in 2014, a union of german research organizations established projekt deal, a national-level project to negotiate licensing agreements with large scientific publishers. negotiations between deal and elsevier began in 2016, and broke down without a successful agreement in 2018; during this time, around 200 german research institutions canceled their license agreements with elsevier, leading elsevier to restrict journal access at those institutions.we investigated the effect on researchers’ publishing and citing behaviors from a bibliometric perspective, using a data set of ~400,000 articles published by researchers at deal institutions during 2012–2020. we further investigated these effects with respect to the timing of contract cancellations, research disciplines, collaboration patterns, and article open-access status. we find evidence for a decrease in elsevier’s market share of articles from deal institutions, with the largest year-on-year market share decreases occurring from 2018 to 2020 following the implementation of access restrictions. we also observe year-on-year decreases in the proportion of citations, although the decrease is smaller. we conclude that negotiations with elsevier and access restrictions have led to some reduced willingness to publish in elsevier journals, but that researchers are not strongly affected in their ability to cite elsevier articles, implying that researchers use other methods to access scientific literature.",2
EvoRecipes: A Generative Approach for Evolving Context-Aware Recipes,"{""Generative AI e.g. Large Language Models (LLMs) can be used to generate new recipes. However, LLMs struggle with more complex aspects like recipe semantics and process comprehension. Furthermore, LLMs have limited ability to account for user preferences since they are based on statistical patterns. As a result, these recipes may be invalid. Evolutionary algorithms inspired by the process of natural selection are optimization algorithms that use stochastic operators to generate new solutions. These algorithms can generate large number of solutions from the set of possible solution space. Moreover, these algorithms have the capability to incorporate user preferences in fitness function to generate novel recipes that are more aligned with the fitness objective. In this paper, we propose the EvoRecipes framework to generate novel recipes. The EvoRecipes framework utilizes both Genetic Algorithm and generative AI in addition to RecipeOn ontology, and RecipeKG knowledge graph. Genetic Algorithm explore the large solution space of encoded recipe solutions and are capable of incorporating user preferences, while LLMs are used to generate recipe text from encoded recipe solutions. EvoRecipes uses a population of context-aware recipe solutions from the RecipeKG knowledge graph. RecipeKG encodes recipes in RDF format using classes and properties as defined in the RecipeOn ontology. Moreover, to evaluate the alignment of EvoRecipe generated recipes with multiple intended objectives, we propose a fitness function that incorporates novelty, simplicity, visual appeal, and feasibility. Additionally, to evaluate the quality of the EvoRecipe generated recipes while considering the subjective nature of recipes, we conducted a survey using multi-dimensional metrics (i.e. contextual, procedural, and novelty). Results show that EvoRecipes generated recipes are novel, valid and incorporate user preferences.""}",2023,IEEE Access,"{""computational creativity"",food,""knowledge graph"",ontology,recipe,""recipe evolution""}",Materials Science (all),,,CS,2023,"generative ai e.g. large language models (llms) can be used to generate new recipes. however, llms struggle with more complex aspects like recipe semantics and process comprehension. furthermore, llms have limited ability to account for user preferences since they are based on statistical patterns. as a result, these recipes may be invalid. evolutionary algorithms inspired by the process of natural selection are optimization algorithms that use stochastic operators to generate new solutions. these algorithms can generate large number of solutions from the set of possible solution space. moreover, these algorithms have the capability to incorporate user preferences in fitness function to generate novel recipes that are more aligned with the fitness objective. in this paper, we propose the evorecipes framework to generate novel recipes. the evorecipes framework utilizes both genetic algorithm and generative ai in addition to recipeon ontology, and recipekg knowledge graph. genetic algorithm explore the large solution space of encoded recipe solutions and are capable of incorporating user preferences, while llms are used to generate recipe text from encoded recipe solutions. evorecipes uses a population of context-aware recipe solutions from the recipekg knowledge graph. recipekg encodes recipes in rdf format using classes and properties as defined in the recipeon ontology. moreover, to evaluate the alignment of evorecipe generated recipes with multiple intended objectives, we propose a fitness function that incorporates novelty, simplicity, visual appeal, and feasibility. additionally, to evaluate the quality of the evorecipe generated recipes while considering the subjective nature of recipes, we conducted a survey using multi-dimensional metrics (i.e. contextual, procedural, and novelty). results show that evorecipes generated recipes are novel, valid and incorporate user preferences.",0
CNIM-GCN: Consensus Neighbor Interaction-based Multi-channel Graph Convolutional Networks,"{""Node classification plays a critical role in numerous network applications, and has attracted increasing attention in recent years. Existing state-of-the-art studies aim at maintaining common information between the topology graph and the feature graph in an implicit way, i.e., adopting a common convolution with parameter sharing strategy to preserve common information between the two graphs. Despite their effectiveness, these studies are still far from satisfactory due to the complex correlation information between the two spaces. To address this issue, we present a novel method named Consensus Neighbor Interaction-based Multi-channel Graph Convolutional Networks (CNIM-GCN). CNIM-GCN preserves the common information between the feature space and topology space in an explicit way by introducing a consensus graph for information propagation. A multi-channel graph convolutional networks is developed for effectively fusing information from different graphs. In addition, we further incorporate two types of consistency constraints, i.e., structural consistency constraint and reconstruction consistency constraint, to maintain the consistency between different channels. The former is leveraged to keep the consistency between different spaces at the structural relationship level, while the latter is used to preserve a consistency between the final node representation and the original node feature representation. We carry out extensive experiments on five real-world datasets, including ACM, BlogCatalog, CiteSeer, Flickr and UAI2010. Experimental results show that our proposed approach CNIM-GCN is superior to the state-of-the-art baselines.""}",2023,Expert Systems with Applications,"{""deep learning"",""graph convolutional networks"",""network representation learning"",""node classification""}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2023,"node classification plays a critical role in numerous network applications, and has attracted increasing attention in recent years. existing state-of-the-art studies aim at maintaining common information between the topology graph and the feature graph in an implicit way, i.e., adopting a common convolution with parameter sharing strategy to preserve common information between the two graphs. despite their effectiveness, these studies are still far from satisfactory due to the complex correlation information between the two spaces. to address this issue, we present a novel method named consensus neighbor interaction-based multi-channel graph convolutional networks (cnim-gcn). cnim-gcn preserves the common information between the feature space and topology space in an explicit way by introducing a consensus graph for information propagation. a multi-channel graph convolutional networks is developed for effectively fusing information from different graphs. in addition, we further incorporate two types of consistency constraints, i.e., structural consistency constraint and reconstruction consistency constraint, to maintain the consistency between different channels. the former is leveraged to keep the consistency between different spaces at the structural relationship level, while the latter is used to preserve a consistency between the final node representation and the original node feature representation. we carry out extensive experiments on five real-world datasets, including acm, blogcatalog, citeseer, flickr and uai2010. experimental results show that our proposed approach cnim-gcn is superior to the state-of-the-art baselines.",4
Re-Think Before You Share: A Comprehensive Study on Prioritizing Check-Worthy Claims,"{""The massive amount of misinformation spreading on the internet on a daily basis has enormous negative impacts on societies. Therefore, we need systems to help fact-checkers to combat misinformation and to raise public awareness of this important problem. In this article, we propose a hybrid model which combines bidirectional encoder representations from transformer (BERT) model with various features to prioritize claims based on their check-worthiness. Features we use include domain-specific controversial topics (CT), word embeddings (WE), part-of-speech (POS) tags, and others. In addition, we explore various ways of increasing labeled data size to effectively train the models, such as increasing positive (IncPos) samples, active learning (AL), and utilizing labeled data in other languages. In our extensive experiments, we show that our model outperforms all state-of-the-art models in test collections of Conference and Labs of Evaluation Forum (CLEF) CheckThat! Lab (CTL) 2018 and 2019. In addition, when positive samples are increased in the training set, our model achieves the best mean average precision (MAP) score reported so far for the test collection of CTL 2020. Furthermore, we show that cross-lingual training is effective for prioritizing Arabic and Turkish claims, but not for English.""}",2023,IEEE Transactions on Computational Social Systems,"{""check-worthy claims"",fact-checking,misinformation}",Human-Computer Interaction,Computer Science,Physical Sciences,CS,2023,"the massive amount of misinformation spreading on the internet on a daily basis has enormous negative impacts on societies. therefore, we need systems to help fact-checkers to combat misinformation and to raise public awareness of this important problem. in this article, we propose a hybrid model which combines bidirectional encoder representations from transformer (bert) model with various features to prioritize claims based on their check-worthiness. features we use include domain-specific controversial topics (ct), word embeddings (we), part-of-speech (pos) tags, and others. in addition, we explore various ways of increasing labeled data size to effectively train the models, such as increasing positive (incpos) samples, active learning (al), and utilizing labeled data in other languages. in our extensive experiments, we show that our model outperforms all state-of-the-art models in test collections of conference and labs of evaluation forum (clef) checkthat! lab (ctl) 2018 and 2019. in addition, when positive samples are increased in the training set, our model achieves the best mean average precision (map) score reported so far for the test collection of ctl 2020. furthermore, we show that cross-lingual training is effective for prioritizing arabic and turkish claims, but not for english.",1
Re-Think Before You Share: A Comprehensive Study on Prioritizing Check-Worthy Claims,"{""The massive amount of misinformation spreading on the internet on a daily basis has enormous negative impacts on societies. Therefore, we need systems to help fact-checkers to combat misinformation and to raise public awareness of this important problem. In this article, we propose a hybrid model which combines bidirectional encoder representations from transformer (BERT) model with various features to prioritize claims based on their check-worthiness. Features we use include domain-specific controversial topics (CT), word embeddings (WE), part-of-speech (POS) tags, and others. In addition, we explore various ways of increasing labeled data size to effectively train the models, such as increasing positive (IncPos) samples, active learning (AL), and utilizing labeled data in other languages. In our extensive experiments, we show that our model outperforms all state-of-the-art models in test collections of Conference and Labs of Evaluation Forum (CLEF) CheckThat! Lab (CTL) 2018 and 2019. In addition, when positive samples are increased in the training set, our model achieves the best mean average precision (MAP) score reported so far for the test collection of CTL 2020. Furthermore, we show that cross-lingual training is effective for prioritizing Arabic and Turkish claims, but not for English.""}",2023,IEEE Transactions on Computational Social Systems,"{""check-worthy claims"",fact-checking,misinformation}",Modeling and Simulation,Mathematics,Physical Sciences,CS,2023,"the massive amount of misinformation spreading on the internet on a daily basis has enormous negative impacts on societies. therefore, we need systems to help fact-checkers to combat misinformation and to raise public awareness of this important problem. in this article, we propose a hybrid model which combines bidirectional encoder representations from transformer (bert) model with various features to prioritize claims based on their check-worthiness. features we use include domain-specific controversial topics (ct), word embeddings (we), part-of-speech (pos) tags, and others. in addition, we explore various ways of increasing labeled data size to effectively train the models, such as increasing positive (incpos) samples, active learning (al), and utilizing labeled data in other languages. in our extensive experiments, we show that our model outperforms all state-of-the-art models in test collections of conference and labs of evaluation forum (clef) checkthat! lab (ctl) 2018 and 2019. in addition, when positive samples are increased in the training set, our model achieves the best mean average precision (map) score reported so far for the test collection of ctl 2020. furthermore, we show that cross-lingual training is effective for prioritizing arabic and turkish claims, but not for english.",1
UnScientify: Detecting Scientific Uncertainty in Scholarly Full Text,"{""This demo paper presents UnScientify (https://bit.ly/unscientify-demo), an interactive system designed to detect scientific uncertainty in scholarly full text. The system utilizes a weakly supervised technique that employs a fine-grained annotation scheme to identify verbally formulated uncertainty at the sentence level in scientific texts. The pipeline for the system includes a combination of pattern matching, complex sentence checking, and authorial reference checking. Our approach automates labeling and annotation tasks for scientific uncertainty identification, taking into account different types of scientific uncertainty, that can serve various applications such as information retrieval, text mining, and scholarly document processing. Additionally, UnScientify provides interpretable results, aiding in the comprehension of identified instances of scientific uncertainty in text.""}",2023,CEUR Workshop Proceedings,"{""authorial reference"",""fine-grained annotation"",""label automation"",""pattern matching"",""scholarly document processing"",""scientific uncertainty"",""text mining""}",Computer Science (all),,,CS,2023,"this demo paper presents unscientify (https://bit.ly/unscientify-demo), an interactive system designed to detect scientific uncertainty in scholarly full text. the system utilizes a weakly supervised technique that employs a fine-grained annotation scheme to identify verbally formulated uncertainty at the sentence level in scientific texts. the pipeline for the system includes a combination of pattern matching, complex sentence checking, and authorial reference checking. our approach automates labeling and annotation tasks for scientific uncertainty identification, taking into account different types of scientific uncertainty, that can serve various applications such as information retrieval, text mining, and scholarly document processing. additionally, unscientify provides interpretable results, aiding in the comprehension of identified instances of scientific uncertainty in text.",-1
Shall androids dream of genocides? How generative AI can change the future of memorialization of mass atrocities,"{""The memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. Digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. At the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. The emergence of generative forms of artificial intelligence (AI), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. AI can identify patterns in training data to create new narratives for representing and interpreting mass atrocities—and do so in a fraction of the time it takes for humans. The use of generative AI in this context raises numerous questions: For example, can the paucity of training data on mass atrocities distort how AI interprets some atrocity-related inquiries? How important is the ability to differentiate between human- and AI-made content concerning mass atrocities? Can AI-made content be used to promote false information concerning atrocities? This article addresses these and other questions by examining the opportunities and risks associated with using generative AIs for memorializing mass atrocities. It also discusses recommendations for AIs integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.""}",2023,Discover Artificial Intelligence,,Artificial Intelligence,Computer Science,Physical Sciences,CS,2023,"the memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. at the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. the emergence of generative forms of artificial intelligence (ai), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. ai can identify patterns in training data to create new narratives for representing and interpreting mass atrocities—and do so in a fraction of the time it takes for humans. the use of generative ai in this context raises numerous questions: for example, can the paucity of training data on mass atrocities distort how ai interprets some atrocity-related inquiries? how important is the ability to differentiate between human- and ai-made content concerning mass atrocities? can ai-made content be used to promote false information concerning atrocities? this article addresses these and other questions by examining the opportunities and risks associated with using generative ais for memorializing mass atrocities. it also discusses recommendations for ais integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.",1
Stories and personal experiences in the COVID-19 Discourse,"{""Storytelling, i.e., the use of of anecdotes and personal experiences, plays a crucial role in everyday argumentation. This is particularly true for the highly controversial debates that spark in times of crisis - where the focus of the discussion is on heterogeneous aspects of everyday life. For individuals, stories can have a strong persuasive power; for a larger collective, stories can help decision-makers to develop strategies for addressing the challenges people are facing, especially in times of crisis. In this paper, we analyse the use of storytelling in the COVID-19 discourse. We carry out our analysis on three publicly available Reddit datasets, for a total of 367K comments. We automatically annotate the Reddit datasets by detecting spans containing storytelling and classifying them into: a) personal vs. general: is the story experienced by the speaker? b) argumentative function: does the story clarify a problem, potentially consisting in harm to a specific group? Does it exemplify a solution to a problem, or does it establish the credibility of the speaker?), and c) topic. We then carry out an analysis which establishes the relevance of storytelling in the COVID discourse and further uncovers interactions between topics and types of stories associated to them.""}",2024,"2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings",,Theoretical Computer Science,Mathematics,Physical Sciences,CS,2024,"storytelling, i.e., the use of of anecdotes and personal experiences, plays a crucial role in everyday argumentation. this is particularly true for the highly controversial debates that spark in times of crisis - where the focus of the discussion is on heterogeneous aspects of everyday life. for individuals, stories can have a strong persuasive power; for a larger collective, stories can help decision-makers to develop strategies for addressing the challenges people are facing, especially in times of crisis. in this paper, we analyse the use of storytelling in the covid-19 discourse. we carry out our analysis on three publicly available reddit datasets, for a total of 367k comments. we automatically annotate the reddit datasets by detecting spans containing storytelling and classifying them into: a) personal vs. general: is the story experienced by the speaker? b) argumentative function: does the story clarify a problem, potentially consisting in harm to a specific group? does it exemplify a solution to a problem, or does it establish the credibility of the speaker?), and c) topic. we then carry out an analysis which establishes the relevance of storytelling in the covid discourse and further uncovers interactions between topics and types of stories associated to them.",7
Analysis of Web Browsing Data: A Guide,"{""The use of individual-level browsing data, that is, the records of a person’s visits to online content through a desktop or mobile browser, is of increasing importance for social scientists. Browsing data have characteristics that raise many questions for statistical analysis, yet to date, little hands-on guidance on how to handle them exists. Reviewing extant research, and exploring data sets collected by our four research teams spanning seven countries and several years, with over 14,000 participants and 360 million web visits, we derive recommendations along four steps: preprocessing the raw data; filtering out observations; classifying web visits; and modelling browsing behavior. The recommendations we formulate aim to foster best practices in the field, which so far has paid little attention to justifying the many decisions researchers need to take when analyzing web browsing data.""}",2024,Social Science Computer Review,"{""computational social science"",""digital trace data"",""web browsing data"",""web tracking data""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"the use of individual-level browsing data, that is, the records of a person’s visits to online content through a desktop or mobile browser, is of increasing importance for social scientists. browsing data have characteristics that raise many questions for statistical analysis, yet to date, little hands-on guidance on how to handle them exists. reviewing extant research, and exploring data sets collected by our four research teams spanning seven countries and several years, with over 14,000 participants and 360 million web visits, we derive recommendations along four steps: preprocessing the raw data; filtering out observations; classifying web visits; and modelling browsing behavior. the recommendations we formulate aim to foster best practices in the field, which so far has paid little attention to justifying the many decisions researchers need to take when analyzing web browsing data.",5
TACO - Twitter Arguments from COnversations,"{""Twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. Argument mining is an important analytical task for processing and understanding online discourse. Specifically, it aims to identify the structural elements of arguments, denoted as information and inference. These elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on Twitter. We contribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets covering 200 entire COnversations spanning six heterogeneous topics annotated with an agreement of 0.718 Krippendorff's α among six experts. Second, we provide our annotation framework, incorporating definitions from the Cambridge Dictionary, to define and identify argument components on Twitter. Our transformer-based classifier achieves an 85.06% macro F1 baseline score in detecting arguments. Moreover, our data reveals that Twitter users tend to engage in discussions involving informed inferences and information. TACO serves multiple purposes, such as training tweet classifiers to manage tweets based on inference and information elements, while also providing valuable insights into the conversational reply patterns of tweets.""}",2024,"2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings","{""argument mining"",inference,""information extraction"",resource,""twitter conversations""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. argument mining is an important analytical task for processing and understanding online discourse. specifically, it aims to identify the structural elements of arguments, denoted as information and inference. these elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on twitter. we contribute taco, the first dataset of twitter arguments utilizing 1,814 tweets covering 200 entire conversations spanning six heterogeneous topics annotated with an agreement of 0.718 krippendorff's α among six experts. second, we provide our annotation framework, incorporating definitions from the cambridge dictionary, to define and identify argument components on twitter. our transformer-based classifier achieves an 85.06% macro f1 baseline score in detecting arguments. moreover, our data reveals that twitter users tend to engage in discussions involving informed inferences and information. taco serves multiple purposes, such as training tweet classifiers to manage tweets based on inference and information elements, while also providing valuable insights into the conversational reply patterns of tweets.",-1
[PromptEng] First International Workshop on Prompt Engineering for Pre-Trained Language Models,"{""The recent achievements and availability of Large Language Models have paved the road to a new range of applications and use-cases. Pre-trained language models are now being involved at-scale in many fields where they were until now absent from. More specifically, the progress made by causal generative models has open the door to using them through textual instructions aka. prompts. Unfortunately, the performances of these prompts are highly dependent on the exact phrasing used and therefore practitioners need to adopt fail-retry strategies. This first international workshop on prompt engineering aims at gathering practitioners (both from Academia and Industry) to exchange about good practices, optimizations, results and novel paradigms about the design of efficient prompts to make use of LLMs.""}",2024,WWW 2024 Companion - Companion Proceedings of the ACM Web Conference,"{""best practices"",""collective task"",llm,""prompt engineering""}",Computer Networks and Communications,Computer Science,Physical Sciences,CS,2024,"the recent achievements and availability of large language models have paved the road to a new range of applications and use-cases. pre-trained language models are now being involved at-scale in many fields where they were until now absent from. more specifically, the progress made by causal generative models has open the door to using them through textual instructions aka. prompts. unfortunately, the performances of these prompts are highly dependent on the exact phrasing used and therefore practitioners need to adopt fail-retry strategies. this first international workshop on prompt engineering aims at gathering practitioners (both from academia and industry) to exchange about good practices, optimizations, results and novel paradigms about the design of efficient prompts to make use of llms.",0
On the Anatomy of Real-World R Code for Static Analysis,"{""Context The R programming language has a huge and active community, especially in the area of statistical computing. Its interpreted nature allows for several interesting constructs, like the manipulation of functions at run-time, that hinder the static analysis of R programs. At the same time, there is a lack of existing research regarding how these features, or even the R language as a whole are used in practice. Objective In this paper, we conduct a large-scale, static analysis of more than 50 million lines of real- world R programs and packages to identify their characteristics and the features that are actually used. Moreover, we compare the similarities and differences between the scripts of R users and the implementations of package authors. We provide insights for static analysis tools like the lintr package as well as potential interpreter optimizations and uncover areas for future research. Method We analyze 4 230 R scripts submitted alongside publications and the sources of 19 450 CRAN packages for over 350 000 R files, collecting and summarizing quantitative information for features of interest. Results We find a high frequency of name-based indexing operations, assignments, and loops, but a low frequency for most of R's reflective functions. Furthermore, we find neither testing functions nor many calls to R's foreign function interface (FFI) in the publication submissions. Conclusion R scripts and package sources differ, for example, in their size, the way they include other packages, and their usage of R's reflective capabilities. We provide features that are used frequently and should be prioritized by static analysis tools, like operator assignments, function calls, and certain reflective functions like loadCCS CONCEPTS•General and reference → Empirical studies; • Software and its engineering → Language features.""}",2024,"Proceedings - 2024 IEEE/ACM 21st International Conference on Mining Software Repositories, MSR 2024","{""language feature usage"",""large-scale static analysis"",""r programming language""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"context the r programming language has a huge and active community, especially in the area of statistical computing. its interpreted nature allows for several interesting constructs, like the manipulation of functions at run-time, that hinder the static analysis of r programs. at the same time, there is a lack of existing research regarding how these features, or even the r language as a whole are used in practice. objective in this paper, we conduct a large-scale, static analysis of more than 50 million lines of real- world r programs and packages to identify their characteristics and the features that are actually used. moreover, we compare the similarities and differences between the scripts of r users and the implementations of package authors. we provide insights for static analysis tools like the lintr package as well as potential interpreter optimizations and uncover areas for future research. method we analyze 4 230 r scripts submitted alongside publications and the sources of 19 450 cran packages for over 350 000 r files, collecting and summarizing quantitative information for features of interest. results we find a high frequency of name-based indexing operations, assignments, and loops, but a low frequency for most of r's reflective functions. furthermore, we find neither testing functions nor many calls to r's foreign function interface (ffi) in the publication submissions. conclusion r scripts and package sources differ, for example, in their size, the way they include other packages, and their usage of r's reflective capabilities. we provide features that are used frequently and should be prioritized by static analysis tools, like operator assignments, function calls, and certain reflective functions like loadccs concepts•general and reference → empirical studies; • software and its engineering → language features.",0
A three-way decision approach for dynamically expandable networks,"{""Conventional deep learning models are designed to work on a single task. They are required to be trained from scratch each time new tasks are added. This leads to overhead in training time. Continual deep learning models with dynamically expandable network architecture aim to handle this issue. The key idea in these models is to find a balance between the properties of stability (preserving the learned information) and plasticity (updating and accommodating the new information) also sometimes referred to as the stability-plasticity dilemma. The stability and plasticity of the model critically depends on three-way division of nodes into freeze, partially regularize and duplicate nodes. Freezing more nodes result in high stability but typically low plasticity. On the other hand, duplicating more nodes result in high plasticity but may not have an effective stability. In this paper, we introduce an approach called three-way decisions based dynamically expandable networks or 3WDDEN and its memory-based version called 3WDDEN-replay. The proposed approaches use game-theoretic rough sets to determine effective thresholds for three-way division of nodes by considering a tradeoff game between stability and plasticity. Experimental results of 3WDDEN on MNIST variant datasets show an overall improvement of 3.8% in accuracy compared to standard dynamically expandable network approach or DEN. 3WDDEN-replay further adds to accuracy with additional memory cost.""}",2024,International Journal of Approximate Reasoning,"{""continual deep learning"",""dynamically expandable networks"",""game-theoretic rough sets""}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2024,"conventional deep learning models are designed to work on a single task. they are required to be trained from scratch each time new tasks are added. this leads to overhead in training time. continual deep learning models with dynamically expandable network architecture aim to handle this issue. the key idea in these models is to find a balance between the properties of stability (preserving the learned information) and plasticity (updating and accommodating the new information) also sometimes referred to as the stability-plasticity dilemma. the stability and plasticity of the model critically depends on three-way division of nodes into freeze, partially regularize and duplicate nodes. freezing more nodes result in high stability but typically low plasticity. on the other hand, duplicating more nodes result in high plasticity but may not have an effective stability. in this paper, we introduce an approach called three-way decisions based dynamically expandable networks or 3wdden and its memory-based version called 3wdden-replay. the proposed approaches use game-theoretic rough sets to determine effective thresholds for three-way division of nodes by considering a tradeoff game between stability and plasticity. experimental results of 3wdden on mnist variant datasets show an overall improvement of 3.8% in accuracy compared to standard dynamically expandable network approach or den. 3wdden-replay further adds to accuracy with additional memory cost.",0
A mutually enhanced multi-scale relation-aware graph convolutional network for argument pair extraction,"{""Argument pair extraction (APE) is a fine-grained task of argument mining which aims to identify arguments offered by different participants in some discourse and detect interaction relationships between arguments from different participants. In recent years, many research efforts have been devoted to dealing with APE in a multi-task learning framework. Although these approaches have achieved encouraging results, they still face several challenging issues. First, different types of sentence relationships as well as different levels of information exchange among sentences are largely ignored. Second, they solely model interactions between argument pairs either in an explicit or implicit strategy, while neglecting the complementary effect of the two strategies. In this paper, we propose a novel Mutually Enhanced Multi-Scale Relation-Aware Graph Convolutional Network (MMR-GCN) for APE. Specifically, we first design a multi-scale relation-aware graph aggregation module to explicitly model the complex relationships between review and rebuttal passage sentences. In addition, we propose a mutually enhancement transformer module to implicitly and interactively enhance representations of review and rebuttal passage sentences. We experimentally validate MMR-GCN by comparing with the state-of-the-art APE methods. Experimental results show that it considerably outperforms all baseline methods, and the relative performance improvement of MMR-GCN over the best performing baseline MRC-APE in terms of F1 score reaches to 3.48% and 4.43% on the two benchmark datasets, respectively.""}",2024,Journal of Intelligent Information Systems,"{""argument mining"",""argument pair extraction"",""graph convolutional network"",transformer}",Hardware and Architecture,Computer Science,Physical Sciences,CS,2024,"argument pair extraction (ape) is a fine-grained task of argument mining which aims to identify arguments offered by different participants in some discourse and detect interaction relationships between arguments from different participants. in recent years, many research efforts have been devoted to dealing with ape in a multi-task learning framework. although these approaches have achieved encouraging results, they still face several challenging issues. first, different types of sentence relationships as well as different levels of information exchange among sentences are largely ignored. second, they solely model interactions between argument pairs either in an explicit or implicit strategy, while neglecting the complementary effect of the two strategies. in this paper, we propose a novel mutually enhanced multi-scale relation-aware graph convolutional network (mmr-gcn) for ape. specifically, we first design a multi-scale relation-aware graph aggregation module to explicitly model the complex relationships between review and rebuttal passage sentences. in addition, we propose a mutually enhancement transformer module to implicitly and interactively enhance representations of review and rebuttal passage sentences. we experimentally validate mmr-gcn by comparing with the state-of-the-art ape methods. experimental results show that it considerably outperforms all baseline methods, and the relative performance improvement of mmr-gcn over the best performing baseline mrc-ape in terms of f1 score reaches to 3.48% and 4.43% on the two benchmark datasets, respectively.",6
Stories and personal experiences in the COVID-19 Discourse,"{""Storytelling, i.e., the use of of anecdotes and personal experiences, plays a crucial role in everyday argumentation. This is particularly true for the highly controversial debates that spark in times of crisis - where the focus of the discussion is on heterogeneous aspects of everyday life. For individuals, stories can have a strong persuasive power; for a larger collective, stories can help decision-makers to develop strategies for addressing the challenges people are facing, especially in times of crisis. In this paper, we analyse the use of storytelling in the COVID-19 discourse. We carry out our analysis on three publicly available Reddit datasets, for a total of 367K comments. We automatically annotate the Reddit datasets by detecting spans containing storytelling and classifying them into: a) personal vs. general: is the story experienced by the speaker? b) argumentative function: does the story clarify a problem, potentially consisting in harm to a specific group? Does it exemplify a solution to a problem, or does it establish the credibility of the speaker?), and c) topic. We then carry out an analysis which establishes the relevance of storytelling in the COVID discourse and further uncovers interactions between topics and types of stories associated to them.""}",2024,"2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings",,Computational Theory and Mathematics,Computer Science,Physical Sciences,CS,2024,"storytelling, i.e., the use of of anecdotes and personal experiences, plays a crucial role in everyday argumentation. this is particularly true for the highly controversial debates that spark in times of crisis - where the focus of the discussion is on heterogeneous aspects of everyday life. for individuals, stories can have a strong persuasive power; for a larger collective, stories can help decision-makers to develop strategies for addressing the challenges people are facing, especially in times of crisis. in this paper, we analyse the use of storytelling in the covid-19 discourse. we carry out our analysis on three publicly available reddit datasets, for a total of 367k comments. we automatically annotate the reddit datasets by detecting spans containing storytelling and classifying them into: a) personal vs. general: is the story experienced by the speaker? b) argumentative function: does the story clarify a problem, potentially consisting in harm to a specific group? does it exemplify a solution to a problem, or does it establish the credibility of the speaker?), and c) topic. we then carry out an analysis which establishes the relevance of storytelling in the covid discourse and further uncovers interactions between topics and types of stories associated to them.",7
Bibliometric-Enhanced Information Retrieval: 14th International BIR Workshop (BIR 2024),"{""The 14th iteration of the Bibliometric-enhanced Information Retrieval (BIR) workshop series takes place at ECIR 2024 as a full-day workshop. BIR addresses research topics related to academic search and recommendation, at the intersection of Information Retrieval, Natural Language Processing, and Bibliometrics. As an interdisciplinary scientific event, BIR brings together researchers and practitioners from the Scientometrics/Bibliometrics community on the one hand, and the Information Retrieval and NLP communities on the other hand. BIR is an ever-growing topic investigated by both academia and the industry.""}",2024,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"{""academic search"",bibliometrics,""digital libraries"",""information retrieval"",scientometrics}",Computer Science (all),,,CS,2024,"the 14th iteration of the bibliometric-enhanced information retrieval (bir) workshop series takes place at ecir 2024 as a full-day workshop. bir addresses research topics related to academic search and recommendation, at the intersection of information retrieval, natural language processing, and bibliometrics. as an interdisciplinary scientific event, bir brings together researchers and practitioners from the scientometrics/bibliometrics community on the one hand, and the information retrieval and nlp communities on the other hand. bir is an ever-growing topic investigated by both academia and the industry.",2
Computational reproducibility in computational social science,"{""Open science practices have been widely discussed and have been implemented with varying success in different disciplines. We argue that computational-x disciplines such as computational social science, are also susceptible to the symptoms of the crises, but in terms of reproducibility. We expand the binary definition of reproducibility into a tier system which allows increasing levels of reproducibility based on external verifiability to counteract the practice of open-washing. We provide solutions for barriers in Computational Social Science that hinder researchers from obtaining the highest level of reproducibility, including the use of alternate data sources and considering reproducibility proactively.""}",2024,EPJ Data Science,"{""computational social science"",""open science"",""replicability crisis"",reproducibility}",Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"open science practices have been widely discussed and have been implemented with varying success in different disciplines. we argue that computational-x disciplines such as computational social science, are also susceptible to the symptoms of the crises, but in terms of reproducibility. we expand the binary definition of reproducibility into a tier system which allows increasing levels of reproducibility based on external verifiability to counteract the practice of open-washing. we provide solutions for barriers in computational social science that hinder researchers from obtaining the highest level of reproducibility, including the use of alternate data sources and considering reproducibility proactively.",3
Multilingual Bot Accusations: How Different Linguistic Contexts Shape Perceptions of Social Bots,"{""Recent research indicates that the online use of the term \""bot\""has evolved over time. In the past, people used the term to accuse others of displaying automated behavior. However, it has gradually transformed into a linguistic tool to dehumanize the conversation partner, particularly on polarizing topics. Although this trend has been observed in English-speaking contexts, it is still unclear whether it holds true in other socio-linguistic environments. In this work we extend existing work on bot accusations and explore the phenomenon in a multilingual setting. We identify three distinct accusation patterns that characterize the different languages.""}",2024,"CPSS 2024 - 4th Workshop on Computational Linguistics for the Political and Social Sciences, Proceedings of the Workshop",,Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"recent research indicates that the online use of the term \""bot\""has evolved over time. in the past, people used the term to accuse others of displaying automated behavior. however, it has gradually transformed into a linguistic tool to dehumanize the conversation partner, particularly on polarizing topics. although this trend has been observed in english-speaking contexts, it is still unclear whether it holds true in other socio-linguistic environments. in this work we extend existing work on bot accusations and explore the phenomenon in a multilingual setting. we identify three distinct accusation patterns that characterize the different languages.",7
Introducing the Validation of Data Quality Indicators Through Re-Classification: The example of SQP and pretest surveys,"{""The present study introduces the concept of validating data quality indicators through re-classification. We use the term re-classification to mean the evaluation of how well an indicator detects the quality of different versions of a survey question for which the quality is known a priori. We illustrate its application with two examples. In both, we make use of 12 questions from prior experiments that manipulated text features of questions to create ‘low’ and ‘high’ quality versions of each question. In the first example, we coded each question version in SQP 2.1 to obtain indicators of validity, reliability, and quality. We compared these indicators between the two versions of each question to assess whether the SQP outcomes were sensitive to text features. In the second example, we used a pretest survey to obtain three indicators of survey quality: response latencies, item nonresponse, and consistency over time. Again, we compared these indicators between question versions to assess whether the indicators were sensitive to text features. We give recommendations for applying re-classification and an outlook for future research opportunities.""}",2024,International Journal of Market Research,"{""data quality"",pretesting,re-classification,""survey experiments"",""survey questions"",validation}",Business and International Management,"Business, Management and Accounting",Social Sciences & Humanities,CS,2024,"the present study introduces the concept of validating data quality indicators through re-classification. we use the term re-classification to mean the evaluation of how well an indicator detects the quality of different versions of a survey question for which the quality is known a priori. we illustrate its application with two examples. in both, we make use of 12 questions from prior experiments that manipulated text features of questions to create ‘low’ and ‘high’ quality versions of each question. in the first example, we coded each question version in sqp 2.1 to obtain indicators of validity, reliability, and quality. we compared these indicators between the two versions of each question to assess whether the sqp outcomes were sensitive to text features. in the second example, we used a pretest survey to obtain three indicators of survey quality: response latencies, item nonresponse, and consistency over time. again, we compared these indicators between question versions to assess whether the indicators were sensitive to text features. we give recommendations for applying re-classification and an outlook for future research opportunities.",8
A three-way decision approach for dynamically expandable networks,"{""Conventional deep learning models are designed to work on a single task. They are required to be trained from scratch each time new tasks are added. This leads to overhead in training time. Continual deep learning models with dynamically expandable network architecture aim to handle this issue. The key idea in these models is to find a balance between the properties of stability (preserving the learned information) and plasticity (updating and accommodating the new information) also sometimes referred to as the stability-plasticity dilemma. The stability and plasticity of the model critically depends on three-way division of nodes into freeze, partially regularize and duplicate nodes. Freezing more nodes result in high stability but typically low plasticity. On the other hand, duplicating more nodes result in high plasticity but may not have an effective stability. In this paper, we introduce an approach called three-way decisions based dynamically expandable networks or 3WDDEN and its memory-based version called 3WDDEN-replay. The proposed approaches use game-theoretic rough sets to determine effective thresholds for three-way division of nodes by considering a tradeoff game between stability and plasticity. Experimental results of 3WDDEN on MNIST variant datasets show an overall improvement of 3.8% in accuracy compared to standard dynamically expandable network approach or DEN. 3WDDEN-replay further adds to accuracy with additional memory cost.""}",2024,International Journal of Approximate Reasoning,"{""continual deep learning"",""dynamically expandable networks"",""game-theoretic rough sets""}",Software,Computer Science,Physical Sciences,CS,2024,"conventional deep learning models are designed to work on a single task. they are required to be trained from scratch each time new tasks are added. this leads to overhead in training time. continual deep learning models with dynamically expandable network architecture aim to handle this issue. the key idea in these models is to find a balance between the properties of stability (preserving the learned information) and plasticity (updating and accommodating the new information) also sometimes referred to as the stability-plasticity dilemma. the stability and plasticity of the model critically depends on three-way division of nodes into freeze, partially regularize and duplicate nodes. freezing more nodes result in high stability but typically low plasticity. on the other hand, duplicating more nodes result in high plasticity but may not have an effective stability. in this paper, we introduce an approach called three-way decisions based dynamically expandable networks or 3wdden and its memory-based version called 3wdden-replay. the proposed approaches use game-theoretic rough sets to determine effective thresholds for three-way division of nodes by considering a tradeoff game between stability and plasticity. experimental results of 3wdden on mnist variant datasets show an overall improvement of 3.8% in accuracy compared to standard dynamically expandable network approach or den. 3wdden-replay further adds to accuracy with additional memory cost.",0
SOMD@NSLP2024: Overview and Insights from the Software Mention Detection Shared Task,"{""Software is a central part of the scientific process and involved in obtaining, analysing, visualising and processing research data. Understanding the provenance of research requires an understanding of the involved software. However, software citations in scientific publications often are informal, what creates challenges when aiming at understanding software adoption. This paper provides an overview of the Software Mention Detection (SOMD) shared task conducted as part of the 2024 Natural Scientific Language Processing Workshop, aiming at advancing the state-of-the-art with respect to NLP methods for detecting software mentions and additional information in scholarly publications. The SOMD shared task encompasses three subtasks, concerned with software mention recognition (subtask I), recognition of additional information (subtask II) and classification of involved relations (subtask III). We present an overview of the tasks, received submissions and used techniques. The best submissions achieved F1 scores of 0.74 (subtask I), 0.838 (subtask II) and 0.911 (subtask III) indicating both task feasibility but also potential for further performance gains.""}",2024,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"{""information extraction"",""relation classification"",""scholarly information processing"",""software mention extraction"",""software metadata identification""}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2024,"software is a central part of the scientific process and involved in obtaining, analysing, visualising and processing research data. understanding the provenance of research requires an understanding of the involved software. however, software citations in scientific publications often are informal, what creates challenges when aiming at understanding software adoption. this paper provides an overview of the software mention detection (somd) shared task conducted as part of the 2024 natural scientific language processing workshop, aiming at advancing the state-of-the-art with respect to nlp methods for detecting software mentions and additional information in scholarly publications. the somd shared task encompasses three subtasks, concerned with software mention recognition (subtask i), recognition of additional information (subtask ii) and classification of involved relations (subtask iii). we present an overview of the tasks, received submissions and used techniques. the best submissions achieved f1 scores of 0.74 (subtask i), 0.838 (subtask ii) and 0.911 (subtask iii) indicating both task feasibility but also potential for further performance gains.",0
Citation prediction by leveraging transformers and natural language processing heuristics,"{""In scientific papers, it is common practice to cite other articles to substantiate claims, provide evidence for factual assertions, reference limitations, and research gaps, and fulfill various other purposes. When authors include a citation in a given sentence, there are two considerations they need to take into account: (i) where in the sentence to place the citation and (ii) which citation to choose to support the underlying claim. In this paper, we focus on the first task as it allows multiple potential approaches that rely on the researcher's individual style and the specific norms and conventions of the relevant scientific community. We propose two automatic methodologies that leverage transformers architecture for either solving a Mask-Filling problem or a Named Entity Recognition problem. On top of the results of the proposed methodologies, we apply ad-hoc Natural Language Processing heuristics to further improve their outcome. We also introduce s2orc-9K, an open dataset for fine-tuning models on this task. A formal evaluation demonstrates that the generative approach significantly outperforms five alternative methods when fine-tuned on the novel dataset. Furthermore, this model's results show no statistically significant deviation from the outputs of three senior researchers.""}",2024,Information Processing and Management,"{bert,""citation prediction"",mask-filling,""named entity recognition"",""transformers architecture""}",Management Science and Operations Research,Decision Sciences,Social Sciences & Humanities,CS,2024,"in scientific papers, it is common practice to cite other articles to substantiate claims, provide evidence for factual assertions, reference limitations, and research gaps, and fulfill various other purposes. when authors include a citation in a given sentence, there are two considerations they need to take into account: (i) where in the sentence to place the citation and (ii) which citation to choose to support the underlying claim. in this paper, we focus on the first task as it allows multiple potential approaches that rely on the researcher's individual style and the specific norms and conventions of the relevant scientific community. we propose two automatic methodologies that leverage transformers architecture for either solving a mask-filling problem or a named entity recognition problem. on top of the results of the proposed methodologies, we apply ad-hoc natural language processing heuristics to further improve their outcome. we also introduce s2orc-9k, an open dataset for fine-tuning models on this task. a formal evaluation demonstrates that the generative approach significantly outperforms five alternative methods when fine-tuned on the novel dataset. furthermore, this model's results show no statistically significant deviation from the outputs of three senior researchers.",-1
Exploring Global Gender Gaps in the Blockchain Domain: Insights from LinkedIn Advertising Data,"{""Blockchain technology has gained widespread attention through Bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. There is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. By analyzing gender-disaggregated data from LinkedIn's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader IT sector. This study delves into the volume, velocity, variety, veracity, and value that LinkedIn Ad data offers to assess gender gaps in the blockchain domain at a global level.""}",2024,"Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024","{""gender gaps in the blockchain interests"",""gender gaps in the blockchain jobs"",""gender gaps in the blockchain skills"",""linkedin ad data"",""mining of social media data""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"blockchain technology has gained widespread attention through bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. there is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. by analyzing gender-disaggregated data from linkedin's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader it sector. this study delves into the volume, velocity, variety, veracity, and value that linkedin ad data offers to assess gender gaps in the blockchain domain at a global level.",3
BERTweet's TACO Fiesta: Contrasting Flavors On The Path of Inference And Information-Driven Argument Mining On Twitter,"{""Argument mining, dealing with the classification of text based on inference and information, denotes a challenging analytical task in the rich context of Twitter (now X), a key platform for online discourse and exchange. Thereby, Twitter offers a diverse repository of short messages bearing on both of these elements. For text classification, transformer approaches, particularly BERT, offer state-of-the-art solutions. Our study delves into optimizing the embeddings of the understudied BERTweet transformer for argument mining on Twitter and broader generalization across topics. We explore the impact of pre-classification fine-tuning by aligning similar manifestations of inference and information while contrasting dissimilar instances. Using the TACO dataset, our approach augments tweets for optimizing BERTweet in a Siamese network, strongly improving classification and cross-topic generalization compared to standard methods. Overall, we contribute the transformer WRAPresentations and classifier WRAP, scoring 86.62% F1 for inference detection, 86.30% for information recognition, and 75.29% across four combinations of these elements, to enhance inference and information-driven argument mining on Twitter.""}",2024,Findings of the Association for Computational Linguistics: NAACL 2024 - Findings,,Software,Computer Science,Physical Sciences,CS,2024,"argument mining, dealing with the classification of text based on inference and information, denotes a challenging analytical task in the rich context of twitter (now x), a key platform for online discourse and exchange. thereby, twitter offers a diverse repository of short messages bearing on both of these elements. for text classification, transformer approaches, particularly bert, offer state-of-the-art solutions. our study delves into optimizing the embeddings of the understudied bertweet transformer for argument mining on twitter and broader generalization across topics. we explore the impact of pre-classification fine-tuning by aligning similar manifestations of inference and information while contrasting dissimilar instances. using the taco dataset, our approach augments tweets for optimizing bertweet in a siamese network, strongly improving classification and cross-topic generalization compared to standard methods. overall, we contribute the transformer wrapresentations and classifier wrap, scoring 86.62% f1 for inference detection, 86.30% for information recognition, and 75.29% across four combinations of these elements, to enhance inference and information-driven argument mining on twitter.",-1
A three-way decision approach for dynamically expandable networks,"{""Conventional deep learning models are designed to work on a single task. They are required to be trained from scratch each time new tasks are added. This leads to overhead in training time. Continual deep learning models with dynamically expandable network architecture aim to handle this issue. The key idea in these models is to find a balance between the properties of stability (preserving the learned information) and plasticity (updating and accommodating the new information) also sometimes referred to as the stability-plasticity dilemma. The stability and plasticity of the model critically depends on three-way division of nodes into freeze, partially regularize and duplicate nodes. Freezing more nodes result in high stability but typically low plasticity. On the other hand, duplicating more nodes result in high plasticity but may not have an effective stability. In this paper, we introduce an approach called three-way decisions based dynamically expandable networks or 3WDDEN and its memory-based version called 3WDDEN-replay. The proposed approaches use game-theoretic rough sets to determine effective thresholds for three-way division of nodes by considering a tradeoff game between stability and plasticity. Experimental results of 3WDDEN on MNIST variant datasets show an overall improvement of 3.8% in accuracy compared to standard dynamically expandable network approach or DEN. 3WDDEN-replay further adds to accuracy with additional memory cost.""}",2024,International Journal of Approximate Reasoning,"{""continual deep learning"",""dynamically expandable networks"",""game-theoretic rough sets""}",Applied Mathematics,Mathematics,Physical Sciences,CS,2024,"conventional deep learning models are designed to work on a single task. they are required to be trained from scratch each time new tasks are added. this leads to overhead in training time. continual deep learning models with dynamically expandable network architecture aim to handle this issue. the key idea in these models is to find a balance between the properties of stability (preserving the learned information) and plasticity (updating and accommodating the new information) also sometimes referred to as the stability-plasticity dilemma. the stability and plasticity of the model critically depends on three-way division of nodes into freeze, partially regularize and duplicate nodes. freezing more nodes result in high stability but typically low plasticity. on the other hand, duplicating more nodes result in high plasticity but may not have an effective stability. in this paper, we introduce an approach called three-way decisions based dynamically expandable networks or 3wdden and its memory-based version called 3wdden-replay. the proposed approaches use game-theoretic rough sets to determine effective thresholds for three-way division of nodes by considering a tradeoff game between stability and plasticity. experimental results of 3wdden on mnist variant datasets show an overall improvement of 3.8% in accuracy compared to standard dynamically expandable network approach or den. 3wdden-replay further adds to accuracy with additional memory cost.",0
Identifying Discourse Markers in French Spoken Corpora: Using Machine Learning and Rule-Based Approaches,"{""The objective of this work is to study the identification of French discourse markers (DM), in particular the polyfunctional occurrences such as ‘attetion’, bon, quoi, la preuve. A number of words identified as DM, and traditionally considered as adverbs or interjections, are also, for instance, adjectives or nouns. For example bon can be a DM or an adjective, ‘attetion’ can be a DM or a noun, etc. Hand annotation is in general robust but time consuming. The main difficulty with automatic identification is to take the context of the DM candidate correctly into account. To do that, a mechanisms based on rule-based and machine learning approaches was built, in order to reach an acceptable level of performance and reduce the expert effort. This study will provide a comprehensive use case of a machine learning algorithm, which has proved a good efficiency in dealing with such linguistic phenomena. In addition, an evaluation was done for the Unitex platform in order to determine the efficiency and drawbacks of this platform when dealing with such type of tasks.""}",2024,Communications in Computer and Information Science,"{""discourse markers"",dm,""french corpora"",knn,""machine learning"",""transfert learning""}",Mathematics (all),,,CS,2024,"the objective of this work is to study the identification of french discourse markers (dm), in particular the polyfunctional occurrences such as ‘attetion’, bon, quoi, la preuve. a number of words identified as dm, and traditionally considered as adverbs or interjections, are also, for instance, adjectives or nouns. for example bon can be a dm or an adjective, ‘attetion’ can be a dm or a noun, etc. hand annotation is in general robust but time consuming. the main difficulty with automatic identification is to take the context of the dm candidate correctly into account. to do that, a mechanisms based on rule-based and machine learning approaches was built, in order to reach an acceptable level of performance and reduce the expert effort. this study will provide a comprehensive use case of a machine learning algorithm, which has proved a good efficiency in dealing with such linguistic phenomena. in addition, an evaluation was done for the unitex platform in order to determine the efficiency and drawbacks of this platform when dealing with such type of tasks.",0
Dissecting Paraphrases: The Impact of Prompt Syntax and Supplementary Information on Knowledge Retrieval from Pretrained Language Models,"{""Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge. One method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. Typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. Simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. We designed CONPARELAMA – a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. CONPARELAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of PLMs. Extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying PLMs with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. In addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.""}",2024,Long Papers,,Computer Networks and Communications,Computer Science,Physical Sciences,CS,2024,"pre-trained language models (plms) are known to contain various kinds of knowledge. one method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. we designed conparelama – a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. these paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. conparelama enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of plms. extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying plms with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. in addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.",0
Scaling up search engine audits: Practical insights for algorithm auditing,"{""Algorithm audits have increased in recent years due to a growing need to independently assess the performance of automatically curated services that process, filter and rank the large and dynamic amount of information available on the Internet. Among several methodologies to perform such audits, virtual agents stand out because they offer the ability to perform systematic experiments, simulating human behaviour without the associated costs of recruiting participants. Motivated by the importance of research transparency and replicability of results, this article focuses on the challenges of such an approach. It provides methodological details, recommendations, lessons learned and limitations based on our experience of setting up experiments for eight search engines (including main, news, image and video sections) with hundreds of virtual agents placed in different regions. We demonstrate the successful performance of our research infrastructure across multiple data collections, with diverse experimental designs, and point to different changes and strategies that improve the quality of the method. We conclude that virtual agents are a promising venue for monitoring the performance of algorithms across long periods of time, and we hope that this article can serve as a basis for further research in this area.""}",2024,Journal of Information Science,"{""algorithm auditing"",""data collection"",""search engine audits"",""user modelling""}",Information Systems,Computer Science,Physical Sciences,CS,2024,"algorithm audits have increased in recent years due to a growing need to independently assess the performance of automatically curated services that process, filter and rank the large and dynamic amount of information available on the internet. among several methodologies to perform such audits, virtual agents stand out because they offer the ability to perform systematic experiments, simulating human behaviour without the associated costs of recruiting participants. motivated by the importance of research transparency and replicability of results, this article focuses on the challenges of such an approach. it provides methodological details, recommendations, lessons learned and limitations based on our experience of setting up experiments for eight search engines (including main, news, image and video sections) with hundreds of virtual agents placed in different regions. we demonstrate the successful performance of our research infrastructure across multiple data collections, with diverse experimental designs, and point to different changes and strategies that improve the quality of the method. we conclude that virtual agents are a promising venue for monitoring the performance of algorithms across long periods of time, and we hope that this article can serve as a basis for further research in this area.",5
TACO - Twitter Arguments from COnversations,"{""Twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. Argument mining is an important analytical task for processing and understanding online discourse. Specifically, it aims to identify the structural elements of arguments, denoted as information and inference. These elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on Twitter. We contribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets covering 200 entire COnversations spanning six heterogeneous topics annotated with an agreement of 0.718 Krippendorff's α among six experts. Second, we provide our annotation framework, incorporating definitions from the Cambridge Dictionary, to define and identify argument components on Twitter. Our transformer-based classifier achieves an 85.06% macro F1 baseline score in detecting arguments. Moreover, our data reveals that Twitter users tend to engage in discussions involving informed inferences and information. TACO serves multiple purposes, such as training tweet classifiers to manage tweets based on inference and information elements, while also providing valuable insights into the conversational reply patterns of tweets.""}",2024,"2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings","{""argument mining"",inference,""information extraction"",resource,""twitter conversations""}",Computational Theory and Mathematics,Computer Science,Physical Sciences,CS,2024,"twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. argument mining is an important analytical task for processing and understanding online discourse. specifically, it aims to identify the structural elements of arguments, denoted as information and inference. these elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on twitter. we contribute taco, the first dataset of twitter arguments utilizing 1,814 tweets covering 200 entire conversations spanning six heterogeneous topics annotated with an agreement of 0.718 krippendorff's α among six experts. second, we provide our annotation framework, incorporating definitions from the cambridge dictionary, to define and identify argument components on twitter. our transformer-based classifier achieves an 85.06% macro f1 baseline score in detecting arguments. moreover, our data reveals that twitter users tend to engage in discussions involving informed inferences and information. taco serves multiple purposes, such as training tweet classifiers to manage tweets based on inference and information elements, while also providing valuable insights into the conversational reply patterns of tweets.",-1
Tracing architecture of machine learning models through their mentions in scholarly articles,"{""Relation extraction, is a pivotal task in NLP, impacts information retrieval, natural language understanding (NLU) and knowledge generation. Machine learning model has coined itself as the most influential term in this era of deep learning and LLM. In scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. Knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. In this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. We attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. We report our findings with four state of the art baseline models. The findings report here exemplary performance with LUKE model as winner. The presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.""}",2024,"Proceedings - 2024 7th International Conference on Data Science and Information Technology, DSIT 2024","{baseline,extraction,""machine learning model"",""machine learning model architecture"",relation}",Computer Vision and Pattern Recognition,Computer Science,Physical Sciences,CS,2024,"relation extraction, is a pivotal task in nlp, impacts information retrieval, natural language understanding (nlu) and knowledge generation. machine learning model has coined itself as the most influential term in this era of deep learning and llm. in scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. in this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. we attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. we report our findings with four state of the art baseline models. the findings report here exemplary performance with luke model as winner. the presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.",6
Bias-aware ranking from pairwise comparisons,"{""Human feedback is often used, either directly or indirectly, as input to algorithmic decision making. However, humans are biased: if the algorithm that takes as input the human feedback does not control for potential biases, this might result in biased algorithmic decision making, which can have a tangible impact on people’s lives. In this paper, we study how to detect and correct for evaluators’ bias in the task of ranking people (or items) from pairwise comparisons. Specifically, we assume we are given pairwise comparisons of the items to be ranked produced by a set of evaluators. While the pairwise assessments of the evaluators should reflect to a certain extent the latent (unobservable) true quality scores of the items, they might be affected by each evaluator’s own bias against, or in favor, of some groups of items. By detecting and amending evaluators’ biases, we aim to produce a ranking of the items that is, as much as possible, in accordance with the ranking one would produce by having access to the latent quality scores. Our proposal is a novel method that extends the classic Bradley-Terry model by having a bias parameter for each evaluator which distorts the true quality score of each item, depending on the group the item belongs to. Thanks to the simplicity of the model, we are able to write explicitly its log-likelihood w.r.t. the parameters (i.e., items’ latent scores and evaluators’ bias) and optimize by means of the alternating approach. Our experiments on synthetic and real-world data confirm that our method is able to reconstruct the bias of each single evaluator extremely well and thus to outperform several non-trivial competitors in the task of producing a ranking which is as much as possible close to the unbiased ranking.""}",2024,Data Mining and Knowledge Discovery,"{bradley-terry,""evaluators’ bias"",fairness,""pairwise comparisons"",rankings}",Information Systems,Computer Science,Physical Sciences,CS,2024,"human feedback is often used, either directly or indirectly, as input to algorithmic decision making. however, humans are biased: if the algorithm that takes as input the human feedback does not control for potential biases, this might result in biased algorithmic decision making, which can have a tangible impact on people’s lives. in this paper, we study how to detect and correct for evaluators’ bias in the task of ranking people (or items) from pairwise comparisons. specifically, we assume we are given pairwise comparisons of the items to be ranked produced by a set of evaluators. while the pairwise assessments of the evaluators should reflect to a certain extent the latent (unobservable) true quality scores of the items, they might be affected by each evaluator’s own bias against, or in favor, of some groups of items. by detecting and amending evaluators’ biases, we aim to produce a ranking of the items that is, as much as possible, in accordance with the ranking one would produce by having access to the latent quality scores. our proposal is a novel method that extends the classic bradley-terry model by having a bias parameter for each evaluator which distorts the true quality score of each item, depending on the group the item belongs to. thanks to the simplicity of the model, we are able to write explicitly its log-likelihood w.r.t. the parameters (i.e., items’ latent scores and evaluators’ bias) and optimize by means of the alternating approach. our experiments on synthetic and real-world data confirm that our method is able to reconstruct the bias of each single evaluator extremely well and thus to outperform several non-trivial competitors in the task of producing a ranking which is as much as possible close to the unbiased ranking.",5
Exploring Global Gender Gaps in the Blockchain Domain: Insights from LinkedIn Advertising Data,"{""Blockchain technology has gained widespread attention through Bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. There is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. By analyzing gender-disaggregated data from LinkedIn's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader IT sector. This study delves into the volume, velocity, variety, veracity, and value that LinkedIn Ad data offers to assess gender gaps in the blockchain domain at a global level.""}",2024,"Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024","{""gender gaps in the blockchain interests"",""gender gaps in the blockchain jobs"",""gender gaps in the blockchain skills"",""linkedin ad data"",""mining of social media data""}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2024,"blockchain technology has gained widespread attention through bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. there is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. by analyzing gender-disaggregated data from linkedin's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader it sector. this study delves into the volume, velocity, variety, veracity, and value that linkedin ad data offers to assess gender gaps in the blockchain domain at a global level.",3
Triplétoile: Extraction of knowledge from microblogging text,"{""Numerous methods and pipelines have recently emerged for the automatic extraction of knowledge graphs from documents such as scientific publications and patents. However, adapting these methods to incorporate alternative text sources like micro-blogging posts and news has proven challenging as they struggle to model open-domain entities and relations, typically found in these sources. In this paper, we propose an enhanced information extraction pipeline tailored to the extraction of a knowledge graph comprising open-domain entities from micro-blogging posts on social media platforms. Our pipeline leverages dependency parsing and classifies entity relations in an unsupervised manner through hierarchical clustering over word embeddings. We provide a use case on extracting semantic triples from a corpus of 100 thousand tweets about digital transformation and publicly release the generated knowledge graph. On the same dataset, we conduct two experimental evaluations, showing that the system produces triples with precision over 95% and outperforms similar pipelines of around 5% in terms of precision, while generating a comparatively higher number of triples.""}",2024,Heliyon,"{""hierarchical clustering"",""information extraction"",""knowledge graphs"",""named entity recognition"",""social media analysis"",""word embeddings""}",Multidisciplinary,Multidisciplinary,Multidisciplinary,CS,2024,"numerous methods and pipelines have recently emerged for the automatic extraction of knowledge graphs from documents such as scientific publications and patents. however, adapting these methods to incorporate alternative text sources like micro-blogging posts and news has proven challenging as they struggle to model open-domain entities and relations, typically found in these sources. in this paper, we propose an enhanced information extraction pipeline tailored to the extraction of a knowledge graph comprising open-domain entities from micro-blogging posts on social media platforms. our pipeline leverages dependency parsing and classifies entity relations in an unsupervised manner through hierarchical clustering over word embeddings. we provide a use case on extracting semantic triples from a corpus of 100 thousand tweets about digital transformation and publicly release the generated knowledge graph. on the same dataset, we conduct two experimental evaluations, showing that the system produces triples with precision over 95% and outperforms similar pipelines of around 5% in terms of precision, while generating a comparatively higher number of triples.",5
A mutually enhanced multi-scale relation-aware graph convolutional network for argument pair extraction,"{""Argument pair extraction (APE) is a fine-grained task of argument mining which aims to identify arguments offered by different participants in some discourse and detect interaction relationships between arguments from different participants. In recent years, many research efforts have been devoted to dealing with APE in a multi-task learning framework. Although these approaches have achieved encouraging results, they still face several challenging issues. First, different types of sentence relationships as well as different levels of information exchange among sentences are largely ignored. Second, they solely model interactions between argument pairs either in an explicit or implicit strategy, while neglecting the complementary effect of the two strategies. In this paper, we propose a novel Mutually Enhanced Multi-Scale Relation-Aware Graph Convolutional Network (MMR-GCN) for APE. Specifically, we first design a multi-scale relation-aware graph aggregation module to explicitly model the complex relationships between review and rebuttal passage sentences. In addition, we propose a mutually enhancement transformer module to implicitly and interactively enhance representations of review and rebuttal passage sentences. We experimentally validate MMR-GCN by comparing with the state-of-the-art APE methods. Experimental results show that it considerably outperforms all baseline methods, and the relative performance improvement of MMR-GCN over the best performing baseline MRC-APE in terms of F1 score reaches to 3.48% and 4.43% on the two benchmark datasets, respectively.""}",2024,Journal of Intelligent Information Systems,"{""argument mining"",""argument pair extraction"",""graph convolutional network"",transformer}",Computer Networks and Communications,Computer Science,Physical Sciences,CS,2024,"argument pair extraction (ape) is a fine-grained task of argument mining which aims to identify arguments offered by different participants in some discourse and detect interaction relationships between arguments from different participants. in recent years, many research efforts have been devoted to dealing with ape in a multi-task learning framework. although these approaches have achieved encouraging results, they still face several challenging issues. first, different types of sentence relationships as well as different levels of information exchange among sentences are largely ignored. second, they solely model interactions between argument pairs either in an explicit or implicit strategy, while neglecting the complementary effect of the two strategies. in this paper, we propose a novel mutually enhanced multi-scale relation-aware graph convolutional network (mmr-gcn) for ape. specifically, we first design a multi-scale relation-aware graph aggregation module to explicitly model the complex relationships between review and rebuttal passage sentences. in addition, we propose a mutually enhancement transformer module to implicitly and interactively enhance representations of review and rebuttal passage sentences. we experimentally validate mmr-gcn by comparing with the state-of-the-art ape methods. experimental results show that it considerably outperforms all baseline methods, and the relative performance improvement of mmr-gcn over the best performing baseline mrc-ape in terms of f1 score reaches to 3.48% and 4.43% on the two benchmark datasets, respectively.",6
A Novel Knowledge Plug-In for Incorporating Information About Employability From the O*NET Database Into Conversational Agents,"{""The labor market is a dynamic and rapidly evolving environment. Job positions that require expertise in various sectors often lead candidates to question their suitability. Therefore, it is crucial to furnish them with relevant, accurate, and timely information. In this article, we introduce a knowledge plug-in for existing conversational agents designed to address specific queries related to the job market domain. Additionally, we propose an innovative method for dynamically creating diverse grammars and semantically associating users' questions with a predefined list of domain questions. We present a novel scheme to effectively tackle question-answering tasks in settings with limited resources. Our architecture relies on question-understanding and response-generation modules, both powered by Transformers, the O*NET occupational database, and recommendation engines that suggest training materials. Furthermore, we conducted a user study based on the System Usability Scale (SUS) score, revealing that users highly appreciated the proposed tool. This sentiment was particularly evident when the tool was integrated with other artificial intelligence chatbots capable of handling general information. Simultaneously, our engine adeptly manages information within the investigated domain, providing precise responses and recommendations. This work addresses a critical gap in the delivery of employment information and paves the way for the development of diverse functionalities to assist both candidates and employers.""}",2024,IEEE Access,"{chatbots,""human-robot interaction"",""information extraction"",""labor market"",""online enrolling process"",""user experience"",""virtual assistant""}",Engineering (all),,,CS,2024,"the labor market is a dynamic and rapidly evolving environment. job positions that require expertise in various sectors often lead candidates to question their suitability. therefore, it is crucial to furnish them with relevant, accurate, and timely information. in this article, we introduce a knowledge plug-in for existing conversational agents designed to address specific queries related to the job market domain. additionally, we propose an innovative method for dynamically creating diverse grammars and semantically associating users' questions with a predefined list of domain questions. we present a novel scheme to effectively tackle question-answering tasks in settings with limited resources. our architecture relies on question-understanding and response-generation modules, both powered by transformers, the o*net occupational database, and recommendation engines that suggest training materials. furthermore, we conducted a user study based on the system usability scale (sus) score, revealing that users highly appreciated the proposed tool. this sentiment was particularly evident when the tool was integrated with other artificial intelligence chatbots capable of handling general information. simultaneously, our engine adeptly manages information within the investigated domain, providing precise responses and recommendations. this work addresses a critical gap in the delivery of employment information and paves the way for the development of diverse functionalities to assist both candidates and employers.",1
Patterns in the Growth and Thematic Evolution of Artificial Intelligence Research: A Study Using Bradford Distribution of Productivity and Path Analysis,"{""Artificial intelligence (AI) has emerged as a transformative technology with applications across multiple domains. The corpus of work related to the field of AI has grown significantly in volume as well as in terms of the application of AI in wider domains. However, given the wide application of AI in diverse areas, the measurement and characterization of the span of AI research is often a challenging task. Bibliometrics is a well-established method in the scientific community to measure the patterns and impact of research. It however has also received significant criticism for its overemphasis on the macroscopic picture and the inability to provide a deep understanding of growth and thematic structure of knowledge-creation activities. Therefore, this study presents a framework comprising of two techniques, namely, Bradford's distribution and path analysis to characterize the growth and thematic evolution of the discipline. While the Bradford distribution provides a macroscopic view of artificial intelligence research in terms of patterns of growth, the path analysis method presents a microscopic analysis of the thematic evolutionary trajectories, thereby completing the analytical framework. Detailed insights into the evolution of each subdomain are drawn, major techniques employed in various AI applications are identified, and some relevant implications are discussed to demonstrate the usefulness of the analyses.""}",2024,International Journal of Intelligent Systems,,Theoretical Computer Science,Mathematics,Physical Sciences,CS,2024,"artificial intelligence (ai) has emerged as a transformative technology with applications across multiple domains. the corpus of work related to the field of ai has grown significantly in volume as well as in terms of the application of ai in wider domains. however, given the wide application of ai in diverse areas, the measurement and characterization of the span of ai research is often a challenging task. bibliometrics is a well-established method in the scientific community to measure the patterns and impact of research. it however has also received significant criticism for its overemphasis on the macroscopic picture and the inability to provide a deep understanding of growth and thematic structure of knowledge-creation activities. therefore, this study presents a framework comprising of two techniques, namely, bradford's distribution and path analysis to characterize the growth and thematic evolution of the discipline. while the bradford distribution provides a macroscopic view of artificial intelligence research in terms of patterns of growth, the path analysis method presents a microscopic analysis of the thematic evolutionary trajectories, thereby completing the analytical framework. detailed insights into the evolution of each subdomain are drawn, major techniques employed in various ai applications are identified, and some relevant implications are discussed to demonstrate the usefulness of the analyses.",7
Exploring Global Gender Gaps in the Blockchain Domain: Insights from LinkedIn Advertising Data,"{""Blockchain technology has gained widespread attention through Bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. There is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. By analyzing gender-disaggregated data from LinkedIn's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader IT sector. This study delves into the volume, velocity, variety, veracity, and value that LinkedIn Ad data offers to assess gender gaps in the blockchain domain at a global level.""}",2024,"Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024","{""gender gaps in the blockchain interests"",""gender gaps in the blockchain jobs"",""gender gaps in the blockchain skills"",""linkedin ad data"",""mining of social media data""}",Computer Networks and Communications,Computer Science,Physical Sciences,CS,2024,"blockchain technology has gained widespread attention through bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. there is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. by analyzing gender-disaggregated data from linkedin's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader it sector. this study delves into the volume, velocity, variety, veracity, and value that linkedin ad data offers to assess gender gaps in the blockchain domain at a global level.",3
BERTweet's TACO Fiesta: Contrasting Flavors On The Path of Inference And Information-Driven Argument Mining On Twitter,"{""Argument mining, dealing with the classification of text based on inference and information, denotes a challenging analytical task in the rich context of Twitter (now X), a key platform for online discourse and exchange. Thereby, Twitter offers a diverse repository of short messages bearing on both of these elements. For text classification, transformer approaches, particularly BERT, offer state-of-the-art solutions. Our study delves into optimizing the embeddings of the understudied BERTweet transformer for argument mining on Twitter and broader generalization across topics. We explore the impact of pre-classification fine-tuning by aligning similar manifestations of inference and information while contrasting dissimilar instances. Using the TACO dataset, our approach augments tweets for optimizing BERTweet in a Siamese network, strongly improving classification and cross-topic generalization compared to standard methods. Overall, we contribute the transformer WRAPresentations and classifier WRAP, scoring 86.62% F1 for inference detection, 86.30% for information recognition, and 75.29% across four combinations of these elements, to enhance inference and information-driven argument mining on Twitter.""}",2024,Findings of the Association for Computational Linguistics: NAACL 2024 - Findings,,Computational Theory and Mathematics,Computer Science,Physical Sciences,CS,2024,"argument mining, dealing with the classification of text based on inference and information, denotes a challenging analytical task in the rich context of twitter (now x), a key platform for online discourse and exchange. thereby, twitter offers a diverse repository of short messages bearing on both of these elements. for text classification, transformer approaches, particularly bert, offer state-of-the-art solutions. our study delves into optimizing the embeddings of the understudied bertweet transformer for argument mining on twitter and broader generalization across topics. we explore the impact of pre-classification fine-tuning by aligning similar manifestations of inference and information while contrasting dissimilar instances. using the taco dataset, our approach augments tweets for optimizing bertweet in a siamese network, strongly improving classification and cross-topic generalization compared to standard methods. overall, we contribute the transformer wrapresentations and classifier wrap, scoring 86.62% f1 for inference detection, 86.30% for information recognition, and 75.29% across four combinations of these elements, to enhance inference and information-driven argument mining on twitter.",-1
Connected Components for Scaling Partial-order Blocking to Billion Entities,"{""In entity resolution, blocking pre-partitions data for further processing by more expensive methods. Two entity mentions are in the same block if they share identical or related blocking-keys. Previous work has sometimes related blocking keys by grouping or alphabetically sorting them, but-as was shown for author disambiguation-the respective equivalences or total orders are not necessarily well-suited to model the logical matching-relation between blocking keys. To address this, we present a novel blocking approach that exploits the subset partial order over entity representations to build a matching-based bipartite graph, using connected components as blocks. To prevent over-and underconnectedness, we allow specification of overly general and generalization of overly specific representations. To build the bipartite graph, we contribute a new parallellized algorithm with configurable time/space tradeoff for minimal element search in the subset partial order. As a job-based approach, it combines dynamic scalability and easier integration to make it more convenient than the previously described approaches. Experiments on large gold standards for publication records, author mentions, and affiliation strings suggest that our approach is competitive in performance and allows better addressing of domain-specific problems. For duplicate detection and author disambiguation, our method offers the expected performance as defined by the vector-similarity baseline used in another work on the same dataset and the common surname, first-initial baseline. For top-level institution resolution, we have reproduced the challenges described in prior work, strengthening the conclusion that for affiliation data, overlapping blocks under minimal elements are more suitable than connected components.""}",2024,Journal of Data and Information Quality,"{""additional key words and phrasesentity resolution"",blocking,lattices,""partial orders""}",Information Systems and Management,Decision Sciences,Social Sciences & Humanities,CS,2024,"in entity resolution, blocking pre-partitions data for further processing by more expensive methods. two entity mentions are in the same block if they share identical or related blocking-keys. previous work has sometimes related blocking keys by grouping or alphabetically sorting them, but-as was shown for author disambiguation-the respective equivalences or total orders are not necessarily well-suited to model the logical matching-relation between blocking keys. to address this, we present a novel blocking approach that exploits the subset partial order over entity representations to build a matching-based bipartite graph, using connected components as blocks. to prevent over-and underconnectedness, we allow specification of overly general and generalization of overly specific representations. to build the bipartite graph, we contribute a new parallellized algorithm with configurable time/space tradeoff for minimal element search in the subset partial order. as a job-based approach, it combines dynamic scalability and easier integration to make it more convenient than the previously described approaches. experiments on large gold standards for publication records, author mentions, and affiliation strings suggest that our approach is competitive in performance and allows better addressing of domain-specific problems. for duplicate detection and author disambiguation, our method offers the expected performance as defined by the vector-similarity baseline used in another work on the same dataset and the common surname, first-initial baseline. for top-level institution resolution, we have reproduced the challenges described in prior work, strengthening the conclusion that for affiliation data, overlapping blocks under minimal elements are more suitable than connected components.",6
Data Management Planning across Disciplines and Infrastructures. Introduction to the Special Collection,"{""The Special Collection Data Management Planning across Disciplines and Infrastructures of the Data Science Journal consists of papers describing practical experiences, concepts, and future directions on the design and deployment of effective data management plans and associated tools. Papers contain practical examples on managing and sharing data, consider the integration of data management plans into infrastructures and reflect innovative research into new directions for disciplinary and cross-disciplinary data management planning.""}",2024,Data Science Journal,"{""(cross-) disciplinary data management"",activedmps,""data management plan (dmp)"",""data management planning"",""data management planning infrastructures"",madmps}",Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"the special collection data management planning across disciplines and infrastructures of the data science journal consists of papers describing practical experiences, concepts, and future directions on the design and deployment of effective data management plans and associated tools. papers contain practical examples on managing and sharing data, consider the integration of data management plans into infrastructures and reflect innovative research into new directions for disciplinary and cross-disciplinary data management planning.",2
A mutually enhanced multi-scale relation-aware graph convolutional network for argument pair extraction,"{""Argument pair extraction (APE) is a fine-grained task of argument mining which aims to identify arguments offered by different participants in some discourse and detect interaction relationships between arguments from different participants. In recent years, many research efforts have been devoted to dealing with APE in a multi-task learning framework. Although these approaches have achieved encouraging results, they still face several challenging issues. First, different types of sentence relationships as well as different levels of information exchange among sentences are largely ignored. Second, they solely model interactions between argument pairs either in an explicit or implicit strategy, while neglecting the complementary effect of the two strategies. In this paper, we propose a novel Mutually Enhanced Multi-Scale Relation-Aware Graph Convolutional Network (MMR-GCN) for APE. Specifically, we first design a multi-scale relation-aware graph aggregation module to explicitly model the complex relationships between review and rebuttal passage sentences. In addition, we propose a mutually enhancement transformer module to implicitly and interactively enhance representations of review and rebuttal passage sentences. We experimentally validate MMR-GCN by comparing with the state-of-the-art APE methods. Experimental results show that it considerably outperforms all baseline methods, and the relative performance improvement of MMR-GCN over the best performing baseline MRC-APE in terms of F1 score reaches to 3.48% and 4.43% on the two benchmark datasets, respectively.""}",2024,Journal of Intelligent Information Systems,"{""argument mining"",""argument pair extraction"",""graph convolutional network"",transformer}",Information Systems,Computer Science,Physical Sciences,CS,2024,"argument pair extraction (ape) is a fine-grained task of argument mining which aims to identify arguments offered by different participants in some discourse and detect interaction relationships between arguments from different participants. in recent years, many research efforts have been devoted to dealing with ape in a multi-task learning framework. although these approaches have achieved encouraging results, they still face several challenging issues. first, different types of sentence relationships as well as different levels of information exchange among sentences are largely ignored. second, they solely model interactions between argument pairs either in an explicit or implicit strategy, while neglecting the complementary effect of the two strategies. in this paper, we propose a novel mutually enhanced multi-scale relation-aware graph convolutional network (mmr-gcn) for ape. specifically, we first design a multi-scale relation-aware graph aggregation module to explicitly model the complex relationships between review and rebuttal passage sentences. in addition, we propose a mutually enhancement transformer module to implicitly and interactively enhance representations of review and rebuttal passage sentences. we experimentally validate mmr-gcn by comparing with the state-of-the-art ape methods. experimental results show that it considerably outperforms all baseline methods, and the relative performance improvement of mmr-gcn over the best performing baseline mrc-ape in terms of f1 score reaches to 3.48% and 4.43% on the two benchmark datasets, respectively.",6
Dissecting Paraphrases: The Impact of Prompt Syntax and Supplementary Information on Knowledge Retrieval from Pretrained Language Models,"{""Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge. One method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. Typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. Simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. We designed CONPARELAMA – a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. CONPARELAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of PLMs. Extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying PLMs with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. In addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.""}",2024,Long Papers,,Information Systems,Computer Science,Physical Sciences,CS,2024,"pre-trained language models (plms) are known to contain various kinds of knowledge. one method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. we designed conparelama – a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. these paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. conparelama enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of plms. extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying plms with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. in addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.",0
[PromptEng] First International Workshop on Prompt Engineering for Pre-Trained Language Models,"{""The recent achievements and availability of Large Language Models have paved the road to a new range of applications and use-cases. Pre-trained language models are now being involved at-scale in many fields where they were until now absent from. More specifically, the progress made by causal generative models has open the door to using them through textual instructions aka. prompts. Unfortunately, the performances of these prompts are highly dependent on the exact phrasing used and therefore practitioners need to adopt fail-retry strategies. This first international workshop on prompt engineering aims at gathering practitioners (both from Academia and Industry) to exchange about good practices, optimizations, results and novel paradigms about the design of efficient prompts to make use of LLMs.""}",2024,WWW 2024 Companion - Companion Proceedings of the ACM Web Conference,"{""best practices"",""collective task"",llm,""prompt engineering""}",Software,Computer Science,Physical Sciences,CS,2024,"the recent achievements and availability of large language models have paved the road to a new range of applications and use-cases. pre-trained language models are now being involved at-scale in many fields where they were until now absent from. more specifically, the progress made by causal generative models has open the door to using them through textual instructions aka. prompts. unfortunately, the performances of these prompts are highly dependent on the exact phrasing used and therefore practitioners need to adopt fail-retry strategies. this first international workshop on prompt engineering aims at gathering practitioners (both from academia and industry) to exchange about good practices, optimizations, results and novel paradigms about the design of efficient prompts to make use of llms.",0
RADAr: A Transformer-Based Autoregressive Decoder Architecture for Hierarchical Text Classification,"{""Recent approaches in hierarchical text classification (HTC) rely on the capabilities of a pre-trained transformer model and exploit the label semantics and a graph encoder for the label hierarchy.In this paper, we introduce an effective hierarchical text classifier RADAr (Transformer-based Autoregressive Decoder Architecture) that is based only on an off-the-shelf RoBERTa transformer to process the input and a custom autoregressive decoder with two decoder layers for generating the classification output.Thus, unlike existing approaches for HTC, the encoder of RADAr has no explicit encoding of the label hierarchy and the decoder solely relies on the samples' label sequences observed during training.We demonstrate on three benchmark datasets that RADAr achieves results competitive to the state of the art with less training and inference time.Our model consistently performs better when organizing the label sequences from children to parents versus the inverse, as done in existing HTC approaches.Our experiments show that neither the label semantics nor an explicit graph encoder for the hierarchy is needed.This has strong practical implications for HTC as the architecture has fewer requirements and provides a speed-up by a factor of 2 at inference time.Moreover, training a separate decoder from scratch in conjunction with fine-tuning the encoder allows future researchers and practitioners to exchange the encoder part as new models arise.The source code is available at https://github.com/yousef-younes/RADAr.""}",2024,Frontiers in Artificial Intelligence and Applications,,Artificial Intelligence,Computer Science,Physical Sciences,CS,2024,"recent approaches in hierarchical text classification (htc) rely on the capabilities of a pre-trained transformer model and exploit the label semantics and a graph encoder for the label hierarchy.in this paper, we introduce an effective hierarchical text classifier radar (transformer-based autoregressive decoder architecture) that is based only on an off-the-shelf roberta transformer to process the input and a custom autoregressive decoder with two decoder layers for generating the classification output.thus, unlike existing approaches for htc, the encoder of radar has no explicit encoding of the label hierarchy and the decoder solely relies on the samples' label sequences observed during training.we demonstrate on three benchmark datasets that radar achieves results competitive to the state of the art with less training and inference time.our model consistently performs better when organizing the label sequences from children to parents versus the inverse, as done in existing htc approaches.our experiments show that neither the label semantics nor an explicit graph encoder for the hierarchy is needed.this has strong practical implications for htc as the architecture has fewer requirements and provides a speed-up by a factor of 2 at inference time.moreover, training a separate decoder from scratch in conjunction with fine-tuning the encoder allows future researchers and practitioners to exchange the encoder part as new models arise.the source code is available at https://github.com/yousef-younes/radar.",0
Introducing the Validation of Data Quality Indicators Through Re-Classification: The example of SQP and pretest surveys,"{""The present study introduces the concept of validating data quality indicators through re-classification. We use the term re-classification to mean the evaluation of how well an indicator detects the quality of different versions of a survey question for which the quality is known a priori. We illustrate its application with two examples. In both, we make use of 12 questions from prior experiments that manipulated text features of questions to create ‘low’ and ‘high’ quality versions of each question. In the first example, we coded each question version in SQP 2.1 to obtain indicators of validity, reliability, and quality. We compared these indicators between the two versions of each question to assess whether the SQP outcomes were sensitive to text features. In the second example, we used a pretest survey to obtain three indicators of survey quality: response latencies, item nonresponse, and consistency over time. Again, we compared these indicators between question versions to assess whether the indicators were sensitive to text features. We give recommendations for applying re-classification and an outlook for future research opportunities.""}",2024,International Journal of Market Research,"{""data quality"",pretesting,re-classification,""survey experiments"",""survey questions"",validation}",Economics and Econometrics,"Economics, Econometrics and Finance",Social Sciences & Humanities,CS,2024,"the present study introduces the concept of validating data quality indicators through re-classification. we use the term re-classification to mean the evaluation of how well an indicator detects the quality of different versions of a survey question for which the quality is known a priori. we illustrate its application with two examples. in both, we make use of 12 questions from prior experiments that manipulated text features of questions to create ‘low’ and ‘high’ quality versions of each question. in the first example, we coded each question version in sqp 2.1 to obtain indicators of validity, reliability, and quality. we compared these indicators between the two versions of each question to assess whether the sqp outcomes were sensitive to text features. in the second example, we used a pretest survey to obtain three indicators of survey quality: response latencies, item nonresponse, and consistency over time. again, we compared these indicators between question versions to assess whether the indicators were sensitive to text features. we give recommendations for applying re-classification and an outlook for future research opportunities.",8
Enhancing Software-Related Information Extraction via Single-Choice Question Answering with Large Language Models,"{""This paper describes our participation in the Shared Task on Software Mentions Disambiguation (SOMD), with a focus on improving relation extraction in scholarly texts through generative Large Language Models (LLMs) using single-choice question-answering. The methodology prioritises the use of in-context learning capabilities of LLMs to extract software-related entities and their descriptive attributes, such as distributive information. Our approach uses Retrieval-Augmented Generation (RAG) techniques and LLMs for Named Entity Recognition (NER) and Attributive NER to identify relationships between extracted software entities, providing a structured solution for analysing software citations in academic literature. The paper provides a detailed description of our approach, demonstrating how using LLMs in a single-choice QA paradigm can greatly enhance IE methodologies. Our participation in the SOMD shared task highlights the importance of precise software citation practices and showcases our system’s ability to overcome the challenges of disambiguating and extracting relationships between software mentions. This sets the groundwork for future research and development in this field.""}",2024,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"{""generative large language models"",""information extraction"",""named entity recognition"",""relation extraction"",""retrieval-augmented generation"",""single-choice question answering"",""software citation"",""software mentions disambiguation task""}",Computer Science (all),,,CS,2024,"this paper describes our participation in the shared task on software mentions disambiguation (somd), with a focus on improving relation extraction in scholarly texts through generative large language models (llms) using single-choice question-answering. the methodology prioritises the use of in-context learning capabilities of llms to extract software-related entities and their descriptive attributes, such as distributive information. our approach uses retrieval-augmented generation (rag) techniques and llms for named entity recognition (ner) and attributive ner to identify relationships between extracted software entities, providing a structured solution for analysing software citations in academic literature. the paper provides a detailed description of our approach, demonstrating how using llms in a single-choice qa paradigm can greatly enhance ie methodologies. our participation in the somd shared task highlights the importance of precise software citation practices and showcases our system’s ability to overcome the challenges of disambiguating and extracting relationships between software mentions. this sets the groundwork for future research and development in this field.",0
Enhancing Software-Related Information Extraction via Single-Choice Question Answering with Large Language Models,"{""This paper describes our participation in the Shared Task on Software Mentions Disambiguation (SOMD), with a focus on improving relation extraction in scholarly texts through generative Large Language Models (LLMs) using single-choice question-answering. The methodology prioritises the use of in-context learning capabilities of LLMs to extract software-related entities and their descriptive attributes, such as distributive information. Our approach uses Retrieval-Augmented Generation (RAG) techniques and LLMs for Named Entity Recognition (NER) and Attributive NER to identify relationships between extracted software entities, providing a structured solution for analysing software citations in academic literature. The paper provides a detailed description of our approach, demonstrating how using LLMs in a single-choice QA paradigm can greatly enhance IE methodologies. Our participation in the SOMD shared task highlights the importance of precise software citation practices and showcases our system’s ability to overcome the challenges of disambiguating and extracting relationships between software mentions. This sets the groundwork for future research and development in this field.""}",2024,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"{""generative large language models"",""information extraction"",""named entity recognition"",""relation extraction"",""retrieval-augmented generation"",""single-choice question answering"",""software citation"",""software mentions disambiguation task""}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2024,"this paper describes our participation in the shared task on software mentions disambiguation (somd), with a focus on improving relation extraction in scholarly texts through generative large language models (llms) using single-choice question-answering. the methodology prioritises the use of in-context learning capabilities of llms to extract software-related entities and their descriptive attributes, such as distributive information. our approach uses retrieval-augmented generation (rag) techniques and llms for named entity recognition (ner) and attributive ner to identify relationships between extracted software entities, providing a structured solution for analysing software citations in academic literature. the paper provides a detailed description of our approach, demonstrating how using llms in a single-choice qa paradigm can greatly enhance ie methodologies. our participation in the somd shared task highlights the importance of precise software citation practices and showcases our system’s ability to overcome the challenges of disambiguating and extracting relationships between software mentions. this sets the groundwork for future research and development in this field.",0
Tracing architecture of machine learning models through their mentions in scholarly articles,"{""Relation extraction, is a pivotal task in NLP, impacts information retrieval, natural language understanding (NLU) and knowledge generation. Machine learning model has coined itself as the most influential term in this era of deep learning and LLM. In scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. Knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. In this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. We attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. We report our findings with four state of the art baseline models. The findings report here exemplary performance with LUKE model as winner. The presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.""}",2024,"Proceedings - 2024 7th International Conference on Data Science and Information Technology, DSIT 2024","{baseline,extraction,""machine learning model"",""machine learning model architecture"",relation}",Information Systems and Management,Decision Sciences,Social Sciences & Humanities,CS,2024,"relation extraction, is a pivotal task in nlp, impacts information retrieval, natural language understanding (nlu) and knowledge generation. machine learning model has coined itself as the most influential term in this era of deep learning and llm. in scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. in this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. we attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. we report our findings with four state of the art baseline models. the findings report here exemplary performance with luke model as winner. the presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.",6
On the Anatomy of Real-World R Code for Static Analysis,"{""Context The R programming language has a huge and active community, especially in the area of statistical computing. Its interpreted nature allows for several interesting constructs, like the manipulation of functions at run-time, that hinder the static analysis of R programs. At the same time, there is a lack of existing research regarding how these features, or even the R language as a whole are used in practice. Objective In this paper, we conduct a large-scale, static analysis of more than 50 million lines of real- world R programs and packages to identify their characteristics and the features that are actually used. Moreover, we compare the similarities and differences between the scripts of R users and the implementations of package authors. We provide insights for static analysis tools like the lintr package as well as potential interpreter optimizations and uncover areas for future research. Method We analyze 4 230 R scripts submitted alongside publications and the sources of 19 450 CRAN packages for over 350 000 R files, collecting and summarizing quantitative information for features of interest. Results We find a high frequency of name-based indexing operations, assignments, and loops, but a low frequency for most of R's reflective functions. Furthermore, we find neither testing functions nor many calls to R's foreign function interface (FFI) in the publication submissions. Conclusion R scripts and package sources differ, for example, in their size, the way they include other packages, and their usage of R's reflective capabilities. We provide features that are used frequently and should be prioritized by static analysis tools, like operator assignments, function calls, and certain reflective functions like loadCCS CONCEPTS•General and reference → Empirical studies; • Software and its engineering → Language features.""}",2024,"Proceedings - 2024 IEEE/ACM 21st International Conference on Mining Software Repositories, MSR 2024","{""language feature usage"",""large-scale static analysis"",""r programming language""}","Safety, Risk, Reliability and Quality",Engineering,Physical Sciences,CS,2024,"context the r programming language has a huge and active community, especially in the area of statistical computing. its interpreted nature allows for several interesting constructs, like the manipulation of functions at run-time, that hinder the static analysis of r programs. at the same time, there is a lack of existing research regarding how these features, or even the r language as a whole are used in practice. objective in this paper, we conduct a large-scale, static analysis of more than 50 million lines of real- world r programs and packages to identify their characteristics and the features that are actually used. moreover, we compare the similarities and differences between the scripts of r users and the implementations of package authors. we provide insights for static analysis tools like the lintr package as well as potential interpreter optimizations and uncover areas for future research. method we analyze 4 230 r scripts submitted alongside publications and the sources of 19 450 cran packages for over 350 000 r files, collecting and summarizing quantitative information for features of interest. results we find a high frequency of name-based indexing operations, assignments, and loops, but a low frequency for most of r's reflective functions. furthermore, we find neither testing functions nor many calls to r's foreign function interface (ffi) in the publication submissions. conclusion r scripts and package sources differ, for example, in their size, the way they include other packages, and their usage of r's reflective capabilities. we provide features that are used frequently and should be prioritized by static analysis tools, like operator assignments, function calls, and certain reflective functions like loadccs concepts•general and reference → empirical studies; • software and its engineering → language features.",0
Dissecting Paraphrases: The Impact of Prompt Syntax and Supplementary Information on Knowledge Retrieval from Pretrained Language Models,"{""Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge. One method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. Typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. Simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. We designed CONPARELAMA – a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. CONPARELAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of PLMs. Extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying PLMs with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. In addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.""}",2024,Long Papers,,Software,Computer Science,Physical Sciences,CS,2024,"pre-trained language models (plms) are known to contain various kinds of knowledge. one method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. we designed conparelama – a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. these paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. conparelama enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of plms. extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying plms with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. in addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.",0
Introducing the Validation of Data Quality Indicators Through Re-Classification: The example of SQP and pretest surveys,"{""The present study introduces the concept of validating data quality indicators through re-classification. We use the term re-classification to mean the evaluation of how well an indicator detects the quality of different versions of a survey question for which the quality is known a priori. We illustrate its application with two examples. In both, we make use of 12 questions from prior experiments that manipulated text features of questions to create ‘low’ and ‘high’ quality versions of each question. In the first example, we coded each question version in SQP 2.1 to obtain indicators of validity, reliability, and quality. We compared these indicators between the two versions of each question to assess whether the SQP outcomes were sensitive to text features. In the second example, we used a pretest survey to obtain three indicators of survey quality: response latencies, item nonresponse, and consistency over time. Again, we compared these indicators between question versions to assess whether the indicators were sensitive to text features. We give recommendations for applying re-classification and an outlook for future research opportunities.""}",2024,International Journal of Market Research,"{""data quality"",pretesting,re-classification,""survey experiments"",""survey questions"",validation}",Marketing,"Business, Management and Accounting",Social Sciences & Humanities,CS,2024,"the present study introduces the concept of validating data quality indicators through re-classification. we use the term re-classification to mean the evaluation of how well an indicator detects the quality of different versions of a survey question for which the quality is known a priori. we illustrate its application with two examples. in both, we make use of 12 questions from prior experiments that manipulated text features of questions to create ‘low’ and ‘high’ quality versions of each question. in the first example, we coded each question version in sqp 2.1 to obtain indicators of validity, reliability, and quality. we compared these indicators between the two versions of each question to assess whether the sqp outcomes were sensitive to text features. in the second example, we used a pretest survey to obtain three indicators of survey quality: response latencies, item nonresponse, and consistency over time. again, we compared these indicators between question versions to assess whether the indicators were sensitive to text features. we give recommendations for applying re-classification and an outlook for future research opportunities.",8
SOMD@NSLP2024: Overview and Insights from the Software Mention Detection Shared Task,"{""Software is a central part of the scientific process and involved in obtaining, analysing, visualising and processing research data. Understanding the provenance of research requires an understanding of the involved software. However, software citations in scientific publications often are informal, what creates challenges when aiming at understanding software adoption. This paper provides an overview of the Software Mention Detection (SOMD) shared task conducted as part of the 2024 Natural Scientific Language Processing Workshop, aiming at advancing the state-of-the-art with respect to NLP methods for detecting software mentions and additional information in scholarly publications. The SOMD shared task encompasses three subtasks, concerned with software mention recognition (subtask I), recognition of additional information (subtask II) and classification of involved relations (subtask III). We present an overview of the tasks, received submissions and used techniques. The best submissions achieved F1 scores of 0.74 (subtask I), 0.838 (subtask II) and 0.911 (subtask III) indicating both task feasibility but also potential for further performance gains.""}",2024,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"{""information extraction"",""relation classification"",""scholarly information processing"",""software mention extraction"",""software metadata identification""}",Computer Science (all),,,CS,2024,"software is a central part of the scientific process and involved in obtaining, analysing, visualising and processing research data. understanding the provenance of research requires an understanding of the involved software. however, software citations in scientific publications often are informal, what creates challenges when aiming at understanding software adoption. this paper provides an overview of the software mention detection (somd) shared task conducted as part of the 2024 natural scientific language processing workshop, aiming at advancing the state-of-the-art with respect to nlp methods for detecting software mentions and additional information in scholarly publications. the somd shared task encompasses three subtasks, concerned with software mention recognition (subtask i), recognition of additional information (subtask ii) and classification of involved relations (subtask iii). we present an overview of the tasks, received submissions and used techniques. the best submissions achieved f1 scores of 0.74 (subtask i), 0.838 (subtask ii) and 0.911 (subtask iii) indicating both task feasibility but also potential for further performance gains.",0
Uncovering latent profiles of ICT self-concept among adults in Germany and their relation with gender,"{""Self-concept related to the use of information and communication technology (ICT-SC) is reflected in how people feel and behave when confronted with digital technologies. Although evidence from variable-centered analyses suggests a hierarchical and multidimensional structure of ICT-SC in heterogeneous populations, it is not yet known whether different profiles of general ICT-SC and specific ICT-SC domains (communicate, process and store, generate content, safe application, solve problems) exist. This study aims to extend previous research using person-centered analyses and to examine whether different profiles of ICT-SC can be identified in a heterogeneous adult population (18–69 years) from Germany and how these profiles relate to gender. Results of a latent profile analysis (German quota sample, N = 369) indicate a reliable three-profile solution. Profile I (n = 48) is characterised by rather low ICT-SC with relative profile strengths in the verbal-interactive domains (communicate, process and store). Profile II (n = 149) is characterised by low to average ICT-SC across ICT-SC domains. Profile III (n = 172) is characterised by high ICT-SC with profile strengths in the technical-analytical domains (safe application, solve problems). Gender did not correlate significantly with profile membership. We discuss the practical implications of the results for ICT-SC interventions and suggest directions for future research.""}",2024,Behaviour and Information Technology,"{digcomp,""gender differences"",""ict competence"",""ict self-concept"",""latent profile analysis""}",Human-Computer Interaction,Computer Science,Physical Sciences,CS,2024,"self-concept related to the use of information and communication technology (ict-sc) is reflected in how people feel and behave when confronted with digital technologies. although evidence from variable-centered analyses suggests a hierarchical and multidimensional structure of ict-sc in heterogeneous populations, it is not yet known whether different profiles of general ict-sc and specific ict-sc domains (communicate, process and store, generate content, safe application, solve problems) exist. this study aims to extend previous research using person-centered analyses and to examine whether different profiles of ict-sc can be identified in a heterogeneous adult population (18–69 years) from germany and how these profiles relate to gender. results of a latent profile analysis (german quota sample, n = 369) indicate a reliable three-profile solution. profile i (n = 48) is characterised by rather low ict-sc with relative profile strengths in the verbal-interactive domains (communicate, process and store). profile ii (n = 149) is characterised by low to average ict-sc across ict-sc domains. profile iii (n = 172) is characterised by high ict-sc with profile strengths in the technical-analytical domains (safe application, solve problems). gender did not correlate significantly with profile membership. we discuss the practical implications of the results for ict-sc interventions and suggest directions for future research.",7
Multilingual Bot Accusations: How Different Linguistic Contexts Shape Perceptions of Social Bots,"{""Recent research indicates that the online use of the term \""bot\""has evolved over time. In the past, people used the term to accuse others of displaying automated behavior. However, it has gradually transformed into a linguistic tool to dehumanize the conversation partner, particularly on polarizing topics. Although this trend has been observed in English-speaking contexts, it is still unclear whether it holds true in other socio-linguistic environments. In this work we extend existing work on bot accusations and explore the phenomenon in a multilingual setting. We identify three distinct accusation patterns that characterize the different languages.""}",2024,"CPSS 2024 - 4th Workshop on Computational Linguistics for the Political and Social Sciences, Proceedings of the Workshop",,Computational Theory and Mathematics,Computer Science,Physical Sciences,CS,2024,"recent research indicates that the online use of the term \""bot\""has evolved over time. in the past, people used the term to accuse others of displaying automated behavior. however, it has gradually transformed into a linguistic tool to dehumanize the conversation partner, particularly on polarizing topics. although this trend has been observed in english-speaking contexts, it is still unclear whether it holds true in other socio-linguistic environments. in this work we extend existing work on bot accusations and explore the phenomenon in a multilingual setting. we identify three distinct accusation patterns that characterize the different languages.",7
Bibliometric-Enhanced Information Retrieval: 14th International BIR Workshop (BIR 2024),"{""The 14th iteration of the Bibliometric-enhanced Information Retrieval (BIR) workshop series takes place at ECIR 2024 as a full-day workshop. BIR addresses research topics related to academic search and recommendation, at the intersection of Information Retrieval, Natural Language Processing, and Bibliometrics. As an interdisciplinary scientific event, BIR brings together researchers and practitioners from the Scientometrics/Bibliometrics community on the one hand, and the Information Retrieval and NLP communities on the other hand. BIR is an ever-growing topic investigated by both academia and the industry.""}",2024,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"{""academic search"",bibliometrics,""digital libraries"",""information retrieval"",scientometrics}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2024,"the 14th iteration of the bibliometric-enhanced information retrieval (bir) workshop series takes place at ecir 2024 as a full-day workshop. bir addresses research topics related to academic search and recommendation, at the intersection of information retrieval, natural language processing, and bibliometrics. as an interdisciplinary scientific event, bir brings together researchers and practitioners from the scientometrics/bibliometrics community on the one hand, and the information retrieval and nlp communities on the other hand. bir is an ever-growing topic investigated by both academia and the industry.",2
Automating Citation Placement with Natural Language Processing and Transformers,"{""In scientific writing, references are crucial in supporting claims, spotlighting evidence, and highlighting research gaps. However, where to add a reference and which reference to cite are subjectively chosen by the papers’ authors; thus the automation of the task is challenging and requires proper investigations. This paper focuses on the automatic placement of references, considering its diverse approaches depending on writing style and community norms, and investigates the use of transformers and Natural Language Processing heuristics to predict i) if a reference is needed in a scientific statement, and ii) where the reference should be placed within the statement. For this investigation, this paper investigates two techniques, namely Mask-filling (MF) and Named Entity Recognition (NER), and provides insights on how to solve this task.""}",2024,CEUR Workshop Proceedings,"{""citation prediction"",""generative approach"",""named entity recognition"",""natural language processing""}",Computer Science (all),,,CS,2024,"in scientific writing, references are crucial in supporting claims, spotlighting evidence, and highlighting research gaps. however, where to add a reference and which reference to cite are subjectively chosen by the papers’ authors; thus the automation of the task is challenging and requires proper investigations. this paper focuses on the automatic placement of references, considering its diverse approaches depending on writing style and community norms, and investigates the use of transformers and natural language processing heuristics to predict i) if a reference is needed in a scientific statement, and ii) where the reference should be placed within the statement. for this investigation, this paper investigates two techniques, namely mask-filling (mf) and named entity recognition (ner), and provides insights on how to solve this task.",-1
A Novel Knowledge Plug-In for Incorporating Information About Employability From the O*NET Database Into Conversational Agents,"{""The labor market is a dynamic and rapidly evolving environment. Job positions that require expertise in various sectors often lead candidates to question their suitability. Therefore, it is crucial to furnish them with relevant, accurate, and timely information. In this article, we introduce a knowledge plug-in for existing conversational agents designed to address specific queries related to the job market domain. Additionally, we propose an innovative method for dynamically creating diverse grammars and semantically associating users' questions with a predefined list of domain questions. We present a novel scheme to effectively tackle question-answering tasks in settings with limited resources. Our architecture relies on question-understanding and response-generation modules, both powered by Transformers, the O*NET occupational database, and recommendation engines that suggest training materials. Furthermore, we conducted a user study based on the System Usability Scale (SUS) score, revealing that users highly appreciated the proposed tool. This sentiment was particularly evident when the tool was integrated with other artificial intelligence chatbots capable of handling general information. Simultaneously, our engine adeptly manages information within the investigated domain, providing precise responses and recommendations. This work addresses a critical gap in the delivery of employment information and paves the way for the development of diverse functionalities to assist both candidates and employers.""}",2024,IEEE Access,"{chatbots,""human-robot interaction"",""information extraction"",""labor market"",""online enrolling process"",""user experience"",""virtual assistant""}",Materials Science (all),,,CS,2024,"the labor market is a dynamic and rapidly evolving environment. job positions that require expertise in various sectors often lead candidates to question their suitability. therefore, it is crucial to furnish them with relevant, accurate, and timely information. in this article, we introduce a knowledge plug-in for existing conversational agents designed to address specific queries related to the job market domain. additionally, we propose an innovative method for dynamically creating diverse grammars and semantically associating users' questions with a predefined list of domain questions. we present a novel scheme to effectively tackle question-answering tasks in settings with limited resources. our architecture relies on question-understanding and response-generation modules, both powered by transformers, the o*net occupational database, and recommendation engines that suggest training materials. furthermore, we conducted a user study based on the system usability scale (sus) score, revealing that users highly appreciated the proposed tool. this sentiment was particularly evident when the tool was integrated with other artificial intelligence chatbots capable of handling general information. simultaneously, our engine adeptly manages information within the investigated domain, providing precise responses and recommendations. this work addresses a critical gap in the delivery of employment information and paves the way for the development of diverse functionalities to assist both candidates and employers.",1
Multilingual Bot Accusations: How Different Linguistic Contexts Shape Perceptions of Social Bots,"{""Recent research indicates that the online use of the term \""bot\""has evolved over time. In the past, people used the term to accuse others of displaying automated behavior. However, it has gradually transformed into a linguistic tool to dehumanize the conversation partner, particularly on polarizing topics. Although this trend has been observed in English-speaking contexts, it is still unclear whether it holds true in other socio-linguistic environments. In this work we extend existing work on bot accusations and explore the phenomenon in a multilingual setting. We identify three distinct accusation patterns that characterize the different languages.""}",2024,"CPSS 2024 - 4th Workshop on Computational Linguistics for the Political and Social Sciences, Proceedings of the Workshop",,Information Systems,Computer Science,Physical Sciences,CS,2024,"recent research indicates that the online use of the term \""bot\""has evolved over time. in the past, people used the term to accuse others of displaying automated behavior. however, it has gradually transformed into a linguistic tool to dehumanize the conversation partner, particularly on polarizing topics. although this trend has been observed in english-speaking contexts, it is still unclear whether it holds true in other socio-linguistic environments. in this work we extend existing work on bot accusations and explore the phenomenon in a multilingual setting. we identify three distinct accusation patterns that characterize the different languages.",7
Exploring Global Gender Gaps in the Blockchain Domain: Insights from LinkedIn Advertising Data,"{""Blockchain technology has gained widespread attention through Bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. There is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. By analyzing gender-disaggregated data from LinkedIn's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader IT sector. This study delves into the volume, velocity, variety, veracity, and value that LinkedIn Ad data offers to assess gender gaps in the blockchain domain at a global level.""}",2024,"Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024","{""gender gaps in the blockchain interests"",""gender gaps in the blockchain jobs"",""gender gaps in the blockchain skills"",""linkedin ad data"",""mining of social media data""}",Information Systems and Management,Decision Sciences,Social Sciences & Humanities,CS,2024,"blockchain technology has gained widespread attention through bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. there is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. by analyzing gender-disaggregated data from linkedin's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader it sector. this study delves into the volume, velocity, variety, veracity, and value that linkedin ad data offers to assess gender gaps in the blockchain domain at a global level.",3
Connected Components for Scaling Partial-order Blocking to Billion Entities,"{""In entity resolution, blocking pre-partitions data for further processing by more expensive methods. Two entity mentions are in the same block if they share identical or related blocking-keys. Previous work has sometimes related blocking keys by grouping or alphabetically sorting them, but-as was shown for author disambiguation-the respective equivalences or total orders are not necessarily well-suited to model the logical matching-relation between blocking keys. To address this, we present a novel blocking approach that exploits the subset partial order over entity representations to build a matching-based bipartite graph, using connected components as blocks. To prevent over-and underconnectedness, we allow specification of overly general and generalization of overly specific representations. To build the bipartite graph, we contribute a new parallellized algorithm with configurable time/space tradeoff for minimal element search in the subset partial order. As a job-based approach, it combines dynamic scalability and easier integration to make it more convenient than the previously described approaches. Experiments on large gold standards for publication records, author mentions, and affiliation strings suggest that our approach is competitive in performance and allows better addressing of domain-specific problems. For duplicate detection and author disambiguation, our method offers the expected performance as defined by the vector-similarity baseline used in another work on the same dataset and the common surname, first-initial baseline. For top-level institution resolution, we have reproduced the challenges described in prior work, strengthening the conclusion that for affiliation data, overlapping blocks under minimal elements are more suitable than connected components.""}",2024,Journal of Data and Information Quality,"{""additional key words and phrasesentity resolution"",blocking,lattices,""partial orders""}",Information Systems,Computer Science,Physical Sciences,CS,2024,"in entity resolution, blocking pre-partitions data for further processing by more expensive methods. two entity mentions are in the same block if they share identical or related blocking-keys. previous work has sometimes related blocking keys by grouping or alphabetically sorting them, but-as was shown for author disambiguation-the respective equivalences or total orders are not necessarily well-suited to model the logical matching-relation between blocking keys. to address this, we present a novel blocking approach that exploits the subset partial order over entity representations to build a matching-based bipartite graph, using connected components as blocks. to prevent over-and underconnectedness, we allow specification of overly general and generalization of overly specific representations. to build the bipartite graph, we contribute a new parallellized algorithm with configurable time/space tradeoff for minimal element search in the subset partial order. as a job-based approach, it combines dynamic scalability and easier integration to make it more convenient than the previously described approaches. experiments on large gold standards for publication records, author mentions, and affiliation strings suggest that our approach is competitive in performance and allows better addressing of domain-specific problems. for duplicate detection and author disambiguation, our method offers the expected performance as defined by the vector-similarity baseline used in another work on the same dataset and the common surname, first-initial baseline. for top-level institution resolution, we have reproduced the challenges described in prior work, strengthening the conclusion that for affiliation data, overlapping blocks under minimal elements are more suitable than connected components.",6
Overview of the Fourth Workshop on Scholarly Document Processing,"{""The workshop on Scholarly Document Processing (SDP) started in 2020 to accelerate research, inform policy and educate the public on natural language processing for scientific text. The fourth iteration of the workshop, SDP24 was held at the 62nd Annual Meeting of the Association for Computational Linguistics (ACL24) as a hybrid event. The SDP workshop saw a great increase in interest, with 57 submissions, of which 28 were accepted. The program consisted of a research track, four invited talks and two shared tasks: 1) DAGPap24: Detecting automatically generated scientific papers and 2) Context24: Multimodal Evidence and Grounding Context Identification for Scientific Claims. The program was geared towards NLP, information extraction, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and providing solutions to open challenges.""}",2024,"SDP 2024 - 4th Workshop on Scholarly Document Processing, Proceedings of the Workshop",,Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"the workshop on scholarly document processing (sdp) started in 2020 to accelerate research, inform policy and educate the public on natural language processing for scientific text. the fourth iteration of the workshop, sdp24 was held at the 62nd annual meeting of the association for computational linguistics (acl24) as a hybrid event. the sdp workshop saw a great increase in interest, with 57 submissions, of which 28 were accepted. the program consisted of a research track, four invited talks and two shared tasks: 1) dagpap24: detecting automatically generated scientific papers and 2) context24: multimodal evidence and grounding context identification for scientific claims. the program was geared towards nlp, information extraction, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and providing solutions to open challenges.",2
AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories,"{""We illustrate how standard psychometric inventories originally designed for assessing noncognitive human traits can be repurposed as diagnostic tools to evaluate analogous traits in large language models (LLMs). We start from the assumption that LLMs, inadvertently yet inevitably, acquire psychological traits (metaphorically speaking) from the vast text corpora on which they are trained. Such corpora contain sediments of the personalities, values, beliefs, and biases of the countless human authors of these texts, which LLMs learn through a complex training process. The traits that LLMs acquire in such a way can potentially influence their behavior, that is, their outputs in downstream tasks and applications in which they are employed, which in turn may have real-world consequences for individuals and social groups. By eliciting LLMs’ responses to language-based psychometric inventories, we can bring their traits to light. Psychometric profiling enables researchers to study and compare LLMs in terms of noncognitive characteristics, thereby providing a window into the personalities, values, beliefs, and biases these models exhibit (or mimic). We discuss the history of similar ideas and outline possible psychometric approaches for LLMs. We demonstrate one promising approach, zero-shot classification, for several LLMs and psychometric inventories. We conclude by highlighting open challenges and future avenues of research for AI Psychometrics.""}",2024,Perspectives on Psychological Science,"{""artificial intelligence"",""gender/sex diversity beliefs"",""large language model"",""moral foundations"",""natural language inference"",""natural language processing"",personality,psychometrics,values}",Psychology (all),,,CS,2024,"we illustrate how standard psychometric inventories originally designed for assessing noncognitive human traits can be repurposed as diagnostic tools to evaluate analogous traits in large language models (llms). we start from the assumption that llms, inadvertently yet inevitably, acquire psychological traits (metaphorically speaking) from the vast text corpora on which they are trained. such corpora contain sediments of the personalities, values, beliefs, and biases of the countless human authors of these texts, which llms learn through a complex training process. the traits that llms acquire in such a way can potentially influence their behavior, that is, their outputs in downstream tasks and applications in which they are employed, which in turn may have real-world consequences for individuals and social groups. by eliciting llms’ responses to language-based psychometric inventories, we can bring their traits to light. psychometric profiling enables researchers to study and compare llms in terms of noncognitive characteristics, thereby providing a window into the personalities, values, beliefs, and biases these models exhibit (or mimic). we discuss the history of similar ideas and outline possible psychometric approaches for llms. we demonstrate one promising approach, zero-shot classification, for several llms and psychometric inventories. we conclude by highlighting open challenges and future avenues of research for ai psychometrics.",0
Predicting political attitudes from web tracking data: a machine learning approach,"{""Anecdotal evidence suggests that the surge of populism and subsequent political polarization might make voters’ political preferences more detectable from digital trace data. This potential scenario could expose voters to the risk of being targeted and easily influenced by political actors. This study investigates the linkage between over 19,000,000 website visits, tracked from 1,003 users in Germany, and their survey responses to explore whether website choices can accurately predict political attitudes across five dimensions: Immigration, democracy, issues (such as climate and the European Union), populism, and trust. Our findings indicate a limited ability to identify political attitudes from individuals’ website visits. Our most effective machine learning algorithm predicted interest in politics and attitudes toward democracy but with dependency on model parameters. Although website categories exhibited suggestive patterns, they only marginally distinguished between individuals with anti- or pro-immigration attitudes, as well as those with populist or mainstream attitudes. This further confirm the reliability of surveys in measuring attitudes compared to digital trace data and, from a normative perspective, suggests that the potential to extract sensitive political information from online behavioral data, which could be utilized for microtargeting, remains limited.""}",2024,Journal of Information Technology and Politics,"{""life-style, immigration, climate change, democracy, european union"",""machine learning"",""political attitudes"",surveys,""web tracking data""}",Computer Science (all),,,CS,2024,"anecdotal evidence suggests that the surge of populism and subsequent political polarization might make voters’ political preferences more detectable from digital trace data. this potential scenario could expose voters to the risk of being targeted and easily influenced by political actors. this study investigates the linkage between over 19,000,000 website visits, tracked from 1,003 users in germany, and their survey responses to explore whether website choices can accurately predict political attitudes across five dimensions: immigration, democracy, issues (such as climate and the european union), populism, and trust. our findings indicate a limited ability to identify political attitudes from individuals’ website visits. our most effective machine learning algorithm predicted interest in politics and attitudes toward democracy but with dependency on model parameters. although website categories exhibited suggestive patterns, they only marginally distinguished between individuals with anti- or pro-immigration attitudes, as well as those with populist or mainstream attitudes. this further confirm the reliability of surveys in measuring attitudes compared to digital trace data and, from a normative perspective, suggests that the potential to extract sensitive political information from online behavioral data, which could be utilized for microtargeting, remains limited.",1
Mapping the spatial turn in social science energy research. A computational literature review,"{""Social science scholars have identified a “spatial turn” in energy research over the last three decades. This article systematically reviews the literature on energy, space, and place and decomposes this inter- and transdisciplinary academic landscape. A corpus of 7879 research articles related to spatial perspectives on energy issues is processed and analyzed based on a step-by-step framework for the automated, transparent, and reproducible analysis of large sets of research articles. For this purpose, natural language processing approaches, including named entity recognition and structural topic modeling, are adopted. Based on this large-n selection procedure, selected topics related to the geographical political economy of the energy transition are reviewed in detail. The review maps the geographical scope and scale of the research field, highlights major topics, and shows the distribution of methodological approaches and the role of geographic information systems in this research field. The results show a growing body of literature attentive to socio-spatial variation and the uneven spatiality of energy systems. Nevertheless, uneven geographical distributions of studies with a strong focus on the major industrialized countries and generally only a few comparative cases were also found. In particular, research on the energy transition and renewable energy policy is strongly informed by studies addressing the Global North, limiting the evidence base for other regional contexts from the Global South.""}",2024,Renewable and Sustainable Energy Reviews,"{""climate change"",""computational text analysis"",""energy transition"",geography,""spatial analysis""}","Renewable Energy, Sustainability and the Environment",Energy,Physical Sciences,CS,2024,"social science scholars have identified a “spatial turn” in energy research over the last three decades. this article systematically reviews the literature on energy, space, and place and decomposes this inter- and transdisciplinary academic landscape. a corpus of 7879 research articles related to spatial perspectives on energy issues is processed and analyzed based on a step-by-step framework for the automated, transparent, and reproducible analysis of large sets of research articles. for this purpose, natural language processing approaches, including named entity recognition and structural topic modeling, are adopted. based on this large-n selection procedure, selected topics related to the geographical political economy of the energy transition are reviewed in detail. the review maps the geographical scope and scale of the research field, highlights major topics, and shows the distribution of methodological approaches and the role of geographic information systems in this research field. the results show a growing body of literature attentive to socio-spatial variation and the uneven spatiality of energy systems. nevertheless, uneven geographical distributions of studies with a strong focus on the major industrialized countries and generally only a few comparative cases were also found. in particular, research on the energy transition and renewable energy policy is strongly informed by studies addressing the global north, limiting the evidence base for other regional contexts from the global south.",2
A mutually enhanced multi-scale relation-aware graph convolutional network for argument pair extraction,"{""Argument pair extraction (APE) is a fine-grained task of argument mining which aims to identify arguments offered by different participants in some discourse and detect interaction relationships between arguments from different participants. In recent years, many research efforts have been devoted to dealing with APE in a multi-task learning framework. Although these approaches have achieved encouraging results, they still face several challenging issues. First, different types of sentence relationships as well as different levels of information exchange among sentences are largely ignored. Second, they solely model interactions between argument pairs either in an explicit or implicit strategy, while neglecting the complementary effect of the two strategies. In this paper, we propose a novel Mutually Enhanced Multi-Scale Relation-Aware Graph Convolutional Network (MMR-GCN) for APE. Specifically, we first design a multi-scale relation-aware graph aggregation module to explicitly model the complex relationships between review and rebuttal passage sentences. In addition, we propose a mutually enhancement transformer module to implicitly and interactively enhance representations of review and rebuttal passage sentences. We experimentally validate MMR-GCN by comparing with the state-of-the-art APE methods. Experimental results show that it considerably outperforms all baseline methods, and the relative performance improvement of MMR-GCN over the best performing baseline MRC-APE in terms of F1 score reaches to 3.48% and 4.43% on the two benchmark datasets, respectively.""}",2024,Journal of Intelligent Information Systems,"{""argument mining"",""argument pair extraction"",""graph convolutional network"",transformer}",Software,Computer Science,Physical Sciences,CS,2024,"argument pair extraction (ape) is a fine-grained task of argument mining which aims to identify arguments offered by different participants in some discourse and detect interaction relationships between arguments from different participants. in recent years, many research efforts have been devoted to dealing with ape in a multi-task learning framework. although these approaches have achieved encouraging results, they still face several challenging issues. first, different types of sentence relationships as well as different levels of information exchange among sentences are largely ignored. second, they solely model interactions between argument pairs either in an explicit or implicit strategy, while neglecting the complementary effect of the two strategies. in this paper, we propose a novel mutually enhanced multi-scale relation-aware graph convolutional network (mmr-gcn) for ape. specifically, we first design a multi-scale relation-aware graph aggregation module to explicitly model the complex relationships between review and rebuttal passage sentences. in addition, we propose a mutually enhancement transformer module to implicitly and interactively enhance representations of review and rebuttal passage sentences. we experimentally validate mmr-gcn by comparing with the state-of-the-art ape methods. experimental results show that it considerably outperforms all baseline methods, and the relative performance improvement of mmr-gcn over the best performing baseline mrc-ape in terms of f1 score reaches to 3.48% and 4.43% on the two benchmark datasets, respectively.",6
Citation prediction by leveraging transformers and natural language processing heuristics,"{""In scientific papers, it is common practice to cite other articles to substantiate claims, provide evidence for factual assertions, reference limitations, and research gaps, and fulfill various other purposes. When authors include a citation in a given sentence, there are two considerations they need to take into account: (i) where in the sentence to place the citation and (ii) which citation to choose to support the underlying claim. In this paper, we focus on the first task as it allows multiple potential approaches that rely on the researcher's individual style and the specific norms and conventions of the relevant scientific community. We propose two automatic methodologies that leverage transformers architecture for either solving a Mask-Filling problem or a Named Entity Recognition problem. On top of the results of the proposed methodologies, we apply ad-hoc Natural Language Processing heuristics to further improve their outcome. We also introduce s2orc-9K, an open dataset for fine-tuning models on this task. A formal evaluation demonstrates that the generative approach significantly outperforms five alternative methods when fine-tuned on the novel dataset. Furthermore, this model's results show no statistically significant deviation from the outputs of three senior researchers.""}",2024,Information Processing and Management,"{bert,""citation prediction"",mask-filling,""named entity recognition"",""transformers architecture""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"in scientific papers, it is common practice to cite other articles to substantiate claims, provide evidence for factual assertions, reference limitations, and research gaps, and fulfill various other purposes. when authors include a citation in a given sentence, there are two considerations they need to take into account: (i) where in the sentence to place the citation and (ii) which citation to choose to support the underlying claim. in this paper, we focus on the first task as it allows multiple potential approaches that rely on the researcher's individual style and the specific norms and conventions of the relevant scientific community. we propose two automatic methodologies that leverage transformers architecture for either solving a mask-filling problem or a named entity recognition problem. on top of the results of the proposed methodologies, we apply ad-hoc natural language processing heuristics to further improve their outcome. we also introduce s2orc-9k, an open dataset for fine-tuning models on this task. a formal evaluation demonstrates that the generative approach significantly outperforms five alternative methods when fine-tuned on the novel dataset. furthermore, this model's results show no statistically significant deviation from the outputs of three senior researchers.",-1
Bias-aware ranking from pairwise comparisons,"{""Human feedback is often used, either directly or indirectly, as input to algorithmic decision making. However, humans are biased: if the algorithm that takes as input the human feedback does not control for potential biases, this might result in biased algorithmic decision making, which can have a tangible impact on people’s lives. In this paper, we study how to detect and correct for evaluators’ bias in the task of ranking people (or items) from pairwise comparisons. Specifically, we assume we are given pairwise comparisons of the items to be ranked produced by a set of evaluators. While the pairwise assessments of the evaluators should reflect to a certain extent the latent (unobservable) true quality scores of the items, they might be affected by each evaluator’s own bias against, or in favor, of some groups of items. By detecting and amending evaluators’ biases, we aim to produce a ranking of the items that is, as much as possible, in accordance with the ranking one would produce by having access to the latent quality scores. Our proposal is a novel method that extends the classic Bradley-Terry model by having a bias parameter for each evaluator which distorts the true quality score of each item, depending on the group the item belongs to. Thanks to the simplicity of the model, we are able to write explicitly its log-likelihood w.r.t. the parameters (i.e., items’ latent scores and evaluators’ bias) and optimize by means of the alternating approach. Our experiments on synthetic and real-world data confirm that our method is able to reconstruct the bias of each single evaluator extremely well and thus to outperform several non-trivial competitors in the task of producing a ranking which is as much as possible close to the unbiased ranking.""}",2024,Data Mining and Knowledge Discovery,"{bradley-terry,""evaluators’ bias"",fairness,""pairwise comparisons"",rankings}",Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"human feedback is often used, either directly or indirectly, as input to algorithmic decision making. however, humans are biased: if the algorithm that takes as input the human feedback does not control for potential biases, this might result in biased algorithmic decision making, which can have a tangible impact on people’s lives. in this paper, we study how to detect and correct for evaluators’ bias in the task of ranking people (or items) from pairwise comparisons. specifically, we assume we are given pairwise comparisons of the items to be ranked produced by a set of evaluators. while the pairwise assessments of the evaluators should reflect to a certain extent the latent (unobservable) true quality scores of the items, they might be affected by each evaluator’s own bias against, or in favor, of some groups of items. by detecting and amending evaluators’ biases, we aim to produce a ranking of the items that is, as much as possible, in accordance with the ranking one would produce by having access to the latent quality scores. our proposal is a novel method that extends the classic bradley-terry model by having a bias parameter for each evaluator which distorts the true quality score of each item, depending on the group the item belongs to. thanks to the simplicity of the model, we are able to write explicitly its log-likelihood w.r.t. the parameters (i.e., items’ latent scores and evaluators’ bias) and optimize by means of the alternating approach. our experiments on synthetic and real-world data confirm that our method is able to reconstruct the bias of each single evaluator extremely well and thus to outperform several non-trivial competitors in the task of producing a ranking which is as much as possible close to the unbiased ranking.",5
A Novel Knowledge Plug-In for Incorporating Information About Employability From the O*NET Database Into Conversational Agents,"{""The labor market is a dynamic and rapidly evolving environment. Job positions that require expertise in various sectors often lead candidates to question their suitability. Therefore, it is crucial to furnish them with relevant, accurate, and timely information. In this article, we introduce a knowledge plug-in for existing conversational agents designed to address specific queries related to the job market domain. Additionally, we propose an innovative method for dynamically creating diverse grammars and semantically associating users' questions with a predefined list of domain questions. We present a novel scheme to effectively tackle question-answering tasks in settings with limited resources. Our architecture relies on question-understanding and response-generation modules, both powered by Transformers, the O*NET occupational database, and recommendation engines that suggest training materials. Furthermore, we conducted a user study based on the System Usability Scale (SUS) score, revealing that users highly appreciated the proposed tool. This sentiment was particularly evident when the tool was integrated with other artificial intelligence chatbots capable of handling general information. Simultaneously, our engine adeptly manages information within the investigated domain, providing precise responses and recommendations. This work addresses a critical gap in the delivery of employment information and paves the way for the development of diverse functionalities to assist both candidates and employers.""}",2024,IEEE Access,"{chatbots,""human-robot interaction"",""information extraction"",""labor market"",""online enrolling process"",""user experience"",""virtual assistant""}",Computer Science (all),,,CS,2024,"the labor market is a dynamic and rapidly evolving environment. job positions that require expertise in various sectors often lead candidates to question their suitability. therefore, it is crucial to furnish them with relevant, accurate, and timely information. in this article, we introduce a knowledge plug-in for existing conversational agents designed to address specific queries related to the job market domain. additionally, we propose an innovative method for dynamically creating diverse grammars and semantically associating users' questions with a predefined list of domain questions. we present a novel scheme to effectively tackle question-answering tasks in settings with limited resources. our architecture relies on question-understanding and response-generation modules, both powered by transformers, the o*net occupational database, and recommendation engines that suggest training materials. furthermore, we conducted a user study based on the system usability scale (sus) score, revealing that users highly appreciated the proposed tool. this sentiment was particularly evident when the tool was integrated with other artificial intelligence chatbots capable of handling general information. simultaneously, our engine adeptly manages information within the investigated domain, providing precise responses and recommendations. this work addresses a critical gap in the delivery of employment information and paves the way for the development of diverse functionalities to assist both candidates and employers.",1
Embedding models for supervised automatic extraction and classification of named entities in scientific acknowledgements,"{""Acknowledgments in scientific papers may give an insight into aspects of the scientific community, such as reward systems, collaboration patterns, and hidden research trends. The aim of the paper is to evaluate the performance of different embedding models for the task of automatic extraction and classification of acknowledged entities from the acknowledgment text in scientific papers. We trained and implemented a named entity recognition (NER) task using the flair NLP framework. The training was conducted using three default Flair NER models with four differently-sized corpora and different versions of the flair NLP framework. The Flair Embeddings model trained on the medium corpus with the latest FLAIR version showed the best accuracy of 0.79. Expanding the size of a training corpus from very small to medium size massively increased the accuracy of all training algorithms, but further expansion of the training corpus did not bring further improvement. Moreover, the performance of the model slightly deteriorated. Our model is able to recognize six entity types: funding agency, grant number, individuals, university, corporation, and miscellaneous. The model works more precisely for some entity types than for others; thus, individuals and grant numbers showed a very good F1-Score over 0.9. Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data. This model can be applied for the comprehensive analysis of acknowledgment texts and may potentially make a great contribution to the field of automated acknowledgment analysis.""}",2024,Scientometrics,"{acknowledgement,""flair nlp-framework"",""named entity recognition"",""natural language processing"",""text mining"",""web of science""}",Social Sciences (all),,,CS,2024,"acknowledgments in scientific papers may give an insight into aspects of the scientific community, such as reward systems, collaboration patterns, and hidden research trends. the aim of the paper is to evaluate the performance of different embedding models for the task of automatic extraction and classification of acknowledged entities from the acknowledgment text in scientific papers. we trained and implemented a named entity recognition (ner) task using the flair nlp framework. the training was conducted using three default flair ner models with four differently-sized corpora and different versions of the flair nlp framework. the flair embeddings model trained on the medium corpus with the latest flair version showed the best accuracy of 0.79. expanding the size of a training corpus from very small to medium size massively increased the accuracy of all training algorithms, but further expansion of the training corpus did not bring further improvement. moreover, the performance of the model slightly deteriorated. our model is able to recognize six entity types: funding agency, grant number, individuals, university, corporation, and miscellaneous. the model works more precisely for some entity types than for others; thus, individuals and grant numbers showed a very good f1-score over 0.9. most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data. this model can be applied for the comprehensive analysis of acknowledgment texts and may potentially make a great contribution to the field of automated acknowledgment analysis.",8
Self-reported Demographics and Discourse Dynamics in a Persuasive Online Forum,"{""Research on language as interactive discourse underscores the deliberate use of demographic parameters such as gender, ethnicity, and class to shape social identities. For example, by explicitly disclosing one's information and enforcing one's social identity to an online community, the reception by and interaction with the said community is impacted, e.g., strengthening one's opinions by depicting the speaker as credible through their experience in the subject. Here, we present a first thorough study of the role and effects of self-disclosures on online discourse dynamics, focusing on a pervasive type of self-disclosure: author gender. Concretely, we investigate the contexts and properties of gender self-disclosures and their impact on interaction dynamics in an online persuasive forum, ChangeMyView. Our contribution is twofold. At the level of the target phenomenon, we fill a research gap in the understanding of the impact of these self-disclosures on the discourse by bringing together features related to forum activity (votes, number of comments), linguistic/stylistic features from the literature, and discourse topics. At the level of the contributed resource, we enrich and release a comprehensive dataset that will provide a further impulse for research on the interplay between gender disclosures, community interaction, and persuasion in online discourse.""}",2024,"2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings","{""corpus (creation annotation etc.)"",""opinion mining/sentiment analysis"",other}",Computational Theory and Mathematics,Computer Science,Physical Sciences,CS,2024,"research on language as interactive discourse underscores the deliberate use of demographic parameters such as gender, ethnicity, and class to shape social identities. for example, by explicitly disclosing one's information and enforcing one's social identity to an online community, the reception by and interaction with the said community is impacted, e.g., strengthening one's opinions by depicting the speaker as credible through their experience in the subject. here, we present a first thorough study of the role and effects of self-disclosures on online discourse dynamics, focusing on a pervasive type of self-disclosure: author gender. concretely, we investigate the contexts and properties of gender self-disclosures and their impact on interaction dynamics in an online persuasive forum, changemyview. our contribution is twofold. at the level of the target phenomenon, we fill a research gap in the understanding of the impact of these self-disclosures on the discourse by bringing together features related to forum activity (votes, number of comments), linguistic/stylistic features from the literature, and discourse topics. at the level of the contributed resource, we enrich and release a comprehensive dataset that will provide a further impulse for research on the interplay between gender disclosures, community interaction, and persuasion in online discourse.",7
Self-reported Demographics and Discourse Dynamics in a Persuasive Online Forum,"{""Research on language as interactive discourse underscores the deliberate use of demographic parameters such as gender, ethnicity, and class to shape social identities. For example, by explicitly disclosing one's information and enforcing one's social identity to an online community, the reception by and interaction with the said community is impacted, e.g., strengthening one's opinions by depicting the speaker as credible through their experience in the subject. Here, we present a first thorough study of the role and effects of self-disclosures on online discourse dynamics, focusing on a pervasive type of self-disclosure: author gender. Concretely, we investigate the contexts and properties of gender self-disclosures and their impact on interaction dynamics in an online persuasive forum, ChangeMyView. Our contribution is twofold. At the level of the target phenomenon, we fill a research gap in the understanding of the impact of these self-disclosures on the discourse by bringing together features related to forum activity (votes, number of comments), linguistic/stylistic features from the literature, and discourse topics. At the level of the contributed resource, we enrich and release a comprehensive dataset that will provide a further impulse for research on the interplay between gender disclosures, community interaction, and persuasion in online discourse.""}",2024,"2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings","{""corpus (creation annotation etc.)"",""opinion mining/sentiment analysis"",other}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2024,"research on language as interactive discourse underscores the deliberate use of demographic parameters such as gender, ethnicity, and class to shape social identities. for example, by explicitly disclosing one's information and enforcing one's social identity to an online community, the reception by and interaction with the said community is impacted, e.g., strengthening one's opinions by depicting the speaker as credible through their experience in the subject. here, we present a first thorough study of the role and effects of self-disclosures on online discourse dynamics, focusing on a pervasive type of self-disclosure: author gender. concretely, we investigate the contexts and properties of gender self-disclosures and their impact on interaction dynamics in an online persuasive forum, changemyview. our contribution is twofold. at the level of the target phenomenon, we fill a research gap in the understanding of the impact of these self-disclosures on the discourse by bringing together features related to forum activity (votes, number of comments), linguistic/stylistic features from the literature, and discourse topics. at the level of the contributed resource, we enrich and release a comprehensive dataset that will provide a further impulse for research on the interplay between gender disclosures, community interaction, and persuasion in online discourse.",7
Computational reproducibility in computational social science,"{""Open science practices have been widely discussed and have been implemented with varying success in different disciplines. We argue that computational-x disciplines such as computational social science, are also susceptible to the symptoms of the crises, but in terms of reproducibility. We expand the binary definition of reproducibility into a tier system which allows increasing levels of reproducibility based on external verifiability to counteract the practice of open-washing. We provide solutions for barriers in Computational Social Science that hinder researchers from obtaining the highest level of reproducibility, including the use of alternate data sources and considering reproducibility proactively.""}",2024,EPJ Data Science,"{""computational social science"",""open science"",""replicability crisis"",reproducibility}",Computational Mathematics,Mathematics,Physical Sciences,CS,2024,"open science practices have been widely discussed and have been implemented with varying success in different disciplines. we argue that computational-x disciplines such as computational social science, are also susceptible to the symptoms of the crises, but in terms of reproducibility. we expand the binary definition of reproducibility into a tier system which allows increasing levels of reproducibility based on external verifiability to counteract the practice of open-washing. we provide solutions for barriers in computational social science that hinder researchers from obtaining the highest level of reproducibility, including the use of alternate data sources and considering reproducibility proactively.",3
Computational reproducibility in computational social science,"{""Open science practices have been widely discussed and have been implemented with varying success in different disciplines. We argue that computational-x disciplines such as computational social science, are also susceptible to the symptoms of the crises, but in terms of reproducibility. We expand the binary definition of reproducibility into a tier system which allows increasing levels of reproducibility based on external verifiability to counteract the practice of open-washing. We provide solutions for barriers in Computational Social Science that hinder researchers from obtaining the highest level of reproducibility, including the use of alternate data sources and considering reproducibility proactively.""}",2024,EPJ Data Science,"{""computational social science"",""open science"",""replicability crisis"",reproducibility}",Modeling and Simulation,Mathematics,Physical Sciences,CS,2024,"open science practices have been widely discussed and have been implemented with varying success in different disciplines. we argue that computational-x disciplines such as computational social science, are also susceptible to the symptoms of the crises, but in terms of reproducibility. we expand the binary definition of reproducibility into a tier system which allows increasing levels of reproducibility based on external verifiability to counteract the practice of open-washing. we provide solutions for barriers in computational social science that hinder researchers from obtaining the highest level of reproducibility, including the use of alternate data sources and considering reproducibility proactively.",3
Patterns in the Growth and Thematic Evolution of Artificial Intelligence Research: A Study Using Bradford Distribution of Productivity and Path Analysis,"{""Artificial intelligence (AI) has emerged as a transformative technology with applications across multiple domains. The corpus of work related to the field of AI has grown significantly in volume as well as in terms of the application of AI in wider domains. However, given the wide application of AI in diverse areas, the measurement and characterization of the span of AI research is often a challenging task. Bibliometrics is a well-established method in the scientific community to measure the patterns and impact of research. It however has also received significant criticism for its overemphasis on the macroscopic picture and the inability to provide a deep understanding of growth and thematic structure of knowledge-creation activities. Therefore, this study presents a framework comprising of two techniques, namely, Bradford's distribution and path analysis to characterize the growth and thematic evolution of the discipline. While the Bradford distribution provides a macroscopic view of artificial intelligence research in terms of patterns of growth, the path analysis method presents a microscopic analysis of the thematic evolutionary trajectories, thereby completing the analytical framework. Detailed insights into the evolution of each subdomain are drawn, major techniques employed in various AI applications are identified, and some relevant implications are discussed to demonstrate the usefulness of the analyses.""}",2024,International Journal of Intelligent Systems,,Artificial Intelligence,Computer Science,Physical Sciences,CS,2024,"artificial intelligence (ai) has emerged as a transformative technology with applications across multiple domains. the corpus of work related to the field of ai has grown significantly in volume as well as in terms of the application of ai in wider domains. however, given the wide application of ai in diverse areas, the measurement and characterization of the span of ai research is often a challenging task. bibliometrics is a well-established method in the scientific community to measure the patterns and impact of research. it however has also received significant criticism for its overemphasis on the macroscopic picture and the inability to provide a deep understanding of growth and thematic structure of knowledge-creation activities. therefore, this study presents a framework comprising of two techniques, namely, bradford's distribution and path analysis to characterize the growth and thematic evolution of the discipline. while the bradford distribution provides a macroscopic view of artificial intelligence research in terms of patterns of growth, the path analysis method presents a microscopic analysis of the thematic evolutionary trajectories, thereby completing the analytical framework. detailed insights into the evolution of each subdomain are drawn, major techniques employed in various ai applications are identified, and some relevant implications are discussed to demonstrate the usefulness of the analyses.",7
Self-reported Demographics and Discourse Dynamics in a Persuasive Online Forum,"{""Research on language as interactive discourse underscores the deliberate use of demographic parameters such as gender, ethnicity, and class to shape social identities. For example, by explicitly disclosing one's information and enforcing one's social identity to an online community, the reception by and interaction with the said community is impacted, e.g., strengthening one's opinions by depicting the speaker as credible through their experience in the subject. Here, we present a first thorough study of the role and effects of self-disclosures on online discourse dynamics, focusing on a pervasive type of self-disclosure: author gender. Concretely, we investigate the contexts and properties of gender self-disclosures and their impact on interaction dynamics in an online persuasive forum, ChangeMyView. Our contribution is twofold. At the level of the target phenomenon, we fill a research gap in the understanding of the impact of these self-disclosures on the discourse by bringing together features related to forum activity (votes, number of comments), linguistic/stylistic features from the literature, and discourse topics. At the level of the contributed resource, we enrich and release a comprehensive dataset that will provide a further impulse for research on the interplay between gender disclosures, community interaction, and persuasion in online discourse.""}",2024,"2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings","{""corpus (creation annotation etc.)"",""opinion mining/sentiment analysis"",other}",Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"research on language as interactive discourse underscores the deliberate use of demographic parameters such as gender, ethnicity, and class to shape social identities. for example, by explicitly disclosing one's information and enforcing one's social identity to an online community, the reception by and interaction with the said community is impacted, e.g., strengthening one's opinions by depicting the speaker as credible through their experience in the subject. here, we present a first thorough study of the role and effects of self-disclosures on online discourse dynamics, focusing on a pervasive type of self-disclosure: author gender. concretely, we investigate the contexts and properties of gender self-disclosures and their impact on interaction dynamics in an online persuasive forum, changemyview. our contribution is twofold. at the level of the target phenomenon, we fill a research gap in the understanding of the impact of these self-disclosures on the discourse by bringing together features related to forum activity (votes, number of comments), linguistic/stylistic features from the literature, and discourse topics. at the level of the contributed resource, we enrich and release a comprehensive dataset that will provide a further impulse for research on the interplay between gender disclosures, community interaction, and persuasion in online discourse.",7
"Introducing the Democratic Electoral Systems data, 1919-1945","{""This data note introduces an update to the widely-used Democratic Electoral Systems (DES) data that encompasses the period from 1919 to 1945. The data include 243 legislative lower house and presidential elections in 34 interwar democracies. Information on these elections falls into four categories: first and foremost, DES contains variables that capture the institutional rules that define how elections are organized. Second, the data captures the consequences of electoral rules in the form of summary statistics of electoral outcomes. Third, we include democracy classifications for four major democracy datasets so that users can choose their preferred democracy definition when working with the data. Finally, the DES dataset contains multiple identification variables that allow linking the DES data to a wide variety of other datasets. This update to the DES data is fully compatible with prior releases for the post-war period1–3.""}",2024,Open Research Europe,"{democracy,""electoral rules"",""majoritarian systems"",""parliamentary elections"",""presidential elections"",""proportional representation""}",Multidisciplinary,Multidisciplinary,Multidisciplinary,CS,2024,"this data note introduces an update to the widely-used democratic electoral systems (des) data that encompasses the period from 1919 to 1945. the data include 243 legislative lower house and presidential elections in 34 interwar democracies. information on these elections falls into four categories: first and foremost, des contains variables that capture the institutional rules that define how elections are organized. second, the data captures the consequences of electoral rules in the form of summary statistics of electoral outcomes. third, we include democracy classifications for four major democracy datasets so that users can choose their preferred democracy definition when working with the data. finally, the des dataset contains multiple identification variables that allow linking the des data to a wide variety of other datasets. this update to the des data is fully compatible with prior releases for the post-war period1–3.",1
Knowledge Graphs for Digital Transformation Monitoring in Social Media,"{""Several techniques and workflows have emerged recently for automatically extracting knowledge graphs from documents like scientific articles and patents. However, adapting these approaches to integrate alternative text sources such as micro-blogging posts and news and to model open-domain entities and relationships commonly found in these sources is still challenging. This paper introduces an improved information extraction pipeline designed specifically for extracting a knowledge graph comprising open-domain entities from micro-blogging posts on social media platforms. Our pipeline utilizes dependency parsing and employs unsupervised classification of entity relations through hierarchical clustering over word embeddings. We present a case study involving the extraction of semantic triples from a tweet collection concerning digital transformation and show through two experimental evaluations on the same dataset that our system achieves precision rates exceeding 95% and surpasses similar pipelines by approximately 5% in terms of precision, while also generating a notably higher number of triples.""}",2024,CEUR Workshop Proceedings,"{""hierarchical clustering"",""information extraction"",""knowledge graphs"",""named entity recognition"",""social media analysis"",""word embeddings""}",Computer Science (all),,,CS,2024,"several techniques and workflows have emerged recently for automatically extracting knowledge graphs from documents like scientific articles and patents. however, adapting these approaches to integrate alternative text sources such as micro-blogging posts and news and to model open-domain entities and relationships commonly found in these sources is still challenging. this paper introduces an improved information extraction pipeline designed specifically for extracting a knowledge graph comprising open-domain entities from micro-blogging posts on social media platforms. our pipeline utilizes dependency parsing and employs unsupervised classification of entity relations through hierarchical clustering over word embeddings. we present a case study involving the extraction of semantic triples from a tweet collection concerning digital transformation and show through two experimental evaluations on the same dataset that our system achieves precision rates exceeding 95% and surpasses similar pipelines by approximately 5% in terms of precision, while also generating a notably higher number of triples.",5
Novelty in News Search: A Longitudinal Study of the 2020 US Elections,"{""The 2020 US elections news coverage was extensive, with new pieces of information generated rapidly. This evolving scenario presented an opportunity to study the performance of search engines in a context in which they had to quickly process information as it was published. We analyze novelty, a measurement of new items that emerge in the top news search results, to compare the coverage and visibility of different topics. Using virtual agents that simulate human web browsing behavior to collect search engine result pages, we conduct a longitudinal study of news results of five search engines collected in short bursts (every 21 minutes) from two regions (Oregon, US and Frankfurt, Germany), starting on election day and lasting until one day after the announcement of Biden as the winner. We find more new items emerging for election related queries (“joe biden,” “donald trump,” and “us elections”) compared to topical (e.g., “coronavirus”) or stable (e.g., “holocaust”) queries. We demonstrate that our method captures sudden changes in highly covered news topics as well as multiple differences across search engines and regions over time. We highlight novelty imbalances between candidate queries which affect their visibility during electoral periods, and conclude that, when it comes to news, search engines are responsible for such imbalances, either due to their algorithms or the set of news sources that they rely on.""}",2024,Social Science Computer Review,,Social Sciences (all),,,CS,2024,"the 2020 us elections news coverage was extensive, with new pieces of information generated rapidly. this evolving scenario presented an opportunity to study the performance of search engines in a context in which they had to quickly process information as it was published. we analyze novelty, a measurement of new items that emerge in the top news search results, to compare the coverage and visibility of different topics. using virtual agents that simulate human web browsing behavior to collect search engine result pages, we conduct a longitudinal study of news results of five search engines collected in short bursts (every 21 minutes) from two regions (oregon, us and frankfurt, germany), starting on election day and lasting until one day after the announcement of biden as the winner. we find more new items emerging for election related queries (“joe biden,” “donald trump,” and “us elections”) compared to topical (e.g., “coronavirus”) or stable (e.g., “holocaust”) queries. we demonstrate that our method captures sudden changes in highly covered news topics as well as multiple differences across search engines and regions over time. we highlight novelty imbalances between candidate queries which affect their visibility during electoral periods, and conclude that, when it comes to news, search engines are responsible for such imbalances, either due to their algorithms or the set of news sources that they rely on.",1
"Investigating Characteristics, Biases and Evolution of Fact-Checked Claims on the Web","{""Given the recent proliferation of fake news online, fact-checking has emerged as a critical defence against misinformation. Several fact-checking organisations are currently employed in the initiative to assess the truthfulness of online claims. Verified claims serve as foundational data for various cross-domain research, including fields of social science and natural language processing, where they are used to study misinformation and several downstream tasks such as automated fact-verification. However, these fact-checking websites inherently harbour biases, posing challenges for academic endeavours aiming to discern truth from misinformation. In this study, we aim to explore the evolving landscape of online claims verified by multiple fact-checking organisations and analyse the underlying biases of individual fact-checking websites. Leveraging ClaimsKG, the largest available corpus of fact-checked claims, we analyse the temporal evolution of claims, focusing on topics, veracity levels, and entities to offer insights into the complex dimensions of online information. We utilise data and dimensions available from ClaimsKG for our analysis and for dimensions such as topics which are not present in ClaimsKG, we create a topic taxonomy and implement a transformer-based model, for multi-label classification of claims. We also observe how similar claims are co-occurant amongst different websites. Our work serves as a standardised framework for categorising claims sourced from diverse fact-checking organisations, laying the foundation for coherent and interpretable fact-checking datasets. The analysis conducted in this work sheds light on the dynamic landscape of online claims verified by several fact-checking organisations and dives into biases and distributions of several fact-checking websites.""}",2024,HT 2024: Creative Intelligence - 35th ACM Conference on Hypertext and Social Media,"{""claims analysis"",""claims classification"",fact-checking,""knowledge graphs"",""mis- and disinformation""}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2024,"given the recent proliferation of fake news online, fact-checking has emerged as a critical defence against misinformation. several fact-checking organisations are currently employed in the initiative to assess the truthfulness of online claims. verified claims serve as foundational data for various cross-domain research, including fields of social science and natural language processing, where they are used to study misinformation and several downstream tasks such as automated fact-verification. however, these fact-checking websites inherently harbour biases, posing challenges for academic endeavours aiming to discern truth from misinformation. in this study, we aim to explore the evolving landscape of online claims verified by multiple fact-checking organisations and analyse the underlying biases of individual fact-checking websites. leveraging claimskg, the largest available corpus of fact-checked claims, we analyse the temporal evolution of claims, focusing on topics, veracity levels, and entities to offer insights into the complex dimensions of online information. we utilise data and dimensions available from claimskg for our analysis and for dimensions such as topics which are not present in claimskg, we create a topic taxonomy and implement a transformer-based model, for multi-label classification of claims. we also observe how similar claims are co-occurant amongst different websites. our work serves as a standardised framework for categorising claims sourced from diverse fact-checking organisations, laying the foundation for coherent and interpretable fact-checking datasets. the analysis conducted in this work sheds light on the dynamic landscape of online claims verified by several fact-checking organisations and dives into biases and distributions of several fact-checking websites.",1
Examining bias perpetuation in academic search engines: An algorithm audit of Google and Semantic Scholar,"{""Researchers rely on academic Web search engines to find scientific sources, but search engine mechanisms may selectively present content that aligns with biases embedded in queries. This study examines whether confirmation biased queries prompted into Google Scholar and Semantic Scholar will yield results aligned with a query’s bias. Six queries (topics across health and technology domains such as ‘vaccines’, ‘Internet use’) were analyzed for disparities in search results. We confirm that biased queries (targeting ‘benefits’ or ‘risks’) affect search results in line with bias, with technology-related queries displaying more significant disparities. Overall, Semantic Scholar exhibited fewer disparities than Google Scholar. Topics rated as more polarizing did not consistently show more disparate results. Academic search results that perpetuate confirmation bias have strong implications for both researchers and citizens searching for evidence. More research is needed to explore how scientific inquiry and academic search engines interact.""}",2024,First Monday,,Human-Computer Interaction,Computer Science,Physical Sciences,CS,2024,"researchers rely on academic web search engines to find scientific sources, but search engine mechanisms may selectively present content that aligns with biases embedded in queries. this study examines whether confirmation biased queries prompted into google scholar and semantic scholar will yield results aligned with a query’s bias. six queries (topics across health and technology domains such as ‘vaccines’, ‘internet use’) were analyzed for disparities in search results. we confirm that biased queries (targeting ‘benefits’ or ‘risks’) affect search results in line with bias, with technology-related queries displaying more significant disparities. overall, semantic scholar exhibited fewer disparities than google scholar. topics rated as more polarizing did not consistently show more disparate results. academic search results that perpetuate confirmation bias have strong implications for both researchers and citizens searching for evidence. more research is needed to explore how scientific inquiry and academic search engines interact.",5
Dissecting Paraphrases: The Impact of Prompt Syntax and Supplementary Information on Knowledge Retrieval from Pretrained Language Models,"{""Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge. One method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. Typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. Simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. We designed CONPARELAMA – a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. CONPARELAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of PLMs. Extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying PLMs with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. In addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.""}",2024,Long Papers,,Hardware and Architecture,Computer Science,Physical Sciences,CS,2024,"pre-trained language models (plms) are known to contain various kinds of knowledge. one method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. we designed conparelama – a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. these paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. conparelama enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of plms. extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying plms with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. in addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.",0
The Unseen Targets of Hate: A Systematic Review of Hateful Communication Datasets,"{""Machine learning (ML)-based content moderation tools are essential to keep online spaces free from hateful communication. Yet ML tools can only be as capable as the quality of the data they are trained on allows them. While there is increasing evidence that they underperform in detecting hateful communications directed towards specific identities and may discriminate against them, we know surprisingly little about the provenance of such bias. To fill this gap, we present a systematic review of the datasets for the automated detection of hateful communication introduced over the past decade, and unpack the quality of the datasets in terms of the identities that they embody: those of the targets of hateful communication that the data curators focused on, as well as those unintentionally included in the datasets. We find, overall, a skewed representation of selected target identities and mismatches between the targets that research conceptualizes and ultimately includes in datasets. Yet, by contextualizing these findings in the language and location of origin of the datasets, we highlight a positive trend towards the broadening and diversification of this research space.""}",2024,Social Science Computer Review,"{""data quality"",""hate targets"",""hateful online communication"",multilinguality,""systematic review""}",Social Sciences (all),,,CS,2024,"machine learning (ml)-based content moderation tools are essential to keep online spaces free from hateful communication. yet ml tools can only be as capable as the quality of the data they are trained on allows them. while there is increasing evidence that they underperform in detecting hateful communications directed towards specific identities and may discriminate against them, we know surprisingly little about the provenance of such bias. to fill this gap, we present a systematic review of the datasets for the automated detection of hateful communication introduced over the past decade, and unpack the quality of the datasets in terms of the identities that they embody: those of the targets of hateful communication that the data curators focused on, as well as those unintentionally included in the datasets. we find, overall, a skewed representation of selected target identities and mismatches between the targets that research conceptualizes and ultimately includes in datasets. yet, by contextualizing these findings in the language and location of origin of the datasets, we highlight a positive trend towards the broadening and diversification of this research space.",1
Incentivizing news consumption on social media platforms using large language models and realistic bot accounts,"{""Polarization, misinformation, declining trust, and wavering support for democratic norms are pressing threats to the US Exposure to verified and balanced news may make citizens more resilient to these threats. This project examines how to enhance users' exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. We rely on a 2-week long field experiment on 28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing a URL to the topic-relevant section of a verified and ideologically balanced news organization and an encouragement to follow its Twitter account. To test differential effects by gender of the bots, the treated users were randomly assigned to receive responses by bots presented as female or male. We examine whether our intervention enhances the following of news media organizations, sharing and liking of news content (determined by our extensive list of news media outlets), tweeting about politics, and liking of political content (determined using our fine-Tuned RoBERTa NLP transformer-based model). Although the treated users followed more news accounts and the users in the female bot treatment liked more news content than the control, these results were small in magnitude and confined to the already politically interested users, as indicated by their pretreatment tweeting about politics. In addition, the effects on liking and posting political content were uniformly null. These findings have implications for social media and news organizations and offer directions for pro-social computational interventions on platforms.""}",2024,PNAS Nexus,"{bots,""news avoidance"",""news engagement"",polarization,""social media""}",Multidisciplinary,Multidisciplinary,Multidisciplinary,CS,2024,"polarization, misinformation, declining trust, and wavering support for democratic norms are pressing threats to the us exposure to verified and balanced news may make citizens more resilient to these threats. this project examines how to enhance users' exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. we rely on a 2-week long field experiment on 28,457 twitter users. we created 28 bots utilizing gpt-2 that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing a url to the topic-relevant section of a verified and ideologically balanced news organization and an encouragement to follow its twitter account. to test differential effects by gender of the bots, the treated users were randomly assigned to receive responses by bots presented as female or male. we examine whether our intervention enhances the following of news media organizations, sharing and liking of news content (determined by our extensive list of news media outlets), tweeting about politics, and liking of political content (determined using our fine-tuned roberta nlp transformer-based model). although the treated users followed more news accounts and the users in the female bot treatment liked more news content than the control, these results were small in magnitude and confined to the already politically interested users, as indicated by their pretreatment tweeting about politics. in addition, the effects on liking and posting political content were uniformly null. these findings have implications for social media and news organizations and offer directions for pro-social computational interventions on platforms.",1
Examining bias perpetuation in academic search engines: An algorithm audit of Google and Semantic Scholar,"{""Researchers rely on academic Web search engines to find scientific sources, but search engine mechanisms may selectively present content that aligns with biases embedded in queries. This study examines whether confirmation biased queries prompted into Google Scholar and Semantic Scholar will yield results aligned with a query’s bias. Six queries (topics across health and technology domains such as ‘vaccines’, ‘Internet use’) were analyzed for disparities in search results. We confirm that biased queries (targeting ‘benefits’ or ‘risks’) affect search results in line with bias, with technology-related queries displaying more significant disparities. Overall, Semantic Scholar exhibited fewer disparities than Google Scholar. Topics rated as more polarizing did not consistently show more disparate results. Academic search results that perpetuate confirmation bias have strong implications for both researchers and citizens searching for evidence. More research is needed to explore how scientific inquiry and academic search engines interact.""}",2024,First Monday,,Computer Networks and Communications,Computer Science,Physical Sciences,CS,2024,"researchers rely on academic web search engines to find scientific sources, but search engine mechanisms may selectively present content that aligns with biases embedded in queries. this study examines whether confirmation biased queries prompted into google scholar and semantic scholar will yield results aligned with a query’s bias. six queries (topics across health and technology domains such as ‘vaccines’, ‘internet use’) were analyzed for disparities in search results. we confirm that biased queries (targeting ‘benefits’ or ‘risks’) affect search results in line with bias, with technology-related queries displaying more significant disparities. overall, semantic scholar exhibited fewer disparities than google scholar. topics rated as more polarizing did not consistently show more disparate results. academic search results that perpetuate confirmation bias have strong implications for both researchers and citizens searching for evidence. more research is needed to explore how scientific inquiry and academic search engines interact.",5
Enhancing Model Performance through Translation-based Data Augmentation in the context of Fake News Detection,"{""The rapid development of social media in recent years has encouraged the sharing of vast amounts of data and the propagation of fake news. This has pushed the scientific community to focus on this phenomenon, particularly those working on natural language processing, by developing detection tools to combat fake news. At the same time, most studies have focused on languages with a high resource content (corpus). This paper aims to shed light on low-resource languages, particularly the Algerian dialect, through an experimental study with two objectives. The first one is to verify if the automatic translation from Modern Standard Arabic (MSA) to the Algerian dialect can be considered an approach to increase the resources in the Algerian dialect, especially with the rise of large language models (LLMs). The second is to verify the impact of the translation-based data augmentation method on fake news detection by using transformer-based Arabic pre-trained models in different data augmentation configurations. We have discovered that LLMs can generate translations that closely resemble human translations. In this study, we demonstrate that data augmentation can result in saturation and a decline in model performance due to the introduction of noise and variations in writing styles.""}",2024,Procedia Computer Science,"{""algerian dialect"",""data augmentation"",""deep learning"",""fake news"",""machine translation""}",Computer Science (all),,,CS,2024,"the rapid development of social media in recent years has encouraged the sharing of vast amounts of data and the propagation of fake news. this has pushed the scientific community to focus on this phenomenon, particularly those working on natural language processing, by developing detection tools to combat fake news. at the same time, most studies have focused on languages with a high resource content (corpus). this paper aims to shed light on low-resource languages, particularly the algerian dialect, through an experimental study with two objectives. the first one is to verify if the automatic translation from modern standard arabic (msa) to the algerian dialect can be considered an approach to increase the resources in the algerian dialect, especially with the rise of large language models (llms). the second is to verify the impact of the translation-based data augmentation method on fake news detection by using transformer-based arabic pre-trained models in different data augmentation configurations. we have discovered that llms can generate translations that closely resemble human translations. in this study, we demonstrate that data augmentation can result in saturation and a decline in model performance due to the introduction of noise and variations in writing styles.",1
"Investigating Characteristics, Biases and Evolution of Fact-Checked Claims on the Web","{""Given the recent proliferation of fake news online, fact-checking has emerged as a critical defence against misinformation. Several fact-checking organisations are currently employed in the initiative to assess the truthfulness of online claims. Verified claims serve as foundational data for various cross-domain research, including fields of social science and natural language processing, where they are used to study misinformation and several downstream tasks such as automated fact-verification. However, these fact-checking websites inherently harbour biases, posing challenges for academic endeavours aiming to discern truth from misinformation. In this study, we aim to explore the evolving landscape of online claims verified by multiple fact-checking organisations and analyse the underlying biases of individual fact-checking websites. Leveraging ClaimsKG, the largest available corpus of fact-checked claims, we analyse the temporal evolution of claims, focusing on topics, veracity levels, and entities to offer insights into the complex dimensions of online information. We utilise data and dimensions available from ClaimsKG for our analysis and for dimensions such as topics which are not present in ClaimsKG, we create a topic taxonomy and implement a transformer-based model, for multi-label classification of claims. We also observe how similar claims are co-occurant amongst different websites. Our work serves as a standardised framework for categorising claims sourced from diverse fact-checking organisations, laying the foundation for coherent and interpretable fact-checking datasets. The analysis conducted in this work sheds light on the dynamic landscape of online claims verified by several fact-checking organisations and dives into biases and distributions of several fact-checking websites.""}",2024,HT 2024: Creative Intelligence - 35th ACM Conference on Hypertext and Social Media,"{""claims analysis"",""claims classification"",fact-checking,""knowledge graphs"",""mis- and disinformation""}",Software,Computer Science,Physical Sciences,CS,2024,"given the recent proliferation of fake news online, fact-checking has emerged as a critical defence against misinformation. several fact-checking organisations are currently employed in the initiative to assess the truthfulness of online claims. verified claims serve as foundational data for various cross-domain research, including fields of social science and natural language processing, where they are used to study misinformation and several downstream tasks such as automated fact-verification. however, these fact-checking websites inherently harbour biases, posing challenges for academic endeavours aiming to discern truth from misinformation. in this study, we aim to explore the evolving landscape of online claims verified by multiple fact-checking organisations and analyse the underlying biases of individual fact-checking websites. leveraging claimskg, the largest available corpus of fact-checked claims, we analyse the temporal evolution of claims, focusing on topics, veracity levels, and entities to offer insights into the complex dimensions of online information. we utilise data and dimensions available from claimskg for our analysis and for dimensions such as topics which are not present in claimskg, we create a topic taxonomy and implement a transformer-based model, for multi-label classification of claims. we also observe how similar claims are co-occurant amongst different websites. our work serves as a standardised framework for categorising claims sourced from diverse fact-checking organisations, laying the foundation for coherent and interpretable fact-checking datasets. the analysis conducted in this work sheds light on the dynamic landscape of online claims verified by several fact-checking organisations and dives into biases and distributions of several fact-checking websites.",1
Citation prediction by leveraging transformers and natural language processing heuristics,"{""In scientific papers, it is common practice to cite other articles to substantiate claims, provide evidence for factual assertions, reference limitations, and research gaps, and fulfill various other purposes. When authors include a citation in a given sentence, there are two considerations they need to take into account: (i) where in the sentence to place the citation and (ii) which citation to choose to support the underlying claim. In this paper, we focus on the first task as it allows multiple potential approaches that rely on the researcher's individual style and the specific norms and conventions of the relevant scientific community. We propose two automatic methodologies that leverage transformers architecture for either solving a Mask-Filling problem or a Named Entity Recognition problem. On top of the results of the proposed methodologies, we apply ad-hoc Natural Language Processing heuristics to further improve their outcome. We also introduce s2orc-9K, an open dataset for fine-tuning models on this task. A formal evaluation demonstrates that the generative approach significantly outperforms five alternative methods when fine-tuned on the novel dataset. Furthermore, this model's results show no statistically significant deviation from the outputs of three senior researchers.""}",2024,Information Processing and Management,"{bert,""citation prediction"",mask-filling,""named entity recognition"",""transformers architecture""}",Information Systems,Computer Science,Physical Sciences,CS,2024,"in scientific papers, it is common practice to cite other articles to substantiate claims, provide evidence for factual assertions, reference limitations, and research gaps, and fulfill various other purposes. when authors include a citation in a given sentence, there are two considerations they need to take into account: (i) where in the sentence to place the citation and (ii) which citation to choose to support the underlying claim. in this paper, we focus on the first task as it allows multiple potential approaches that rely on the researcher's individual style and the specific norms and conventions of the relevant scientific community. we propose two automatic methodologies that leverage transformers architecture for either solving a mask-filling problem or a named entity recognition problem. on top of the results of the proposed methodologies, we apply ad-hoc natural language processing heuristics to further improve their outcome. we also introduce s2orc-9k, an open dataset for fine-tuning models on this task. a formal evaluation demonstrates that the generative approach significantly outperforms five alternative methods when fine-tuned on the novel dataset. furthermore, this model's results show no statistically significant deviation from the outputs of three senior researchers.",-1
Tracing architecture of machine learning models through their mentions in scholarly articles,"{""Relation extraction, is a pivotal task in NLP, impacts information retrieval, natural language understanding (NLU) and knowledge generation. Machine learning model has coined itself as the most influential term in this era of deep learning and LLM. In scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. Knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. In this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. We attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. We report our findings with four state of the art baseline models. The findings report here exemplary performance with LUKE model as winner. The presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.""}",2024,"Proceedings - 2024 7th International Conference on Data Science and Information Technology, DSIT 2024","{baseline,extraction,""machine learning model"",""machine learning model architecture"",relation}",Control and Optimization,Mathematics,Physical Sciences,CS,2024,"relation extraction, is a pivotal task in nlp, impacts information retrieval, natural language understanding (nlu) and knowledge generation. machine learning model has coined itself as the most influential term in this era of deep learning and llm. in scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. in this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. we attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. we report our findings with four state of the art baseline models. the findings report here exemplary performance with luke model as winner. the presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.",6
Patterns in the Growth and Thematic Evolution of Artificial Intelligence Research: A Study Using Bradford Distribution of Productivity and Path Analysis,"{""Artificial intelligence (AI) has emerged as a transformative technology with applications across multiple domains. The corpus of work related to the field of AI has grown significantly in volume as well as in terms of the application of AI in wider domains. However, given the wide application of AI in diverse areas, the measurement and characterization of the span of AI research is often a challenging task. Bibliometrics is a well-established method in the scientific community to measure the patterns and impact of research. It however has also received significant criticism for its overemphasis on the macroscopic picture and the inability to provide a deep understanding of growth and thematic structure of knowledge-creation activities. Therefore, this study presents a framework comprising of two techniques, namely, Bradford's distribution and path analysis to characterize the growth and thematic evolution of the discipline. While the Bradford distribution provides a macroscopic view of artificial intelligence research in terms of patterns of growth, the path analysis method presents a microscopic analysis of the thematic evolutionary trajectories, thereby completing the analytical framework. Detailed insights into the evolution of each subdomain are drawn, major techniques employed in various AI applications are identified, and some relevant implications are discussed to demonstrate the usefulness of the analyses.""}",2024,International Journal of Intelligent Systems,,Human-Computer Interaction,Computer Science,Physical Sciences,CS,2024,"artificial intelligence (ai) has emerged as a transformative technology with applications across multiple domains. the corpus of work related to the field of ai has grown significantly in volume as well as in terms of the application of ai in wider domains. however, given the wide application of ai in diverse areas, the measurement and characterization of the span of ai research is often a challenging task. bibliometrics is a well-established method in the scientific community to measure the patterns and impact of research. it however has also received significant criticism for its overemphasis on the macroscopic picture and the inability to provide a deep understanding of growth and thematic structure of knowledge-creation activities. therefore, this study presents a framework comprising of two techniques, namely, bradford's distribution and path analysis to characterize the growth and thematic evolution of the discipline. while the bradford distribution provides a macroscopic view of artificial intelligence research in terms of patterns of growth, the path analysis method presents a microscopic analysis of the thematic evolutionary trajectories, thereby completing the analytical framework. detailed insights into the evolution of each subdomain are drawn, major techniques employed in various ai applications are identified, and some relevant implications are discussed to demonstrate the usefulness of the analyses.",7
Embedding models for supervised automatic extraction and classification of named entities in scientific acknowledgements,"{""Acknowledgments in scientific papers may give an insight into aspects of the scientific community, such as reward systems, collaboration patterns, and hidden research trends. The aim of the paper is to evaluate the performance of different embedding models for the task of automatic extraction and classification of acknowledged entities from the acknowledgment text in scientific papers. We trained and implemented a named entity recognition (NER) task using the flair NLP framework. The training was conducted using three default Flair NER models with four differently-sized corpora and different versions of the flair NLP framework. The Flair Embeddings model trained on the medium corpus with the latest FLAIR version showed the best accuracy of 0.79. Expanding the size of a training corpus from very small to medium size massively increased the accuracy of all training algorithms, but further expansion of the training corpus did not bring further improvement. Moreover, the performance of the model slightly deteriorated. Our model is able to recognize six entity types: funding agency, grant number, individuals, university, corporation, and miscellaneous. The model works more precisely for some entity types than for others; thus, individuals and grant numbers showed a very good F1-Score over 0.9. Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data. This model can be applied for the comprehensive analysis of acknowledgment texts and may potentially make a great contribution to the field of automated acknowledgment analysis.""}",2024,Scientometrics,"{acknowledgement,""flair nlp-framework"",""named entity recognition"",""natural language processing"",""text mining"",""web of science""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"acknowledgments in scientific papers may give an insight into aspects of the scientific community, such as reward systems, collaboration patterns, and hidden research trends. the aim of the paper is to evaluate the performance of different embedding models for the task of automatic extraction and classification of acknowledged entities from the acknowledgment text in scientific papers. we trained and implemented a named entity recognition (ner) task using the flair nlp framework. the training was conducted using three default flair ner models with four differently-sized corpora and different versions of the flair nlp framework. the flair embeddings model trained on the medium corpus with the latest flair version showed the best accuracy of 0.79. expanding the size of a training corpus from very small to medium size massively increased the accuracy of all training algorithms, but further expansion of the training corpus did not bring further improvement. moreover, the performance of the model slightly deteriorated. our model is able to recognize six entity types: funding agency, grant number, individuals, university, corporation, and miscellaneous. the model works more precisely for some entity types than for others; thus, individuals and grant numbers showed a very good f1-score over 0.9. most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data. this model can be applied for the comprehensive analysis of acknowledgment texts and may potentially make a great contribution to the field of automated acknowledgment analysis.",8
TACO - Twitter Arguments from COnversations,"{""Twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. Argument mining is an important analytical task for processing and understanding online discourse. Specifically, it aims to identify the structural elements of arguments, denoted as information and inference. These elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on Twitter. We contribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets covering 200 entire COnversations spanning six heterogeneous topics annotated with an agreement of 0.718 Krippendorff's α among six experts. Second, we provide our annotation framework, incorporating definitions from the Cambridge Dictionary, to define and identify argument components on Twitter. Our transformer-based classifier achieves an 85.06% macro F1 baseline score in detecting arguments. Moreover, our data reveals that Twitter users tend to engage in discussions involving informed inferences and information. TACO serves multiple purposes, such as training tweet classifiers to manage tweets based on inference and information elements, while also providing valuable insights into the conversational reply patterns of tweets.""}",2024,"2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings","{""argument mining"",inference,""information extraction"",resource,""twitter conversations""}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2024,"twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. argument mining is an important analytical task for processing and understanding online discourse. specifically, it aims to identify the structural elements of arguments, denoted as information and inference. these elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on twitter. we contribute taco, the first dataset of twitter arguments utilizing 1,814 tweets covering 200 entire conversations spanning six heterogeneous topics annotated with an agreement of 0.718 krippendorff's α among six experts. second, we provide our annotation framework, incorporating definitions from the cambridge dictionary, to define and identify argument components on twitter. our transformer-based classifier achieves an 85.06% macro f1 baseline score in detecting arguments. moreover, our data reveals that twitter users tend to engage in discussions involving informed inferences and information. taco serves multiple purposes, such as training tweet classifiers to manage tweets based on inference and information elements, while also providing valuable insights into the conversational reply patterns of tweets.",-1
Exploring Global Gender Gaps in the Blockchain Domain: Insights from LinkedIn Advertising Data,"{""Blockchain technology has gained widespread attention through Bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. There is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. By analyzing gender-disaggregated data from LinkedIn's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader IT sector. This study delves into the volume, velocity, variety, veracity, and value that LinkedIn Ad data offers to assess gender gaps in the blockchain domain at a global level.""}",2024,"Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024","{""gender gaps in the blockchain interests"",""gender gaps in the blockchain jobs"",""gender gaps in the blockchain skills"",""linkedin ad data"",""mining of social media data""}","Safety, Risk, Reliability and Quality",Engineering,Physical Sciences,CS,2024,"blockchain technology has gained widespread attention through bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. there is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. by analyzing gender-disaggregated data from linkedin's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader it sector. this study delves into the volume, velocity, variety, veracity, and value that linkedin ad data offers to assess gender gaps in the blockchain domain at a global level.",3
Ideological self-selection in online news exposure: Evidence from Europe and the US,"{""Today’s high-choice digital media environments allow citizens to completely refrain from online news exposure and, if they do use news, to select sources that align with their ideological preferences. Yet due to measurement problems and cross-country differences, recent research has been inconclusive regarding the prevalence of ideological self-selection into like-minded online news. We introduce a multi-method design combining the web-browsing histories and survey responses of more than 7000 participants from six major democracies with supervised text classification to separate political from nonpolitical news exposure. We find that political online news exposure is both substantially less prevalent and subject to stronger ideological self-selection than nonpolitical online news exposure, especially in the United States. By highlighting the peculiar role of political news content, the results improve the understanding of online news exposure and the role of digital media in democracy.""}",2024,Science Advances,,Multidisciplinary,Multidisciplinary,Multidisciplinary,CS,2024,"today’s high-choice digital media environments allow citizens to completely refrain from online news exposure and, if they do use news, to select sources that align with their ideological preferences. yet due to measurement problems and cross-country differences, recent research has been inconclusive regarding the prevalence of ideological self-selection into like-minded online news. we introduce a multi-method design combining the web-browsing histories and survey responses of more than 7000 participants from six major democracies with supervised text classification to separate political from nonpolitical news exposure. we find that political online news exposure is both substantially less prevalent and subject to stronger ideological self-selection than nonpolitical online news exposure, especially in the united states. by highlighting the peculiar role of political news content, the results improve the understanding of online news exposure and the role of digital media in democracy.",1
Patterns in the Growth and Thematic Evolution of Artificial Intelligence Research: A Study Using Bradford Distribution of Productivity and Path Analysis,"{""Artificial intelligence (AI) has emerged as a transformative technology with applications across multiple domains. The corpus of work related to the field of AI has grown significantly in volume as well as in terms of the application of AI in wider domains. However, given the wide application of AI in diverse areas, the measurement and characterization of the span of AI research is often a challenging task. Bibliometrics is a well-established method in the scientific community to measure the patterns and impact of research. It however has also received significant criticism for its overemphasis on the macroscopic picture and the inability to provide a deep understanding of growth and thematic structure of knowledge-creation activities. Therefore, this study presents a framework comprising of two techniques, namely, Bradford's distribution and path analysis to characterize the growth and thematic evolution of the discipline. While the Bradford distribution provides a macroscopic view of artificial intelligence research in terms of patterns of growth, the path analysis method presents a microscopic analysis of the thematic evolutionary trajectories, thereby completing the analytical framework. Detailed insights into the evolution of each subdomain are drawn, major techniques employed in various AI applications are identified, and some relevant implications are discussed to demonstrate the usefulness of the analyses.""}",2024,International Journal of Intelligent Systems,,Software,Computer Science,Physical Sciences,CS,2024,"artificial intelligence (ai) has emerged as a transformative technology with applications across multiple domains. the corpus of work related to the field of ai has grown significantly in volume as well as in terms of the application of ai in wider domains. however, given the wide application of ai in diverse areas, the measurement and characterization of the span of ai research is often a challenging task. bibliometrics is a well-established method in the scientific community to measure the patterns and impact of research. it however has also received significant criticism for its overemphasis on the macroscopic picture and the inability to provide a deep understanding of growth and thematic structure of knowledge-creation activities. therefore, this study presents a framework comprising of two techniques, namely, bradford's distribution and path analysis to characterize the growth and thematic evolution of the discipline. while the bradford distribution provides a macroscopic view of artificial intelligence research in terms of patterns of growth, the path analysis method presents a microscopic analysis of the thematic evolutionary trajectories, thereby completing the analytical framework. detailed insights into the evolution of each subdomain are drawn, major techniques employed in various ai applications are identified, and some relevant implications are discussed to demonstrate the usefulness of the analyses.",7
A mutually enhanced multi-scale relation-aware graph convolutional network for argument pair extraction,"{""Argument pair extraction (APE) is a fine-grained task of argument mining which aims to identify arguments offered by different participants in some discourse and detect interaction relationships between arguments from different participants. In recent years, many research efforts have been devoted to dealing with APE in a multi-task learning framework. Although these approaches have achieved encouraging results, they still face several challenging issues. First, different types of sentence relationships as well as different levels of information exchange among sentences are largely ignored. Second, they solely model interactions between argument pairs either in an explicit or implicit strategy, while neglecting the complementary effect of the two strategies. In this paper, we propose a novel Mutually Enhanced Multi-Scale Relation-Aware Graph Convolutional Network (MMR-GCN) for APE. Specifically, we first design a multi-scale relation-aware graph aggregation module to explicitly model the complex relationships between review and rebuttal passage sentences. In addition, we propose a mutually enhancement transformer module to implicitly and interactively enhance representations of review and rebuttal passage sentences. We experimentally validate MMR-GCN by comparing with the state-of-the-art APE methods. Experimental results show that it considerably outperforms all baseline methods, and the relative performance improvement of MMR-GCN over the best performing baseline MRC-APE in terms of F1 score reaches to 3.48% and 4.43% on the two benchmark datasets, respectively.""}",2024,Journal of Intelligent Information Systems,"{""argument mining"",""argument pair extraction"",""graph convolutional network"",transformer}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2024,"argument pair extraction (ape) is a fine-grained task of argument mining which aims to identify arguments offered by different participants in some discourse and detect interaction relationships between arguments from different participants. in recent years, many research efforts have been devoted to dealing with ape in a multi-task learning framework. although these approaches have achieved encouraging results, they still face several challenging issues. first, different types of sentence relationships as well as different levels of information exchange among sentences are largely ignored. second, they solely model interactions between argument pairs either in an explicit or implicit strategy, while neglecting the complementary effect of the two strategies. in this paper, we propose a novel mutually enhanced multi-scale relation-aware graph convolutional network (mmr-gcn) for ape. specifically, we first design a multi-scale relation-aware graph aggregation module to explicitly model the complex relationships between review and rebuttal passage sentences. in addition, we propose a mutually enhancement transformer module to implicitly and interactively enhance representations of review and rebuttal passage sentences. we experimentally validate mmr-gcn by comparing with the state-of-the-art ape methods. experimental results show that it considerably outperforms all baseline methods, and the relative performance improvement of mmr-gcn over the best performing baseline mrc-ape in terms of f1 score reaches to 3.48% and 4.43% on the two benchmark datasets, respectively.",6
Stories and personal experiences in the COVID-19 Discourse,"{""Storytelling, i.e., the use of of anecdotes and personal experiences, plays a crucial role in everyday argumentation. This is particularly true for the highly controversial debates that spark in times of crisis - where the focus of the discussion is on heterogeneous aspects of everyday life. For individuals, stories can have a strong persuasive power; for a larger collective, stories can help decision-makers to develop strategies for addressing the challenges people are facing, especially in times of crisis. In this paper, we analyse the use of storytelling in the COVID-19 discourse. We carry out our analysis on three publicly available Reddit datasets, for a total of 367K comments. We automatically annotate the Reddit datasets by detecting spans containing storytelling and classifying them into: a) personal vs. general: is the story experienced by the speaker? b) argumentative function: does the story clarify a problem, potentially consisting in harm to a specific group? Does it exemplify a solution to a problem, or does it establish the credibility of the speaker?), and c) topic. We then carry out an analysis which establishes the relevance of storytelling in the COVID discourse and further uncovers interactions between topics and types of stories associated to them.""}",2024,"2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings",,Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"storytelling, i.e., the use of of anecdotes and personal experiences, plays a crucial role in everyday argumentation. this is particularly true for the highly controversial debates that spark in times of crisis - where the focus of the discussion is on heterogeneous aspects of everyday life. for individuals, stories can have a strong persuasive power; for a larger collective, stories can help decision-makers to develop strategies for addressing the challenges people are facing, especially in times of crisis. in this paper, we analyse the use of storytelling in the covid-19 discourse. we carry out our analysis on three publicly available reddit datasets, for a total of 367k comments. we automatically annotate the reddit datasets by detecting spans containing storytelling and classifying them into: a) personal vs. general: is the story experienced by the speaker? b) argumentative function: does the story clarify a problem, potentially consisting in harm to a specific group? does it exemplify a solution to a problem, or does it establish the credibility of the speaker?), and c) topic. we then carry out an analysis which establishes the relevance of storytelling in the covid discourse and further uncovers interactions between topics and types of stories associated to them.",7
On the Anatomy of Real-World R Code for Static Analysis,"{""Context The R programming language has a huge and active community, especially in the area of statistical computing. Its interpreted nature allows for several interesting constructs, like the manipulation of functions at run-time, that hinder the static analysis of R programs. At the same time, there is a lack of existing research regarding how these features, or even the R language as a whole are used in practice. Objective In this paper, we conduct a large-scale, static analysis of more than 50 million lines of real- world R programs and packages to identify their characteristics and the features that are actually used. Moreover, we compare the similarities and differences between the scripts of R users and the implementations of package authors. We provide insights for static analysis tools like the lintr package as well as potential interpreter optimizations and uncover areas for future research. Method We analyze 4 230 R scripts submitted alongside publications and the sources of 19 450 CRAN packages for over 350 000 R files, collecting and summarizing quantitative information for features of interest. Results We find a high frequency of name-based indexing operations, assignments, and loops, but a low frequency for most of R's reflective functions. Furthermore, we find neither testing functions nor many calls to R's foreign function interface (FFI) in the publication submissions. Conclusion R scripts and package sources differ, for example, in their size, the way they include other packages, and their usage of R's reflective capabilities. We provide features that are used frequently and should be prioritized by static analysis tools, like operator assignments, function calls, and certain reflective functions like loadCCS CONCEPTS•General and reference → Empirical studies; • Software and its engineering → Language features.""}",2024,"Proceedings - 2024 IEEE/ACM 21st International Conference on Mining Software Repositories, MSR 2024","{""language feature usage"",""large-scale static analysis"",""r programming language""}",Software,Computer Science,Physical Sciences,CS,2024,"context the r programming language has a huge and active community, especially in the area of statistical computing. its interpreted nature allows for several interesting constructs, like the manipulation of functions at run-time, that hinder the static analysis of r programs. at the same time, there is a lack of existing research regarding how these features, or even the r language as a whole are used in practice. objective in this paper, we conduct a large-scale, static analysis of more than 50 million lines of real- world r programs and packages to identify their characteristics and the features that are actually used. moreover, we compare the similarities and differences between the scripts of r users and the implementations of package authors. we provide insights for static analysis tools like the lintr package as well as potential interpreter optimizations and uncover areas for future research. method we analyze 4 230 r scripts submitted alongside publications and the sources of 19 450 cran packages for over 350 000 r files, collecting and summarizing quantitative information for features of interest. results we find a high frequency of name-based indexing operations, assignments, and loops, but a low frequency for most of r's reflective functions. furthermore, we find neither testing functions nor many calls to r's foreign function interface (ffi) in the publication submissions. conclusion r scripts and package sources differ, for example, in their size, the way they include other packages, and their usage of r's reflective capabilities. we provide features that are used frequently and should be prioritized by static analysis tools, like operator assignments, function calls, and certain reflective functions like loadccs concepts•general and reference → empirical studies; • software and its engineering → language features.",0
"Investigating Characteristics, Biases and Evolution of Fact-Checked Claims on the Web","{""Given the recent proliferation of fake news online, fact-checking has emerged as a critical defence against misinformation. Several fact-checking organisations are currently employed in the initiative to assess the truthfulness of online claims. Verified claims serve as foundational data for various cross-domain research, including fields of social science and natural language processing, where they are used to study misinformation and several downstream tasks such as automated fact-verification. However, these fact-checking websites inherently harbour biases, posing challenges for academic endeavours aiming to discern truth from misinformation. In this study, we aim to explore the evolving landscape of online claims verified by multiple fact-checking organisations and analyse the underlying biases of individual fact-checking websites. Leveraging ClaimsKG, the largest available corpus of fact-checked claims, we analyse the temporal evolution of claims, focusing on topics, veracity levels, and entities to offer insights into the complex dimensions of online information. We utilise data and dimensions available from ClaimsKG for our analysis and for dimensions such as topics which are not present in ClaimsKG, we create a topic taxonomy and implement a transformer-based model, for multi-label classification of claims. We also observe how similar claims are co-occurant amongst different websites. Our work serves as a standardised framework for categorising claims sourced from diverse fact-checking organisations, laying the foundation for coherent and interpretable fact-checking datasets. The analysis conducted in this work sheds light on the dynamic landscape of online claims verified by several fact-checking organisations and dives into biases and distributions of several fact-checking websites.""}",2024,HT 2024: Creative Intelligence - 35th ACM Conference on Hypertext and Social Media,"{""claims analysis"",""claims classification"",fact-checking,""knowledge graphs"",""mis- and disinformation""}",Human-Computer Interaction,Computer Science,Physical Sciences,CS,2024,"given the recent proliferation of fake news online, fact-checking has emerged as a critical defence against misinformation. several fact-checking organisations are currently employed in the initiative to assess the truthfulness of online claims. verified claims serve as foundational data for various cross-domain research, including fields of social science and natural language processing, where they are used to study misinformation and several downstream tasks such as automated fact-verification. however, these fact-checking websites inherently harbour biases, posing challenges for academic endeavours aiming to discern truth from misinformation. in this study, we aim to explore the evolving landscape of online claims verified by multiple fact-checking organisations and analyse the underlying biases of individual fact-checking websites. leveraging claimskg, the largest available corpus of fact-checked claims, we analyse the temporal evolution of claims, focusing on topics, veracity levels, and entities to offer insights into the complex dimensions of online information. we utilise data and dimensions available from claimskg for our analysis and for dimensions such as topics which are not present in claimskg, we create a topic taxonomy and implement a transformer-based model, for multi-label classification of claims. we also observe how similar claims are co-occurant amongst different websites. our work serves as a standardised framework for categorising claims sourced from diverse fact-checking organisations, laying the foundation for coherent and interpretable fact-checking datasets. the analysis conducted in this work sheds light on the dynamic landscape of online claims verified by several fact-checking organisations and dives into biases and distributions of several fact-checking websites.",1
Tracing architecture of machine learning models through their mentions in scholarly articles,"{""Relation extraction, is a pivotal task in NLP, impacts information retrieval, natural language understanding (NLU) and knowledge generation. Machine learning model has coined itself as the most influential term in this era of deep learning and LLM. In scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. Knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. In this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. We attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. We report our findings with four state of the art baseline models. The findings report here exemplary performance with LUKE model as winner. The presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.""}",2024,"Proceedings - 2024 7th International Conference on Data Science and Information Technology, DSIT 2024","{baseline,extraction,""machine learning model"",""machine learning model architecture"",relation}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2024,"relation extraction, is a pivotal task in nlp, impacts information retrieval, natural language understanding (nlu) and knowledge generation. machine learning model has coined itself as the most influential term in this era of deep learning and llm. in scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. in this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. we attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. we report our findings with four state of the art baseline models. the findings report here exemplary performance with luke model as winner. the presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.",6
The Unseen Targets of Hate: A Systematic Review of Hateful Communication Datasets,"{""Machine learning (ML)-based content moderation tools are essential to keep online spaces free from hateful communication. Yet ML tools can only be as capable as the quality of the data they are trained on allows them. While there is increasing evidence that they underperform in detecting hateful communications directed towards specific identities and may discriminate against them, we know surprisingly little about the provenance of such bias. To fill this gap, we present a systematic review of the datasets for the automated detection of hateful communication introduced over the past decade, and unpack the quality of the datasets in terms of the identities that they embody: those of the targets of hateful communication that the data curators focused on, as well as those unintentionally included in the datasets. We find, overall, a skewed representation of selected target identities and mismatches between the targets that research conceptualizes and ultimately includes in datasets. Yet, by contextualizing these findings in the language and location of origin of the datasets, we highlight a positive trend towards the broadening and diversification of this research space.""}",2024,Social Science Computer Review,"{""data quality"",""hate targets"",""hateful online communication"",multilinguality,""systematic review""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2024,"machine learning (ml)-based content moderation tools are essential to keep online spaces free from hateful communication. yet ml tools can only be as capable as the quality of the data they are trained on allows them. while there is increasing evidence that they underperform in detecting hateful communications directed towards specific identities and may discriminate against them, we know surprisingly little about the provenance of such bias. to fill this gap, we present a systematic review of the datasets for the automated detection of hateful communication introduced over the past decade, and unpack the quality of the datasets in terms of the identities that they embody: those of the targets of hateful communication that the data curators focused on, as well as those unintentionally included in the datasets. we find, overall, a skewed representation of selected target identities and mismatches between the targets that research conceptualizes and ultimately includes in datasets. yet, by contextualizing these findings in the language and location of origin of the datasets, we highlight a positive trend towards the broadening and diversification of this research space.",1
Tracing architecture of machine learning models through their mentions in scholarly articles,"{""Relation extraction, is a pivotal task in NLP, impacts information retrieval, natural language understanding (NLU) and knowledge generation. Machine learning model has coined itself as the most influential term in this era of deep learning and LLM. In scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. Knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. In this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. We attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. We report our findings with four state of the art baseline models. The findings report here exemplary performance with LUKE model as winner. The presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.""}",2024,"Proceedings - 2024 7th International Conference on Data Science and Information Technology, DSIT 2024","{baseline,extraction,""machine learning model"",""machine learning model architecture"",relation}",Modeling and Simulation,Mathematics,Physical Sciences,CS,2024,"relation extraction, is a pivotal task in nlp, impacts information retrieval, natural language understanding (nlu) and knowledge generation. machine learning model has coined itself as the most influential term in this era of deep learning and llm. in scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. in this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. we attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. we report our findings with four state of the art baseline models. the findings report here exemplary performance with luke model as winner. the presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.",6
FASSILA: A Corpus for Algerian Dialect Fake News Detection and Sentiment Analysis,"{""In the context of low-resource languages, the Algerian dialect (AD) faces challenges due to the absence of annotated corpora, hindering its effective processing, notably in Machine Learning (ML) applications reliant on corpora for training and assessment. This study outlines the development process of a specialized corpus for Fake News (FN) detection and sentiment analysis (SA) in AD called FASSILA. This corpus comprises 10,087 sentences, encompassing over 19,497 unique words in AD, addresses the language's significant lack of linguistic resources, and covers seven distinct domains. We propose an FN detection and SA annotation scheme detailing the data collection, cleaning, and labeling. The remarkable Inter-Annotator Agreement indicates that the annotation scheme produces high-quality and consistent annotations. Subsequent classification experiments using BERT-based and ML models are presented, demonstrating promising results and highlighting avenues for further research. The dataset is currently freely available to facilitate future advancements in the field.""}",2024,Procedia Computer Science,"{""algerian dialect"",corpus,""deep learning"",""fake news"",llms,""machine learning"",""sentiment analysis""}",Computer Science (all),,,CS,2024,"in the context of low-resource languages, the algerian dialect (ad) faces challenges due to the absence of annotated corpora, hindering its effective processing, notably in machine learning (ml) applications reliant on corpora for training and assessment. this study outlines the development process of a specialized corpus for fake news (fn) detection and sentiment analysis (sa) in ad called fassila. this corpus comprises 10,087 sentences, encompassing over 19,497 unique words in ad, addresses the language's significant lack of linguistic resources, and covers seven distinct domains. we propose an fn detection and sa annotation scheme detailing the data collection, cleaning, and labeling. the remarkable inter-annotator agreement indicates that the annotation scheme produces high-quality and consistent annotations. subsequent classification experiments using bert-based and ml models are presented, demonstrating promising results and highlighting avenues for further research. the dataset is currently freely available to facilitate future advancements in the field.",0
Citation prediction by leveraging transformers and natural language processing heuristics,"{""In scientific papers, it is common practice to cite other articles to substantiate claims, provide evidence for factual assertions, reference limitations, and research gaps, and fulfill various other purposes. When authors include a citation in a given sentence, there are two considerations they need to take into account: (i) where in the sentence to place the citation and (ii) which citation to choose to support the underlying claim. In this paper, we focus on the first task as it allows multiple potential approaches that rely on the researcher's individual style and the specific norms and conventions of the relevant scientific community. We propose two automatic methodologies that leverage transformers architecture for either solving a Mask-Filling problem or a Named Entity Recognition problem. On top of the results of the proposed methodologies, we apply ad-hoc Natural Language Processing heuristics to further improve their outcome. We also introduce s2orc-9K, an open dataset for fine-tuning models on this task. A formal evaluation demonstrates that the generative approach significantly outperforms five alternative methods when fine-tuned on the novel dataset. Furthermore, this model's results show no statistically significant deviation from the outputs of three senior researchers.""}",2024,Information Processing and Management,"{bert,""citation prediction"",mask-filling,""named entity recognition"",""transformers architecture""}",Media Technology,Engineering,Physical Sciences,CS,2024,"in scientific papers, it is common practice to cite other articles to substantiate claims, provide evidence for factual assertions, reference limitations, and research gaps, and fulfill various other purposes. when authors include a citation in a given sentence, there are two considerations they need to take into account: (i) where in the sentence to place the citation and (ii) which citation to choose to support the underlying claim. in this paper, we focus on the first task as it allows multiple potential approaches that rely on the researcher's individual style and the specific norms and conventions of the relevant scientific community. we propose two automatic methodologies that leverage transformers architecture for either solving a mask-filling problem or a named entity recognition problem. on top of the results of the proposed methodologies, we apply ad-hoc natural language processing heuristics to further improve their outcome. we also introduce s2orc-9k, an open dataset for fine-tuning models on this task. a formal evaluation demonstrates that the generative approach significantly outperforms five alternative methods when fine-tuned on the novel dataset. furthermore, this model's results show no statistically significant deviation from the outputs of three senior researchers.",-1
Exploring Global Gender Gaps in the Blockchain Domain: Insights from LinkedIn Advertising Data,"{""Blockchain technology has gained widespread attention through Bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. There is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. By analyzing gender-disaggregated data from LinkedIn's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader IT sector. This study delves into the volume, velocity, variety, veracity, and value that LinkedIn Ad data offers to assess gender gaps in the blockchain domain at a global level.""}",2024,"Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024","{""gender gaps in the blockchain interests"",""gender gaps in the blockchain jobs"",""gender gaps in the blockchain skills"",""linkedin ad data"",""mining of social media data""}",Information Systems,Computer Science,Physical Sciences,CS,2024,"blockchain technology has gained widespread attention through bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. there is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. by analyzing gender-disaggregated data from linkedin's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader it sector. this study delves into the volume, velocity, variety, veracity, and value that linkedin ad data offers to assess gender gaps in the blockchain domain at a global level.",3
Automatic Analysis of Political Debates and Manifestos: Successes and Challenges,"{""The opinions of political actors (e.g., politicians, parties, organizations) expressed through claims are the core elements of political debates and decision-making. Political actors communicate through different channels: parties publish manifestos for major elections, while individual actors make statements on a day-to-day basis as reflected in the media. These two channels offer different approaches for analysis: Manifestos, on the one hand, are useful to characterize the parties’ positions at a global ideological level over time. In contrast, individual statements can be collected to analyze debates in particular policy domains on a fine-grained level, in terms of individual actors and claims. In this article, we summarize a series of studies we have carried out. We apply NLP-driven (semi-)automatic analyses on these two channels and compare their potentials and challenges. The fine-grained analysis yields rich insights into the communication but comes at the cost of three challenges: (a) a substantial hunger for manual annotation, introducing practical hurdles for analysis both within and across languages; (b) difficulties in claim classification arising from the uneven frequency distribution over the theory-based annotation schemas; (c) the need to map actor mentions onto canonical versions. Manifesto-based analysis avoids these challenges to a substantial extent when a more coarse-grained analysis of party positions is sufficient. We highlight the benefits and challenges of both approaches, and conclude by outlining perspectives for addressing the challenges in future research.""}",2024,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"{""argument mining"",""claim identification"",""discourse network analysis"",""party positioning""}",Computer Science (all),,,CS,2024,"the opinions of political actors (e.g., politicians, parties, organizations) expressed through claims are the core elements of political debates and decision-making. political actors communicate through different channels: parties publish manifestos for major elections, while individual actors make statements on a day-to-day basis as reflected in the media. these two channels offer different approaches for analysis: manifestos, on the one hand, are useful to characterize the parties’ positions at a global ideological level over time. in contrast, individual statements can be collected to analyze debates in particular policy domains on a fine-grained level, in terms of individual actors and claims. in this article, we summarize a series of studies we have carried out. we apply nlp-driven (semi-)automatic analyses on these two channels and compare their potentials and challenges. the fine-grained analysis yields rich insights into the communication but comes at the cost of three challenges: (a) a substantial hunger for manual annotation, introducing practical hurdles for analysis both within and across languages; (b) difficulties in claim classification arising from the uneven frequency distribution over the theory-based annotation schemas; (c) the need to map actor mentions onto canonical versions. manifesto-based analysis avoids these challenges to a substantial extent when a more coarse-grained analysis of party positions is sufficient. we highlight the benefits and challenges of both approaches, and conclude by outlining perspectives for addressing the challenges in future research.",7
Identifying Discourse Markers in French Spoken Corpora: Using Machine Learning and Rule-Based Approaches,"{""The objective of this work is to study the identification of French discourse markers (DM), in particular the polyfunctional occurrences such as ‘attetion’, bon, quoi, la preuve. A number of words identified as DM, and traditionally considered as adverbs or interjections, are also, for instance, adjectives or nouns. For example bon can be a DM or an adjective, ‘attetion’ can be a DM or a noun, etc. Hand annotation is in general robust but time consuming. The main difficulty with automatic identification is to take the context of the DM candidate correctly into account. To do that, a mechanisms based on rule-based and machine learning approaches was built, in order to reach an acceptable level of performance and reduce the expert effort. This study will provide a comprehensive use case of a machine learning algorithm, which has proved a good efficiency in dealing with such linguistic phenomena. In addition, an evaluation was done for the Unitex platform in order to determine the efficiency and drawbacks of this platform when dealing with such type of tasks.""}",2024,Communications in Computer and Information Science,"{""discourse markers"",dm,""french corpora"",knn,""machine learning"",""transfert learning""}",Computer Science (all),,,CS,2024,"the objective of this work is to study the identification of french discourse markers (dm), in particular the polyfunctional occurrences such as ‘attetion’, bon, quoi, la preuve. a number of words identified as dm, and traditionally considered as adverbs or interjections, are also, for instance, adjectives or nouns. for example bon can be a dm or an adjective, ‘attetion’ can be a dm or a noun, etc. hand annotation is in general robust but time consuming. the main difficulty with automatic identification is to take the context of the dm candidate correctly into account. to do that, a mechanisms based on rule-based and machine learning approaches was built, in order to reach an acceptable level of performance and reduce the expert effort. this study will provide a comprehensive use case of a machine learning algorithm, which has proved a good efficiency in dealing with such linguistic phenomena. in addition, an evaluation was done for the unitex platform in order to determine the efficiency and drawbacks of this platform when dealing with such type of tasks.",0
Preface to the Joint Workshop of the 5th Extraction and Evaluation of Knowledge Entities from Scientific Documents (EEKE2024) and the 4th AI + Informetrics (AII2024),"{""The Joint Workshop of the 5th Extraction and Evaluation of Knowledge Entities from Scientific Documents (EEKE2024; https://eeke-workshop.github.io/) and the 4th AI + Informetrics (AII2024; https://ai-informetrics.github.io/) was held in Changchun, China and online, co-located with the iConference2024. The two workshop series are designed to actively engage diverse communities in addressing open challenges related to the extraction and evaluation of knowledge entities from scientific documents and the modeling and applications of AI-empowered informetrics for broad interests in science of science, science, technology, & innovation, etc. The joint workshop features a comprehensive agenda, including keynotes from leading experts, oral presentations showcasing cutting-edge research, and poster sessions for in-depth discussions. The primary topics covered in the proceedings encompass the methodologies and applications of entity extraction, as well as the convergence of AI and informetrics, to drive advancements in these fields.""}",2024,CEUR Workshop Proceedings,"{""artificial intelligence"",informetrics,""knowledge entity evaluation"",""knowledge entity extraction"",""scientific document""}",Computer Science (all),,,CS,2024,"the joint workshop of the 5th extraction and evaluation of knowledge entities from scientific documents (eeke2024; https://eeke-workshop.github.io/) and the 4th ai + informetrics (aii2024; https://ai-informetrics.github.io/) was held in changchun, china and online, co-located with the iconference2024. the two workshop series are designed to actively engage diverse communities in addressing open challenges related to the extraction and evaluation of knowledge entities from scientific documents and the modeling and applications of ai-empowered informetrics for broad interests in science of science, science, technology, & innovation, etc. the joint workshop features a comprehensive agenda, including keynotes from leading experts, oral presentations showcasing cutting-edge research, and poster sessions for in-depth discussions. the primary topics covered in the proceedings encompass the methodologies and applications of entity extraction, as well as the convergence of ai and informetrics, to drive advancements in these fields.",2
Data Management Planning across Disciplines and Infrastructures. Introduction to the Special Collection,"{""The Special Collection Data Management Planning across Disciplines and Infrastructures of the Data Science Journal consists of papers describing practical experiences, concepts, and future directions on the design and deployment of effective data management plans and associated tools. Papers contain practical examples on managing and sharing data, consider the integration of data management plans into infrastructures and reflect innovative research into new directions for disciplinary and cross-disciplinary data management planning.""}",2024,Data Science Journal,"{""(cross-) disciplinary data management"",activedmps,""data management plan (dmp)"",""data management planning"",""data management planning infrastructures"",madmps}",Computer Science (miscellaneous),Computer Science,Physical Sciences,CS,2024,"the special collection data management planning across disciplines and infrastructures of the data science journal consists of papers describing practical experiences, concepts, and future directions on the design and deployment of effective data management plans and associated tools. papers contain practical examples on managing and sharing data, consider the integration of data management plans into infrastructures and reflect innovative research into new directions for disciplinary and cross-disciplinary data management planning.",2
Semantics for culinary health care,"{""In this chapter, we demonstrate the potential of semantic web and knowledge graphs in the context of culinary healthcare. We explore the idea of annotating text-based recipes with cooking semantics, rendering the underlying concepts machine-readable through the application of AI and NLP techniques. This annotated data can be harnessed to generate innovative, nutritionally sound recipes, with optimization criteria customized to prioritize health-related considerations. The concept of Semantic Recipe Generation offers promising applications, such as catering to individuals with specific dietary requirements and promoting ecofriendly cooking practices. We envision the possibility of extending this approach to develop a robust system that contributes to improving the well-being.""}",2024,Roles and Challenges of Semantic Intelligence in Healthcare Cognitive Computing,"{""automated recipe creation"",health,""recipe knowledge graph"",""semantic culinary health"",""semantic recipe""}",Computer Science (all),,,CS,2024,"in this chapter, we demonstrate the potential of semantic web and knowledge graphs in the context of culinary healthcare. we explore the idea of annotating text-based recipes with cooking semantics, rendering the underlying concepts machine-readable through the application of ai and nlp techniques. this annotated data can be harnessed to generate innovative, nutritionally sound recipes, with optimization criteria customized to prioritize health-related considerations. the concept of semantic recipe generation offers promising applications, such as catering to individuals with specific dietary requirements and promoting ecofriendly cooking practices. we envision the possibility of extending this approach to develop a robust system that contributes to improving the well-being.",5
Exploring Global Gender Gaps in the Blockchain Domain: Insights from LinkedIn Advertising Data,"{""Blockchain technology has gained widespread attention through Bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. There is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. By analyzing gender-disaggregated data from LinkedIn's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader IT sector. This study delves into the volume, velocity, variety, veracity, and value that LinkedIn Ad data offers to assess gender gaps in the blockchain domain at a global level.""}",2024,"Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024","{""gender gaps in the blockchain interests"",""gender gaps in the blockchain jobs"",""gender gaps in the blockchain skills"",""linkedin ad data"",""mining of social media data""}",Modeling and Simulation,Mathematics,Physical Sciences,CS,2024,"blockchain technology has gained widespread attention through bitcoin, but the blockchain domain is still striving to increase gender diversity and widely assess skills gaps by gender. there is limited awareness of women's participation in blockchain, prompting this study to assess and explore global gender gaps in interests, skills, and professions within the field. by analyzing gender-disaggregated data from linkedin's advertisement platform, we reveal that women are significantly underrepresented in blockchain compared to men, with the gender gap being even more pronounced than in the broader it sector. this study delves into the volume, velocity, variety, veracity, and value that linkedin ad data offers to assess gender gaps in the blockchain domain at a global level.",3
It is not About Bias but Discrimination,"{""Growing interest in the bias of LLMs is followed by empirical evidence of socially and morally undesirable patterns of LLMs output. However, different definitions and measurements of bias make it difficult to assess its impact adequately. To facilitate effective and constructive scholarly communication about bias, we make two contributions in this paper: First, we unpack the conceptual confusion in defining bias, where bias is used to indicate both descriptive and normative discrepancies between LLMs and desired outcomes. Second, we suggest deontological reasons why bias is unacceptable. Common arguments against bias are based on teleological grounds which focus on the consequences of biased LLMs. We argue that bias should be identified and mitigated when and because it is morally wrongful discrimination, regardless of its outcome. To support this argument, we connect biased LLMs with Deborah Hellman’s meaning-based account of discrimination. Bias in LLMs can be demeaning and capable of lowering the social status of affected individuals, making it morally wrongful discrimination. Such bias should be mitigated to prevent morally wrongful discrimination via technological means. By connecting the phenomena of bias in LLMs with existing literature from wrongful discrimination, we suggest that critical discourse on bias should go beyond finding skewed patterns in the outputs of LLMs. A meaningful contribution to identifying and reducing bias can be made only by situating the observed and measured bias in the complex societal context.""}",2024,CEUR Workshop Proceedings,"{bias,""computational social science"",discrimination,""large language models""}",Computer Science (all),,,CS,2024,"growing interest in the bias of llms is followed by empirical evidence of socially and morally undesirable patterns of llms output. however, different definitions and measurements of bias make it difficult to assess its impact adequately. to facilitate effective and constructive scholarly communication about bias, we make two contributions in this paper: first, we unpack the conceptual confusion in defining bias, where bias is used to indicate both descriptive and normative discrepancies between llms and desired outcomes. second, we suggest deontological reasons why bias is unacceptable. common arguments against bias are based on teleological grounds which focus on the consequences of biased llms. we argue that bias should be identified and mitigated when and because it is morally wrongful discrimination, regardless of its outcome. to support this argument, we connect biased llms with deborah hellman’s meaning-based account of discrimination. bias in llms can be demeaning and capable of lowering the social status of affected individuals, making it morally wrongful discrimination. such bias should be mitigated to prevent morally wrongful discrimination via technological means. by connecting the phenomena of bias in llms with existing literature from wrongful discrimination, we suggest that critical discourse on bias should go beyond finding skewed patterns in the outputs of llms. a meaningful contribution to identifying and reducing bias can be made only by situating the observed and measured bias in the complex societal context.",0
A three-way decision approach for dynamically expandable networks,"{""Conventional deep learning models are designed to work on a single task. They are required to be trained from scratch each time new tasks are added. This leads to overhead in training time. Continual deep learning models with dynamically expandable network architecture aim to handle this issue. The key idea in these models is to find a balance between the properties of stability (preserving the learned information) and plasticity (updating and accommodating the new information) also sometimes referred to as the stability-plasticity dilemma. The stability and plasticity of the model critically depends on three-way division of nodes into freeze, partially regularize and duplicate nodes. Freezing more nodes result in high stability but typically low plasticity. On the other hand, duplicating more nodes result in high plasticity but may not have an effective stability. In this paper, we introduce an approach called three-way decisions based dynamically expandable networks or 3WDDEN and its memory-based version called 3WDDEN-replay. The proposed approaches use game-theoretic rough sets to determine effective thresholds for three-way division of nodes by considering a tradeoff game between stability and plasticity. Experimental results of 3WDDEN on MNIST variant datasets show an overall improvement of 3.8% in accuracy compared to standard dynamically expandable network approach or DEN. 3WDDEN-replay further adds to accuracy with additional memory cost.""}",2024,International Journal of Approximate Reasoning,"{""continual deep learning"",""dynamically expandable networks"",""game-theoretic rough sets""}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2024,"conventional deep learning models are designed to work on a single task. they are required to be trained from scratch each time new tasks are added. this leads to overhead in training time. continual deep learning models with dynamically expandable network architecture aim to handle this issue. the key idea in these models is to find a balance between the properties of stability (preserving the learned information) and plasticity (updating and accommodating the new information) also sometimes referred to as the stability-plasticity dilemma. the stability and plasticity of the model critically depends on three-way division of nodes into freeze, partially regularize and duplicate nodes. freezing more nodes result in high stability but typically low plasticity. on the other hand, duplicating more nodes result in high plasticity but may not have an effective stability. in this paper, we introduce an approach called three-way decisions based dynamically expandable networks or 3wdden and its memory-based version called 3wdden-replay. the proposed approaches use game-theoretic rough sets to determine effective thresholds for three-way division of nodes by considering a tradeoff game between stability and plasticity. experimental results of 3wdden on mnist variant datasets show an overall improvement of 3.8% in accuracy compared to standard dynamically expandable network approach or den. 3wdden-replay further adds to accuracy with additional memory cost.",0
Bias-aware ranking from pairwise comparisons,"{""Human feedback is often used, either directly or indirectly, as input to algorithmic decision making. However, humans are biased: if the algorithm that takes as input the human feedback does not control for potential biases, this might result in biased algorithmic decision making, which can have a tangible impact on people’s lives. In this paper, we study how to detect and correct for evaluators’ bias in the task of ranking people (or items) from pairwise comparisons. Specifically, we assume we are given pairwise comparisons of the items to be ranked produced by a set of evaluators. While the pairwise assessments of the evaluators should reflect to a certain extent the latent (unobservable) true quality scores of the items, they might be affected by each evaluator’s own bias against, or in favor, of some groups of items. By detecting and amending evaluators’ biases, we aim to produce a ranking of the items that is, as much as possible, in accordance with the ranking one would produce by having access to the latent quality scores. Our proposal is a novel method that extends the classic Bradley-Terry model by having a bias parameter for each evaluator which distorts the true quality score of each item, depending on the group the item belongs to. Thanks to the simplicity of the model, we are able to write explicitly its log-likelihood w.r.t. the parameters (i.e., items’ latent scores and evaluators’ bias) and optimize by means of the alternating approach. Our experiments on synthetic and real-world data confirm that our method is able to reconstruct the bias of each single evaluator extremely well and thus to outperform several non-trivial competitors in the task of producing a ranking which is as much as possible close to the unbiased ranking.""}",2024,Data Mining and Knowledge Discovery,"{bradley-terry,""evaluators’ bias"",fairness,""pairwise comparisons"",rankings}",Computer Networks and Communications,Computer Science,Physical Sciences,CS,2024,"human feedback is often used, either directly or indirectly, as input to algorithmic decision making. however, humans are biased: if the algorithm that takes as input the human feedback does not control for potential biases, this might result in biased algorithmic decision making, which can have a tangible impact on people’s lives. in this paper, we study how to detect and correct for evaluators’ bias in the task of ranking people (or items) from pairwise comparisons. specifically, we assume we are given pairwise comparisons of the items to be ranked produced by a set of evaluators. while the pairwise assessments of the evaluators should reflect to a certain extent the latent (unobservable) true quality scores of the items, they might be affected by each evaluator’s own bias against, or in favor, of some groups of items. by detecting and amending evaluators’ biases, we aim to produce a ranking of the items that is, as much as possible, in accordance with the ranking one would produce by having access to the latent quality scores. our proposal is a novel method that extends the classic bradley-terry model by having a bias parameter for each evaluator which distorts the true quality score of each item, depending on the group the item belongs to. thanks to the simplicity of the model, we are able to write explicitly its log-likelihood w.r.t. the parameters (i.e., items’ latent scores and evaluators’ bias) and optimize by means of the alternating approach. our experiments on synthetic and real-world data confirm that our method is able to reconstruct the bias of each single evaluator extremely well and thus to outperform several non-trivial competitors in the task of producing a ranking which is as much as possible close to the unbiased ranking.",5
"Investigating Characteristics, Biases and Evolution of Fact-Checked Claims on the Web","{""Given the recent proliferation of fake news online, fact-checking has emerged as a critical defence against misinformation. Several fact-checking organisations are currently employed in the initiative to assess the truthfulness of online claims. Verified claims serve as foundational data for various cross-domain research, including fields of social science and natural language processing, where they are used to study misinformation and several downstream tasks such as automated fact-verification. However, these fact-checking websites inherently harbour biases, posing challenges for academic endeavours aiming to discern truth from misinformation. In this study, we aim to explore the evolving landscape of online claims verified by multiple fact-checking organisations and analyse the underlying biases of individual fact-checking websites. Leveraging ClaimsKG, the largest available corpus of fact-checked claims, we analyse the temporal evolution of claims, focusing on topics, veracity levels, and entities to offer insights into the complex dimensions of online information. We utilise data and dimensions available from ClaimsKG for our analysis and for dimensions such as topics which are not present in ClaimsKG, we create a topic taxonomy and implement a transformer-based model, for multi-label classification of claims. We also observe how similar claims are co-occurant amongst different websites. Our work serves as a standardised framework for categorising claims sourced from diverse fact-checking organisations, laying the foundation for coherent and interpretable fact-checking datasets. The analysis conducted in this work sheds light on the dynamic landscape of online claims verified by several fact-checking organisations and dives into biases and distributions of several fact-checking websites.""}",2024,HT 2024: Creative Intelligence - 35th ACM Conference on Hypertext and Social Media,"{""claims analysis"",""claims classification"",fact-checking,""knowledge graphs"",""mis- and disinformation""}",Computer Graphics and Computer-Aided Design,Computer Science,Physical Sciences,CS,2024,"given the recent proliferation of fake news online, fact-checking has emerged as a critical defence against misinformation. several fact-checking organisations are currently employed in the initiative to assess the truthfulness of online claims. verified claims serve as foundational data for various cross-domain research, including fields of social science and natural language processing, where they are used to study misinformation and several downstream tasks such as automated fact-verification. however, these fact-checking websites inherently harbour biases, posing challenges for academic endeavours aiming to discern truth from misinformation. in this study, we aim to explore the evolving landscape of online claims verified by multiple fact-checking organisations and analyse the underlying biases of individual fact-checking websites. leveraging claimskg, the largest available corpus of fact-checked claims, we analyse the temporal evolution of claims, focusing on topics, veracity levels, and entities to offer insights into the complex dimensions of online information. we utilise data and dimensions available from claimskg for our analysis and for dimensions such as topics which are not present in claimskg, we create a topic taxonomy and implement a transformer-based model, for multi-label classification of claims. we also observe how similar claims are co-occurant amongst different websites. our work serves as a standardised framework for categorising claims sourced from diverse fact-checking organisations, laying the foundation for coherent and interpretable fact-checking datasets. the analysis conducted in this work sheds light on the dynamic landscape of online claims verified by several fact-checking organisations and dives into biases and distributions of several fact-checking websites.",1
GESIS-DSM at PerpectiveArg2024: A Matter of Style? Socio-Cultural Differences in Argumentation,"{""This paper describes the contribution of team GESIS-DSM to the Perspective Argument Retrieval Task, a task on retrieving socioculturally relevant and diverse arguments for different user queries. Our experiments and analyses aim to explore the nature of the sociocultural specialization in argument retrieval: (how) do the arguments written by different socio-cultural groups differ? We investigate the impact of content and style for the task of identifying arguments relevant to a query and a certain demographic attribute. In its different configurations, our system employs sentence embedding representations, arguments generated with Large Language Model, as well as stylistic features. Our final method places third overall in the shared task, and, in comparison, does particularly well in the most difficult evaluation scenario, where the socio-cultural background of the argument author is implicit (i.e. has to be inferred from the text). This result indicates that socio-cultural differences in argument production may indeed be a matter of style.""}",2024,"ArgMining 2024 - 11th Workshop on Argument Mining, Proceedings of the Workshop",,Software,Computer Science,Physical Sciences,CS,2024,"this paper describes the contribution of team gesis-dsm to the perspective argument retrieval task, a task on retrieving socioculturally relevant and diverse arguments for different user queries. our experiments and analyses aim to explore the nature of the sociocultural specialization in argument retrieval: (how) do the arguments written by different socio-cultural groups differ? we investigate the impact of content and style for the task of identifying arguments relevant to a query and a certain demographic attribute. in its different configurations, our system employs sentence embedding representations, arguments generated with large language model, as well as stylistic features. our final method places third overall in the shared task, and, in comparison, does particularly well in the most difficult evaluation scenario, where the socio-cultural background of the argument author is implicit (i.e. has to be inferred from the text). this result indicates that socio-cultural differences in argument production may indeed be a matter of style.",0
Tracing architecture of machine learning models through their mentions in scholarly articles,"{""Relation extraction, is a pivotal task in NLP, impacts information retrieval, natural language understanding (NLU) and knowledge generation. Machine learning model has coined itself as the most influential term in this era of deep learning and LLM. In scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. Knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. In this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. We attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. We report our findings with four state of the art baseline models. The findings report here exemplary performance with LUKE model as winner. The presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.""}",2024,"Proceedings - 2024 7th International Conference on Data Science and Information Technology, DSIT 2024","{baseline,extraction,""machine learning model"",""machine learning model architecture"",relation}",Information Systems,Computer Science,Physical Sciences,CS,2024,"relation extraction, is a pivotal task in nlp, impacts information retrieval, natural language understanding (nlu) and knowledge generation. machine learning model has coined itself as the most influential term in this era of deep learning and llm. in scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. in this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. we attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. we report our findings with four state of the art baseline models. the findings report here exemplary performance with luke model as winner. the presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.",6
LLM Based Bilingual Rumor Verification Using Evidence From Authorities,"{""The spread of misinformation as rumors is getting more prevalent on social media with its widespread use as access to instant information. Rumors on social media platforms can have damaging consequences unless timely intercepted. The existing studies on rumor verification use linguistic patterns, sentiment orientation, and network structures. It requires training data preparation and updating the model to stay up to date with newer rumors. However, little attention is paid to benefit from the known trusted, and credible authorities to verify rumors. In this study, we address rumor verification on platform X (previously Twitter) by using evidence from the timeline of authority accounts. We propose LLM-based bilingual rumor verification for English and Arabic using SBERT and BM25 to retrieve evidence candidates i.e., relevant tweets from the authority timeline, and finetuned XLM-RoBERTa to detect their stance of the rumor. It achieves F1-score of 0.8133 for English and 0.7647 for Arabic to detect stance labels for the rumor using evidence candidates. The rumor is verified by weighted aggregation of its stance labels having accuracy of 0.6923 and 0.5769 for Arabic.""}",2025,CEUR Workshop Proceedings,"{""llm-based rumor verification"",""rumor evidence stance detection"",""rumor verification""}",Computer Science (all),,,CS,2025,"the spread of misinformation as rumors is getting more prevalent on social media with its widespread use as access to instant information. rumors on social media platforms can have damaging consequences unless timely intercepted. the existing studies on rumor verification use linguistic patterns, sentiment orientation, and network structures. it requires training data preparation and updating the model to stay up to date with newer rumors. however, little attention is paid to benefit from the known trusted, and credible authorities to verify rumors. in this study, we address rumor verification on platform x (previously twitter) by using evidence from the timeline of authority accounts. we propose llm-based bilingual rumor verification for english and arabic using sbert and bm25 to retrieve evidence candidates i.e., relevant tweets from the authority timeline, and finetuned xlm-roberta to detect their stance of the rumor. it achieves f1-score of 0.8133 for english and 0.7647 for arabic to detect stance labels for the rumor using evidence candidates. the rumor is verified by weighted aggregation of its stance labels having accuracy of 0.6923 and 0.5769 for arabic.",1
The First Workshop on Scholarly Information Access (SCOLIA),"{""The first workshop on Scholarly Information Access (SCOLIA) will take place at ECIR 2025 as a half-day workshop. The workshop is building upon and following up on the long series of the Bibliometric-enhanced Information Retrieval (BIR) workshops at ECIR. SCOLIA addresses research topics related to academic search and recommendation, at the intersection of Information Retrieval, Natural Language Processing (including generative AI), and Bibliometrics. As an interdisciplinary and intersectoral scientific event, addressing an ever-growing topic investigated by both academia and industry, SCOLIA brings together researchers and practitioners from the aforementioned communities. The interactive format fosters engagement of all participants and fruitful discussions. The outcome of the workshop will reflect the current state and identify future research questions.""}",2025,Lecture Notes in Computer Science,"{""academic search"",bibliometrics,""digital libraries"",""information access"",""information retrieval"",""natural language processing""}",Computer Science (all),,,CS,2025,"the first workshop on scholarly information access (scolia) will take place at ecir 2025 as a half-day workshop. the workshop is building upon and following up on the long series of the bibliometric-enhanced information retrieval (bir) workshops at ecir. scolia addresses research topics related to academic search and recommendation, at the intersection of information retrieval, natural language processing (including generative ai), and bibliometrics. as an interdisciplinary and intersectoral scientific event, addressing an ever-growing topic investigated by both academia and industry, scolia brings together researchers and practitioners from the aforementioned communities. the interactive format fosters engagement of all participants and fruitful discussions. the outcome of the workshop will reflect the current state and identify future research questions.",2
Out-of-Vocabulary Pashto Spell Checker using Morphological Operations,"{""A spell checking model detects spelling errors in the input text, generates possible corrections for each error, and organizes them based on their relevance. Such tools are essential for writing, editing, and publishing in a language. In literature, different variants of edit distance and language models are used for dealing with spelling errors. The existing approaches have improved for efficient use of computational and memory resources without compromising accuracy. Exploiting the efforts made for other low-resource languages, we have proposed the first spell checking model for Pashto language. A common issue with the existing spell checker models of low-resourced languages is having many false positives due to limited vocabulary. Our proposed approach makes use of morphological operations to generate out-of-vocabulary words and is coupled with a filtering mechanism to retain the words with more repeating syllable patterns only. The n-gram probabilities are combined through linear interpolation to detect and correct spelling errors. The test samples with spelling errors are generated by randomizing characters for non-word errors and words in higher-order n-gram for real word errors. The proposed approach achieves detection accuracy of 85.5% and correction accuracy of 75.0%, respectively.""}",2025,"2025 International Conference on Communication Technologies, ComTech 2025","{""edit distance"",""language models"",""linear interpolation"",""spell checker""}",Computer Networks and Communications,Computer Science,Physical Sciences,CS,2025,"a spell checking model detects spelling errors in the input text, generates possible corrections for each error, and organizes them based on their relevance. such tools are essential for writing, editing, and publishing in a language. in literature, different variants of edit distance and language models are used for dealing with spelling errors. the existing approaches have improved for efficient use of computational and memory resources without compromising accuracy. exploiting the efforts made for other low-resource languages, we have proposed the first spell checking model for pashto language. a common issue with the existing spell checker models of low-resourced languages is having many false positives due to limited vocabulary. our proposed approach makes use of morphological operations to generate out-of-vocabulary words and is coupled with a filtering mechanism to retain the words with more repeating syllable patterns only. the n-gram probabilities are combined through linear interpolation to detect and correct spelling errors. the test samples with spelling errors are generated by randomizing characters for non-word errors and words in higher-order n-gram for real word errors. the proposed approach achieves detection accuracy of 85.5% and correction accuracy of 75.0%, respectively.",0
Graph colouring using evolutionary computation: A case study of blind naked mole-rat algorithm,"{""Graph colouring problem (GCP) is an NP-complete optimization problem. It is famous for its applications in scheduling, register allocation, and map colouring. In recent years, biological inspired and especially Swarm intelligence (SI) techniques have gained popularity for solving complex optimization problems. In this article, we have proposed blind naked mole rat-based colouring (BNMR-Col) for graphs. BNMR-Col uses both exploitation and exploration to find the best solution in search space. Exploitation uses both local moves and global moves to find a better solution in the surroundings of an existing solution. On the other hand, exploration generates new solution by combining various solutions from the search space. BNMR-Col shows better convergence rate and approaches the lowest colour value in 83% of the cases when tested on standard benchmark graph instances.""}",2025,Expert Systems,"{""intelligent system"",""meta heuristic"",optimization,""swarm intelligence""}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2025,"graph colouring problem (gcp) is an np-complete optimization problem. it is famous for its applications in scheduling, register allocation, and map colouring. in recent years, biological inspired and especially swarm intelligence (si) techniques have gained popularity for solving complex optimization problems. in this article, we have proposed blind naked mole rat-based colouring (bnmr-col) for graphs. bnmr-col uses both exploitation and exploration to find the best solution in search space. exploitation uses both local moves and global moves to find a better solution in the surroundings of an existing solution. on the other hand, exploration generates new solution by combining various solutions from the search space. bnmr-col shows better convergence rate and approaches the lowest colour value in 83% of the cases when tested on standard benchmark graph instances.",4
Extracting and Modeling Tabular Data from Marine Geology Publications into a Heterogeneous Information Network,"{""Scientific publications serve as a source of disseminating information across research communities, often containing diverse data elements such as plain-text, tables, and figures. Tables in particular offer a structured presentation of essential research data, enabling efficient information access. Automatic extraction of tabular data alongside contextual information from scientific publications can significantly enhance research workflows and integrate more research data into scholarly research cycle, particularly supporting Research Data Management (RDM). In marine geology, the researchers conduct expeditions at oceanographic locations and accumulate substantial amounts of valuable data such as Sedimentation Rate (SR), Mass Accumulation Rate (MAR) alongside relevant contextual information, often enriched with spatio-temporal context in tables of publications. These expeditions are costly and time intensive, emphasizing on the value of making such data more accessible and reusable. This paper introduces an end to end approach to extract and model heterogeneous tabular data from marine geology publications. Our approach extracts metadata and tabular content from publications, modeling them into a Heterogeneous Information Network (HIN). The network uncovers hidden relationships and patterns across multiple documents, offering new insights and facilitating enhanced data referencing. Experimental results and exploration on marine geology datasets demonstrate the effectiveness of our approach, showcasing its potential to support research data management and data driven scientific exploration.""}",2025,International Conference on Pattern Recognition Applications and Methods,"{""data modeling"",""heterogeneous information network"",""information extraction"",""marine science publication"",""research data management"",""tabular data""}",Computer Vision and Pattern Recognition,Computer Science,Physical Sciences,CS,2025,"scientific publications serve as a source of disseminating information across research communities, often containing diverse data elements such as plain-text, tables, and figures. tables in particular offer a structured presentation of essential research data, enabling efficient information access. automatic extraction of tabular data alongside contextual information from scientific publications can significantly enhance research workflows and integrate more research data into scholarly research cycle, particularly supporting research data management (rdm). in marine geology, the researchers conduct expeditions at oceanographic locations and accumulate substantial amounts of valuable data such as sedimentation rate (sr), mass accumulation rate (mar) alongside relevant contextual information, often enriched with spatio-temporal context in tables of publications. these expeditions are costly and time intensive, emphasizing on the value of making such data more accessible and reusable. this paper introduces an end to end approach to extract and model heterogeneous tabular data from marine geology publications. our approach extracts metadata and tabular content from publications, modeling them into a heterogeneous information network (hin). the network uncovers hidden relationships and patterns across multiple documents, offering new insights and facilitating enhanced data referencing. experimental results and exploration on marine geology datasets demonstrate the effectiveness of our approach, showcasing its potential to support research data management and data driven scientific exploration.",2
Exploration of Hugging Face Models by Heterogeneous Information Network and Linking Across Scholarly Repositories,"{""With the pervasive integration of Machine Learning (ML) focusing complex tasks across various domains, generally respective models and datasets are made available in numerous scientific repositories such as Hugging Face, GitHub for recognition and understanding within the research communities hence supporting open science initiative. However, the adaptability of these repositories is increasing among users hence raising a concern about the usability of these models and datasets effectively. Therefore, it is necessary to explore these repositories and compile comprehensive information that could facilitate users as well as repositories itself. Hugging Face, a leading repository, aims to furnish a platform that organizes and presents detailed information on models and datasets employed in research. As its adoption escalates within the research communities, the necessity to delve into such repositories becomes crucial to offer researchers valuable insights and promote efficient knowledge dissemination. This study focuses on exploring Hugging Face, particularly its machine learning models by exploiting various relevant features and the insights are presented in a Heterogeneous Information Network (HIN). Our research not only demonstrates the effectiveness of the available models on Hugging Face but also highlights potential links to relevant scholarly repositories by highlighting the significance of exploration which could contribute to the future integration of repositories, facilitating a more unified and accessible framework for scientific research.""}",2025,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"{""data exploration"",""heterogeneous information network"",""hugging face"",""machine learning models"",""repository exploration""}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2025,"with the pervasive integration of machine learning (ml) focusing complex tasks across various domains, generally respective models and datasets are made available in numerous scientific repositories such as hugging face, github for recognition and understanding within the research communities hence supporting open science initiative. however, the adaptability of these repositories is increasing among users hence raising a concern about the usability of these models and datasets effectively. therefore, it is necessary to explore these repositories and compile comprehensive information that could facilitate users as well as repositories itself. hugging face, a leading repository, aims to furnish a platform that organizes and presents detailed information on models and datasets employed in research. as its adoption escalates within the research communities, the necessity to delve into such repositories becomes crucial to offer researchers valuable insights and promote efficient knowledge dissemination. this study focuses on exploring hugging face, particularly its machine learning models by exploiting various relevant features and the insights are presented in a heterogeneous information network (hin). our research not only demonstrates the effectiveness of the available models on hugging face but also highlights potential links to relevant scholarly repositories by highlighting the significance of exploration which could contribute to the future integration of repositories, facilitating a more unified and accessible framework for scientific research.",3
Annotating scientific uncertainty: A comprehensive model using linguistic patterns and comparison with existing approaches,"{""We present UnScientify,1 a system designed to detect scientific uncertainty in scholarly full text. The system utilizes a weakly supervised technique to identify verbally expressed uncertainty in scientific texts and their authorial references. The core methodology of UnScientify is based on a multi-faceted pipeline that integrates span pattern matching, complex sentence analysis and author reference checking. This approach streamlines the labeling and annotation processes essential for identifying scientific uncertainty, covering a variety of uncertainty expression types to support diverse applications including information retrieval, text mining and scientific document processing. The evaluation results highlight the trade-offs between modern large language models (LLMs) and the UnScientify system. UnScientify, which employs more traditional techniques, achieved superior performance in the scientific uncertainty detection task, attaining an accuracy score of 0.808. This finding underscores the continued relevance and efficiency of UnScientify's simple rule-based and pattern matching strategy for this specific application. The results demonstrate that in scenarios where resource efficiency, interpretability, and domain-specific adaptability are critical, traditional methods can still offer significant advantages.""}",2025,Journal of Informetrics,"{""linguistic patterns"",llm,""research article"",""semantic annotation"",uncertainty}",Computer Science Applications,Computer Science,Physical Sciences,CS,2025,"we present unscientify,1 a system designed to detect scientific uncertainty in scholarly full text. the system utilizes a weakly supervised technique to identify verbally expressed uncertainty in scientific texts and their authorial references. the core methodology of unscientify is based on a multi-faceted pipeline that integrates span pattern matching, complex sentence analysis and author reference checking. this approach streamlines the labeling and annotation processes essential for identifying scientific uncertainty, covering a variety of uncertainty expression types to support diverse applications including information retrieval, text mining and scientific document processing. the evaluation results highlight the trade-offs between modern large language models (llms) and the unscientify system. unscientify, which employs more traditional techniques, achieved superior performance in the scientific uncertainty detection task, attaining an accuracy score of 0.808. this finding underscores the continued relevance and efficiency of unscientify's simple rule-based and pattern matching strategy for this specific application. the results demonstrate that in scenarios where resource efficiency, interpretability, and domain-specific adaptability are critical, traditional methods can still offer significant advantages.",-1
Escaping the Filter Bubble: Evaluating Electroencephalographic Theta Band Synchronization as Indicator for Selective Exposure in Online News Reading,"{""Selective exposure to online news occurs when users favor information that confirms their beliefs, creating filter bubbles and limiting diverse perspectives. Interactive systems can counter this by recommending different perspectives, but to achieve this, they need a real-time metric for selective exposure. We present an experiment where we evaluate Electroencephalography (EEG) and eyetracking as indicators for selective exposure by using eye tracking to recognize which textual parts participants read and using EEG to quantify the magnitude of selective exposure. Participants read online news while we collected EEG and eye movements with their agreement towards the news. We show that the agreement with news correlates positively with the theta band power in the parietal area. Our results indicate that future interactive systems can sense selective exposure using EEG and eye tracking to propose a more balanced information diet. This work presents an integrated experimental setup that identifies selective exposure using gaze and EEG-based metrics.""}",2025,Conference on Human Factors in Computing Systems - Proceedings,"{co-registration,electroencephalography,""eye tracking"",news,reading,""selective exposure""}",Computer Graphics and Computer-Aided Design,Computer Science,Physical Sciences,CS,2025,"selective exposure to online news occurs when users favor information that confirms their beliefs, creating filter bubbles and limiting diverse perspectives. interactive systems can counter this by recommending different perspectives, but to achieve this, they need a real-time metric for selective exposure. we present an experiment where we evaluate electroencephalography (eeg) and eyetracking as indicators for selective exposure by using eye tracking to recognize which textual parts participants read and using eeg to quantify the magnitude of selective exposure. participants read online news while we collected eeg and eye movements with their agreement towards the news. we show that the agreement with news correlates positively with the theta band power in the parietal area. our results indicate that future interactive systems can sense selective exposure using eeg and eye tracking to propose a more balanced information diet. this work presents an integrated experimental setup that identifies selective exposure using gaze and eeg-based metrics.",1
Out-of-Vocabulary Pashto Spell Checker using Morphological Operations,"{""A spell checking model detects spelling errors in the input text, generates possible corrections for each error, and organizes them based on their relevance. Such tools are essential for writing, editing, and publishing in a language. In literature, different variants of edit distance and language models are used for dealing with spelling errors. The existing approaches have improved for efficient use of computational and memory resources without compromising accuracy. Exploiting the efforts made for other low-resource languages, we have proposed the first spell checking model for Pashto language. A common issue with the existing spell checker models of low-resourced languages is having many false positives due to limited vocabulary. Our proposed approach makes use of morphological operations to generate out-of-vocabulary words and is coupled with a filtering mechanism to retain the words with more repeating syllable patterns only. The n-gram probabilities are combined through linear interpolation to detect and correct spelling errors. The test samples with spelling errors are generated by randomizing characters for non-word errors and words in higher-order n-gram for real word errors. The proposed approach achieves detection accuracy of 85.5% and correction accuracy of 75.0%, respectively.""}",2025,"2025 International Conference on Communication Technologies, ComTech 2025","{""edit distance"",""language models"",""linear interpolation"",""spell checker""}",Instrumentation,Physics and Astronomy,Physical Sciences,CS,2025,"a spell checking model detects spelling errors in the input text, generates possible corrections for each error, and organizes them based on their relevance. such tools are essential for writing, editing, and publishing in a language. in literature, different variants of edit distance and language models are used for dealing with spelling errors. the existing approaches have improved for efficient use of computational and memory resources without compromising accuracy. exploiting the efforts made for other low-resource languages, we have proposed the first spell checking model for pashto language. a common issue with the existing spell checker models of low-resourced languages is having many false positives due to limited vocabulary. our proposed approach makes use of morphological operations to generate out-of-vocabulary words and is coupled with a filtering mechanism to retain the words with more repeating syllable patterns only. the n-gram probabilities are combined through linear interpolation to detect and correct spelling errors. the test samples with spelling errors are generated by randomizing characters for non-word errors and words in higher-order n-gram for real word errors. the proposed approach achieves detection accuracy of 85.5% and correction accuracy of 75.0%, respectively.",0
Out-of-Vocabulary Pashto Spell Checker using Morphological Operations,"{""A spell checking model detects spelling errors in the input text, generates possible corrections for each error, and organizes them based on their relevance. Such tools are essential for writing, editing, and publishing in a language. In literature, different variants of edit distance and language models are used for dealing with spelling errors. The existing approaches have improved for efficient use of computational and memory resources without compromising accuracy. Exploiting the efforts made for other low-resource languages, we have proposed the first spell checking model for Pashto language. A common issue with the existing spell checker models of low-resourced languages is having many false positives due to limited vocabulary. Our proposed approach makes use of morphological operations to generate out-of-vocabulary words and is coupled with a filtering mechanism to retain the words with more repeating syllable patterns only. The n-gram probabilities are combined through linear interpolation to detect and correct spelling errors. The test samples with spelling errors are generated by randomizing characters for non-word errors and words in higher-order n-gram for real word errors. The proposed approach achieves detection accuracy of 85.5% and correction accuracy of 75.0%, respectively.""}",2025,"2025 International Conference on Communication Technologies, ComTech 2025","{""edit distance"",""language models"",""linear interpolation"",""spell checker""}",Information Systems and Management,Decision Sciences,Social Sciences & Humanities,CS,2025,"a spell checking model detects spelling errors in the input text, generates possible corrections for each error, and organizes them based on their relevance. such tools are essential for writing, editing, and publishing in a language. in literature, different variants of edit distance and language models are used for dealing with spelling errors. the existing approaches have improved for efficient use of computational and memory resources without compromising accuracy. exploiting the efforts made for other low-resource languages, we have proposed the first spell checking model for pashto language. a common issue with the existing spell checker models of low-resourced languages is having many false positives due to limited vocabulary. our proposed approach makes use of morphological operations to generate out-of-vocabulary words and is coupled with a filtering mechanism to retain the words with more repeating syllable patterns only. the n-gram probabilities are combined through linear interpolation to detect and correct spelling errors. the test samples with spelling errors are generated by randomizing characters for non-word errors and words in higher-order n-gram for real word errors. the proposed approach achieves detection accuracy of 85.5% and correction accuracy of 75.0%, respectively.",0
Out-of-Vocabulary Pashto Spell Checker using Morphological Operations,"{""A spell checking model detects spelling errors in the input text, generates possible corrections for each error, and organizes them based on their relevance. Such tools are essential for writing, editing, and publishing in a language. In literature, different variants of edit distance and language models are used for dealing with spelling errors. The existing approaches have improved for efficient use of computational and memory resources without compromising accuracy. Exploiting the efforts made for other low-resource languages, we have proposed the first spell checking model for Pashto language. A common issue with the existing spell checker models of low-resourced languages is having many false positives due to limited vocabulary. Our proposed approach makes use of morphological operations to generate out-of-vocabulary words and is coupled with a filtering mechanism to retain the words with more repeating syllable patterns only. The n-gram probabilities are combined through linear interpolation to detect and correct spelling errors. The test samples with spelling errors are generated by randomizing characters for non-word errors and words in higher-order n-gram for real word errors. The proposed approach achieves detection accuracy of 85.5% and correction accuracy of 75.0%, respectively.""}",2025,"2025 International Conference on Communication Technologies, ComTech 2025","{""edit distance"",""language models"",""linear interpolation"",""spell checker""}",Hardware and Architecture,Computer Science,Physical Sciences,CS,2025,"a spell checking model detects spelling errors in the input text, generates possible corrections for each error, and organizes them based on their relevance. such tools are essential for writing, editing, and publishing in a language. in literature, different variants of edit distance and language models are used for dealing with spelling errors. the existing approaches have improved for efficient use of computational and memory resources without compromising accuracy. exploiting the efforts made for other low-resource languages, we have proposed the first spell checking model for pashto language. a common issue with the existing spell checker models of low-resourced languages is having many false positives due to limited vocabulary. our proposed approach makes use of morphological operations to generate out-of-vocabulary words and is coupled with a filtering mechanism to retain the words with more repeating syllable patterns only. the n-gram probabilities are combined through linear interpolation to detect and correct spelling errors. the test samples with spelling errors are generated by randomizing characters for non-word errors and words in higher-order n-gram for real word errors. the proposed approach achieves detection accuracy of 85.5% and correction accuracy of 75.0%, respectively.",0
RESI: A COMPREHENSIVE BENCHMARK FOR REPRESENTATIONAL SIMILARITY MEASURES,"{""Measuring the similarity of different representations of neural architectures is a fundamental task and an open research challenge for the machine learning community. This paper presents the first comprehensive benchmark for evaluating representational similarity measures based on well-defined groundings of similarity. The representational similarity (ReSi) benchmark consists of (i) six carefully designed tests for similarity measures, (ii) 24 similarity measures, (iii) 14 neural network architectures, and (iv) seven datasets, spanning the graph, language, and vision domains. The benchmark opens up several important avenues of research on representational similarity that enable novel explorations and applications of neural architectures. We demonstrate the utility of the ReSi benchmark by conducting experiments on various neural network architectures, real-world datasets, and similarity measures. All components of the benchmark are publicly available and thereby facilitate systematic reproduction and production of research results. The benchmark is extensible; future research can build on it and expand on it. We believe that the ReSi benchmark can serve as a sound platform catalyzing future research that aims to systematically evaluate existing and explore novel ways of comparing representations of neural architectures.""}",2025,"13th International Conference on Learning Representations, ICLR 2025",,Computer Science Applications,Computer Science,Physical Sciences,CS,2025,"measuring the similarity of different representations of neural architectures is a fundamental task and an open research challenge for the machine learning community. this paper presents the first comprehensive benchmark for evaluating representational similarity measures based on well-defined groundings of similarity. the representational similarity (resi) benchmark consists of (i) six carefully designed tests for similarity measures, (ii) 24 similarity measures, (iii) 14 neural network architectures, and (iv) seven datasets, spanning the graph, language, and vision domains. the benchmark opens up several important avenues of research on representational similarity that enable novel explorations and applications of neural architectures. we demonstrate the utility of the resi benchmark by conducting experiments on various neural network architectures, real-world datasets, and similarity measures. all components of the benchmark are publicly available and thereby facilitate systematic reproduction and production of research results. the benchmark is extensible; future research can build on it and expand on it. we believe that the resi benchmark can serve as a sound platform catalyzing future research that aims to systematically evaluate existing and explore novel ways of comparing representations of neural architectures.",3
Hybrid Evaluation of Socratic Dialogue for Teaching,"{""We present a kick-starter paper that addresses the opportunities and challenges in the intersection of Generative AI (GenAI), Semantic Web Technologies, and Human-Computer Interaction in Socratic method for educational purposes. Inspired by the example of Large Language Model (LLM) tutors using the Socratic dialogue for teaching, we motivate the need for new hybrid benchmarks and metrics that calculate the tutor’s performance by combining parameters from the LLMs, Knowledge Engineering (KE) and Hybrid Human Artificial Intelligence (HHAI) performance. We explore current problems, and propose a future direction for the hybrid implementation of Socratic dialogue with hybrid evaluation methods.""}",2025,CEUR Workshop Proceedings,"{""generative ai"",""hybrid benchmarks"",""hybrid human ai"",""hybrid metrics"",""knowledge graphs"",""large language models"",""socratic method"",""socratic sub-questions""}",Computer Science (all),,,CS,2025,"we present a kick-starter paper that addresses the opportunities and challenges in the intersection of generative ai (genai), semantic web technologies, and human-computer interaction in socratic method for educational purposes. inspired by the example of large language model (llm) tutors using the socratic dialogue for teaching, we motivate the need for new hybrid benchmarks and metrics that calculate the tutor’s performance by combining parameters from the llms, knowledge engineering (ke) and hybrid human artificial intelligence (hhai) performance. we explore current problems, and propose a future direction for the hybrid implementation of socratic dialogue with hybrid evaluation methods.",0
Evaluating Language Models for Computer Graphics Code Completion,"{""Evaluation benchmarks are essential for developing and training language models, providing both comparison and optimization targets. Existing code completion benchmarks, often based on standalone Python functions and unit tests, are overly simplistic, contaminated, and fail to reflect real-world scenarios. In this paper we present ShaderMatch, a novel benchmark for code completion in computer graphics programming. The benchmark is derived from real-world fragment shaders in OpenGL Shading Language (GLSL) from the Shadertoy platform, forming a zero-shot function completion task with 467 function headers and user-written comments as input. Besides, we propose a two-step evaluation metric: static code comparison followed by frame rendering comparison. Additionally, ShaderMatch introduces eight fine-grained labels for deeper insights. We evaluate over 20 open-source code-specific models and highlight notable performance outliers. Results show that even top models fail to generate working code in 31% of cases, highlighting the challenge posed by GLSL, a low-resource language rarely found in pretraining datasets. ShaderMatch provides a well-annotated, extendable dataset for future research. Data, code, leaderboard, and discussions are available at: https://hf.co/spaces/Vipitis/shadermatch.""}",2025,"Proceedings - 2025 IEEE/ACM International Workshop on Large Language Models for Code, LLM4Code 2025","{benchmarking,""code completion"",""fragment shaders"",glsl,""language models"",""opengl shading language""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2025,"evaluation benchmarks are essential for developing and training language models, providing both comparison and optimization targets. existing code completion benchmarks, often based on standalone python functions and unit tests, are overly simplistic, contaminated, and fail to reflect real-world scenarios. in this paper we present shadermatch, a novel benchmark for code completion in computer graphics programming. the benchmark is derived from real-world fragment shaders in opengl shading language (glsl) from the shadertoy platform, forming a zero-shot function completion task with 467 function headers and user-written comments as input. besides, we propose a two-step evaluation metric: static code comparison followed by frame rendering comparison. additionally, shadermatch introduces eight fine-grained labels for deeper insights. we evaluate over 20 open-source code-specific models and highlight notable performance outliers. results show that even top models fail to generate working code in 31% of cases, highlighting the challenge posed by glsl, a low-resource language rarely found in pretraining datasets. shadermatch provides a well-annotated, extendable dataset for future research. data, code, leaderboard, and discussions are available at: https://hf.co/spaces/vipitis/shadermatch.",0
Evaluating Language Models for Computer Graphics Code Completion,"{""Evaluation benchmarks are essential for developing and training language models, providing both comparison and optimization targets. Existing code completion benchmarks, often based on standalone Python functions and unit tests, are overly simplistic, contaminated, and fail to reflect real-world scenarios. In this paper we present ShaderMatch, a novel benchmark for code completion in computer graphics programming. The benchmark is derived from real-world fragment shaders in OpenGL Shading Language (GLSL) from the Shadertoy platform, forming a zero-shot function completion task with 467 function headers and user-written comments as input. Besides, we propose a two-step evaluation metric: static code comparison followed by frame rendering comparison. Additionally, ShaderMatch introduces eight fine-grained labels for deeper insights. We evaluate over 20 open-source code-specific models and highlight notable performance outliers. Results show that even top models fail to generate working code in 31% of cases, highlighting the challenge posed by GLSL, a low-resource language rarely found in pretraining datasets. ShaderMatch provides a well-annotated, extendable dataset for future research. Data, code, leaderboard, and discussions are available at: https://hf.co/spaces/Vipitis/shadermatch.""}",2025,"Proceedings - 2025 IEEE/ACM International Workshop on Large Language Models for Code, LLM4Code 2025","{benchmarking,""code completion"",""fragment shaders"",glsl,""language models"",""opengl shading language""}",Computational Theory and Mathematics,Computer Science,Physical Sciences,CS,2025,"evaluation benchmarks are essential for developing and training language models, providing both comparison and optimization targets. existing code completion benchmarks, often based on standalone python functions and unit tests, are overly simplistic, contaminated, and fail to reflect real-world scenarios. in this paper we present shadermatch, a novel benchmark for code completion in computer graphics programming. the benchmark is derived from real-world fragment shaders in opengl shading language (glsl) from the shadertoy platform, forming a zero-shot function completion task with 467 function headers and user-written comments as input. besides, we propose a two-step evaluation metric: static code comparison followed by frame rendering comparison. additionally, shadermatch introduces eight fine-grained labels for deeper insights. we evaluate over 20 open-source code-specific models and highlight notable performance outliers. results show that even top models fail to generate working code in 31% of cases, highlighting the challenge posed by glsl, a low-resource language rarely found in pretraining datasets. shadermatch provides a well-annotated, extendable dataset for future research. data, code, leaderboard, and discussions are available at: https://hf.co/spaces/vipitis/shadermatch.",0
Graph colouring using evolutionary computation: A case study of blind naked mole-rat algorithm,"{""Graph colouring problem (GCP) is an NP-complete optimization problem. It is famous for its applications in scheduling, register allocation, and map colouring. In recent years, biological inspired and especially Swarm intelligence (SI) techniques have gained popularity for solving complex optimization problems. In this article, we have proposed blind naked mole rat-based colouring (BNMR-Col) for graphs. BNMR-Col uses both exploitation and exploration to find the best solution in search space. Exploitation uses both local moves and global moves to find a better solution in the surroundings of an existing solution. On the other hand, exploration generates new solution by combining various solutions from the search space. BNMR-Col shows better convergence rate and approaches the lowest colour value in 83% of the cases when tested on standard benchmark graph instances.""}",2025,Expert Systems,"{""intelligent system"",""meta heuristic"",optimization,""swarm intelligence""}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2025,"graph colouring problem (gcp) is an np-complete optimization problem. it is famous for its applications in scheduling, register allocation, and map colouring. in recent years, biological inspired and especially swarm intelligence (si) techniques have gained popularity for solving complex optimization problems. in this article, we have proposed blind naked mole rat-based colouring (bnmr-col) for graphs. bnmr-col uses both exploitation and exploration to find the best solution in search space. exploitation uses both local moves and global moves to find a better solution in the surroundings of an existing solution. on the other hand, exploration generates new solution by combining various solutions from the search space. bnmr-col shows better convergence rate and approaches the lowest colour value in 83% of the cases when tested on standard benchmark graph instances.",4
"The Governance of Decentralized Autonomous Organizations: A Study of Contributors’ Influence, Networks, and Shifts in Voting Power","{""We present a study analyzing the voting behavior of contributors, or vested users, in Decentralized Autonomous Organizations (DAOs). We evaluate their involvement in decision-making processes, discovering that in at least 7.54% of all DAOs, contributors, on average, held the necessary majority to control governance decisions. Furthermore, contributors have singularly decided at least one proposal in 20.41% of DAOs. Notably, contributors tend to be centrally positioned within the DAO governance ecosystem, suggesting the presence of inner power circles. Additionally, we observed a tendency for shifts in governance token ownership shortly before governance polls take place in 1202 (14.81%) of 8116 evaluated proposals. Our findings highlight the central role of contributors across a spectrum of DAOs, including Decentralized Finance protocols. Our research also offers important empirical insights pertinent to ongoing regulatory activities aimed at increasing transparency to DAO governance frameworks.""}",2025,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"{blockchain,dao,ethereum,governance,networks,voting}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2025,"we present a study analyzing the voting behavior of contributors, or vested users, in decentralized autonomous organizations (daos). we evaluate their involvement in decision-making processes, discovering that in at least 7.54% of all daos, contributors, on average, held the necessary majority to control governance decisions. furthermore, contributors have singularly decided at least one proposal in 20.41% of daos. notably, contributors tend to be centrally positioned within the dao governance ecosystem, suggesting the presence of inner power circles. additionally, we observed a tendency for shifts in governance token ownership shortly before governance polls take place in 1202 (14.81%) of 8116 evaluated proposals. our findings highlight the central role of contributors across a spectrum of daos, including decentralized finance protocols. our research also offers important empirical insights pertinent to ongoing regulatory activities aimed at increasing transparency to dao governance frameworks.",-1
Bayes factors for two-group comparisons in Cox regression with an application for reverse-engineering raw data from summary statistics,"{""The use of Cox proportional hazards regression to analyze time-to-event data is ubiquitous in biomedical research. Typically, the frequentist framework is used to draw conclusions about whether hazards are different between patients in an experimental and a control condition. We offer a procedure to compute Bayes factors for simple Cox models, both for the scenario where the full data are available and for the scenario where only summary statistics are available. The procedure is implemented in our ‘baymedr’ R package. The usage of Bayes factors remedies some shortcomings of frequentist inference and has the potential to save scarce resources.""}",2025,Journal of Applied Statistics,"{""bayes factor"",""cox proportional hazards regression"",""particle swarm optimization"",simulation,""summary statistics"",survival}","Statistics, Probability and Uncertainty",Decision Sciences,Social Sciences & Humanities,CS,2025,"the use of cox proportional hazards regression to analyze time-to-event data is ubiquitous in biomedical research. typically, the frequentist framework is used to draw conclusions about whether hazards are different between patients in an experimental and a control condition. we offer a procedure to compute bayes factors for simple cox models, both for the scenario where the full data are available and for the scenario where only summary statistics are available. the procedure is implemented in our ‘baymedr’ r package. the usage of bayes factors remedies some shortcomings of frequentist inference and has the potential to save scarce resources.",8
Graph colouring using evolutionary computation: A case study of blind naked mole-rat algorithm,"{""Graph colouring problem (GCP) is an NP-complete optimization problem. It is famous for its applications in scheduling, register allocation, and map colouring. In recent years, biological inspired and especially Swarm intelligence (SI) techniques have gained popularity for solving complex optimization problems. In this article, we have proposed blind naked mole rat-based colouring (BNMR-Col) for graphs. BNMR-Col uses both exploitation and exploration to find the best solution in search space. Exploitation uses both local moves and global moves to find a better solution in the surroundings of an existing solution. On the other hand, exploration generates new solution by combining various solutions from the search space. BNMR-Col shows better convergence rate and approaches the lowest colour value in 83% of the cases when tested on standard benchmark graph instances.""}",2025,Expert Systems,"{""intelligent system"",""meta heuristic"",optimization,""swarm intelligence""}",Control and Systems Engineering,Engineering,Physical Sciences,CS,2025,"graph colouring problem (gcp) is an np-complete optimization problem. it is famous for its applications in scheduling, register allocation, and map colouring. in recent years, biological inspired and especially swarm intelligence (si) techniques have gained popularity for solving complex optimization problems. in this article, we have proposed blind naked mole rat-based colouring (bnmr-col) for graphs. bnmr-col uses both exploitation and exploration to find the best solution in search space. exploitation uses both local moves and global moves to find a better solution in the surroundings of an existing solution. on the other hand, exploration generates new solution by combining various solutions from the search space. bnmr-col shows better convergence rate and approaches the lowest colour value in 83% of the cases when tested on standard benchmark graph instances.",4
Sharing digital trace data: Researchers’ challenges and needs,"{""Over the past decade, research has made rapid progress in the collection and analysis of digital trace data. However, when it comes to sharing data, researchers still face major barriers that often limit or prevent the reproducibility of research results and the reuse of data. Against this backdrop, we identify three broader categories of user challenges, namely researchers’ capacities & incentives, legal & ethical challenges, and technical hurdles. We describe in detail the problems researchers face in each category and why these often prevent researchers from sharing data, thus limiting both the reproducibility of research outputs and data reuse in other research projects. We conclude each category with specific needs of researchers for sharing digital trace data and making it reusable for others. These are intended to provide researchers as well as research institutes and repositories with approaches to improve the situation of data sharing.""}",2025,Big Data and Society,"{""data access"",""data archives"",""digital trace data"",""open science"",reproducibility,""research data management""}",Computer Science Applications,Computer Science,Physical Sciences,CS,2025,"over the past decade, research has made rapid progress in the collection and analysis of digital trace data. however, when it comes to sharing data, researchers still face major barriers that often limit or prevent the reproducibility of research results and the reuse of data. against this backdrop, we identify three broader categories of user challenges, namely researchers’ capacities & incentives, legal & ethical challenges, and technical hurdles. we describe in detail the problems researchers face in each category and why these often prevent researchers from sharing data, thus limiting both the reproducibility of research outputs and data reuse in other research projects. we conclude each category with specific needs of researchers for sharing digital trace data and making it reusable for others. these are intended to provide researchers as well as research institutes and repositories with approaches to improve the situation of data sharing.",5
Enhancing autonomous vehicle acceptance with age and education sensitive simulation interventions: an experimental trial,"{""The familiarity principle posits that acceptance increases with exposure, which has previously been shown with in vivo and simulated experiences with connected and autonomous vehicles (CAVs). We investigate the impact of a simulated video-based first-person drive on CAV acceptance, as well as the impact of information customization, with a particular focus on acceptance by older individuals and those with lower education. Findings from an online experiment with N = 799 German residents reveal that the simulated experience improved acceptance across response variables such as intention to use and ease of use, particularly among older individuals. However, the opportunity to customize navigation information decreased acceptance of older individuals and those with university degrees and increased acceptance for younger individuals and those with lower educational levels.""}",2025,Transportation Research Part A: Policy and Practice,"{acceptance,""autonomous vehicles"",education,""older adults"",""self-driving cars"",""simulated autonomous driving"",transportation}",Aerospace Engineering,Engineering,Physical Sciences,CS,2025,"the familiarity principle posits that acceptance increases with exposure, which has previously been shown with in vivo and simulated experiences with connected and autonomous vehicles (cavs). we investigate the impact of a simulated video-based first-person drive on cav acceptance, as well as the impact of information customization, with a particular focus on acceptance by older individuals and those with lower education. findings from an online experiment with n = 799 german residents reveal that the simulated experience improved acceptance across response variables such as intention to use and ease of use, particularly among older individuals. however, the opportunity to customize navigation information decreased acceptance of older individuals and those with university degrees and increased acceptance for younger individuals and those with lower educational levels.",1
"The CLEF-2025 CheckThat! Lab: Subjectivity, Fact-Checking, Claim Normalization, and Retrieval","{""The CheckThat! lab aims to advance the development of innovative technologies designed to identify and to counteract online disinformation and manipulation efforts across various languages and platforms. The first five editions of the CheckThat! lab focused on the main tasks of the information verification pipeline: check-worthiness, evidence retrieval and pairing, and verification. Since the 2023 edition, the lab has broadened the focus and addressed new problems on auxiliary tasks supporting research and decision-making during the verification process. In the 2025 edition of the lab, we consider tasks at the core of the verification pipeline again as well as auxiliary tasks: Task 1 is on identification of subjectivity (a follow up of the CheckThat! 2024 edition), Task 2 is on claim normalization, Task 3 addresses fact-checking numerical claims, and Task 4 focuses on scientific web discourse processing. These tasks represent challenging classification and retrieval problems at the document and at the span level, including multilingual settings.""}",2025,Lecture Notes in Computer Science,"{""authority finding"",disinformation,fact-checking,factuality,""model robustness"",""political bias"",subjectivity}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2025,"the checkthat! lab aims to advance the development of innovative technologies designed to identify and to counteract online disinformation and manipulation efforts across various languages and platforms. the first five editions of the checkthat! lab focused on the main tasks of the information verification pipeline: check-worthiness, evidence retrieval and pairing, and verification. since the 2023 edition, the lab has broadened the focus and addressed new problems on auxiliary tasks supporting research and decision-making during the verification process. in the 2025 edition of the lab, we consider tasks at the core of the verification pipeline again as well as auxiliary tasks: task 1 is on identification of subjectivity (a follow up of the checkthat! 2024 edition), task 2 is on claim normalization, task 3 addresses fact-checking numerical claims, and task 4 focuses on scientific web discourse processing. these tasks represent challenging classification and retrieval problems at the document and at the span level, including multilingual settings.",0
"The CLEF-2025 CheckThat! Lab: Subjectivity, Fact-Checking, Claim Normalization, and Retrieval","{""The CheckThat! lab aims to advance the development of innovative technologies designed to identify and to counteract online disinformation and manipulation efforts across various languages and platforms. The first five editions of the CheckThat! lab focused on the main tasks of the information verification pipeline: check-worthiness, evidence retrieval and pairing, and verification. Since the 2023 edition, the lab has broadened the focus and addressed new problems on auxiliary tasks supporting research and decision-making during the verification process. In the 2025 edition of the lab, we consider tasks at the core of the verification pipeline again as well as auxiliary tasks: Task 1 is on identification of subjectivity (a follow up of the CheckThat! 2024 edition), Task 2 is on claim normalization, Task 3 addresses fact-checking numerical claims, and Task 4 focuses on scientific web discourse processing. These tasks represent challenging classification and retrieval problems at the document and at the span level, including multilingual settings.""}",2025,Lecture Notes in Computer Science,"{""authority finding"",disinformation,fact-checking,factuality,""model robustness"",""political bias"",subjectivity}",Computer Science (all),,,CS,2025,"the checkthat! lab aims to advance the development of innovative technologies designed to identify and to counteract online disinformation and manipulation efforts across various languages and platforms. the first five editions of the checkthat! lab focused on the main tasks of the information verification pipeline: check-worthiness, evidence retrieval and pairing, and verification. since the 2023 edition, the lab has broadened the focus and addressed new problems on auxiliary tasks supporting research and decision-making during the verification process. in the 2025 edition of the lab, we consider tasks at the core of the verification pipeline again as well as auxiliary tasks: task 1 is on identification of subjectivity (a follow up of the checkthat! 2024 edition), task 2 is on claim normalization, task 3 addresses fact-checking numerical claims, and task 4 focuses on scientific web discourse processing. these tasks represent challenging classification and retrieval problems at the document and at the span level, including multilingual settings.",0
Escaping the Filter Bubble: Evaluating Electroencephalographic Theta Band Synchronization as Indicator for Selective Exposure in Online News Reading,"{""Selective exposure to online news occurs when users favor information that confirms their beliefs, creating filter bubbles and limiting diverse perspectives. Interactive systems can counter this by recommending different perspectives, but to achieve this, they need a real-time metric for selective exposure. We present an experiment where we evaluate Electroencephalography (EEG) and eyetracking as indicators for selective exposure by using eye tracking to recognize which textual parts participants read and using EEG to quantify the magnitude of selective exposure. Participants read online news while we collected EEG and eye movements with their agreement towards the news. We show that the agreement with news correlates positively with the theta band power in the parietal area. Our results indicate that future interactive systems can sense selective exposure using EEG and eye tracking to propose a more balanced information diet. This work presents an integrated experimental setup that identifies selective exposure using gaze and EEG-based metrics.""}",2025,Conference on Human Factors in Computing Systems - Proceedings,"{co-registration,electroencephalography,""eye tracking"",news,reading,""selective exposure""}",Human-Computer Interaction,Computer Science,Physical Sciences,CS,2025,"selective exposure to online news occurs when users favor information that confirms their beliefs, creating filter bubbles and limiting diverse perspectives. interactive systems can counter this by recommending different perspectives, but to achieve this, they need a real-time metric for selective exposure. we present an experiment where we evaluate electroencephalography (eeg) and eyetracking as indicators for selective exposure by using eye tracking to recognize which textual parts participants read and using eeg to quantify the magnitude of selective exposure. participants read online news while we collected eeg and eye movements with their agreement towards the news. we show that the agreement with news correlates positively with the theta band power in the parietal area. our results indicate that future interactive systems can sense selective exposure using eeg and eye tracking to propose a more balanced information diet. this work presents an integrated experimental setup that identifies selective exposure using gaze and eeg-based metrics.",1
Bayes factors for two-group comparisons in Cox regression with an application for reverse-engineering raw data from summary statistics,"{""The use of Cox proportional hazards regression to analyze time-to-event data is ubiquitous in biomedical research. Typically, the frequentist framework is used to draw conclusions about whether hazards are different between patients in an experimental and a control condition. We offer a procedure to compute Bayes factors for simple Cox models, both for the scenario where the full data are available and for the scenario where only summary statistics are available. The procedure is implemented in our ‘baymedr’ R package. The usage of Bayes factors remedies some shortcomings of frequentist inference and has the potential to save scarce resources.""}",2025,Journal of Applied Statistics,"{""bayes factor"",""cox proportional hazards regression"",""particle swarm optimization"",simulation,""summary statistics"",survival}",Statistics and Probability,Mathematics,Physical Sciences,CS,2025,"the use of cox proportional hazards regression to analyze time-to-event data is ubiquitous in biomedical research. typically, the frequentist framework is used to draw conclusions about whether hazards are different between patients in an experimental and a control condition. we offer a procedure to compute bayes factors for simple cox models, both for the scenario where the full data are available and for the scenario where only summary statistics are available. the procedure is implemented in our ‘baymedr’ r package. the usage of bayes factors remedies some shortcomings of frequentist inference and has the potential to save scarce resources.",8
TJMN: Target-enhanced joint meta network with contrastive learning for cross-domain recommendation,"{""Cross-domain recommendation (CDR) provides a promising solution to mitigate the sparsity issue in the target domain by exploiting auxiliary information from the source domain. Recently, meta learning based methods have been proposed and achieved the state-of-the-art performance. However, these methods learn the transfer bridge solely relying on the source domain while the rich information from the target domain are ignored. Moreover, they leverage either a common transfer bridge or a personalized transfer bridge to transform user representations, without considering the multi-grained characteristics of user preference. In this paper, we propose a target-enhanced joint meta network with contrastive learning (JTMN) for cross-domain recommendation. To be specific, we develop a target bridge to incorporate information from the target domain to guide the learning process of user preference transfer. In addition, we introduce multi-grained transfer bridges to model the complex transfer patterns of user preference across different domains. At last, a target-aware contrastive learning layer is designed to obtain better user representations. The experimental results on six CDR tasks demonstrate that our proposed TJMN model significantly outperforms all strong baselines, especially when the training data become more sparse.""}",2025,Knowledge-Based Systems,"{""contrastive learning"",""cross-domain recommendation"",""meta network"",""recommender systems""}",Information Systems and Management,Decision Sciences,Social Sciences & Humanities,CS,2025,"cross-domain recommendation (cdr) provides a promising solution to mitigate the sparsity issue in the target domain by exploiting auxiliary information from the source domain. recently, meta learning based methods have been proposed and achieved the state-of-the-art performance. however, these methods learn the transfer bridge solely relying on the source domain while the rich information from the target domain are ignored. moreover, they leverage either a common transfer bridge or a personalized transfer bridge to transform user representations, without considering the multi-grained characteristics of user preference. in this paper, we propose a target-enhanced joint meta network with contrastive learning (jtmn) for cross-domain recommendation. to be specific, we develop a target bridge to incorporate information from the target domain to guide the learning process of user preference transfer. in addition, we introduce multi-grained transfer bridges to model the complex transfer patterns of user preference across different domains. at last, a target-aware contrastive learning layer is designed to obtain better user representations. the experimental results on six cdr tasks demonstrate that our proposed tjmn model significantly outperforms all strong baselines, especially when the training data become more sparse.",0
Sharing digital trace data: Researchers’ challenges and needs,"{""Over the past decade, research has made rapid progress in the collection and analysis of digital trace data. However, when it comes to sharing data, researchers still face major barriers that often limit or prevent the reproducibility of research results and the reuse of data. Against this backdrop, we identify three broader categories of user challenges, namely researchers’ capacities & incentives, legal & ethical challenges, and technical hurdles. We describe in detail the problems researchers face in each category and why these often prevent researchers from sharing data, thus limiting both the reproducibility of research outputs and data reuse in other research projects. We conclude each category with specific needs of researchers for sharing digital trace data and making it reusable for others. These are intended to provide researchers as well as research institutes and repositories with approaches to improve the situation of data sharing.""}",2025,Big Data and Society,"{""data access"",""data archives"",""digital trace data"",""open science"",reproducibility,""research data management""}",Information Systems and Management,Decision Sciences,Social Sciences & Humanities,CS,2025,"over the past decade, research has made rapid progress in the collection and analysis of digital trace data. however, when it comes to sharing data, researchers still face major barriers that often limit or prevent the reproducibility of research results and the reuse of data. against this backdrop, we identify three broader categories of user challenges, namely researchers’ capacities & incentives, legal & ethical challenges, and technical hurdles. we describe in detail the problems researchers face in each category and why these often prevent researchers from sharing data, thus limiting both the reproducibility of research outputs and data reuse in other research projects. we conclude each category with specific needs of researchers for sharing digital trace data and making it reusable for others. these are intended to provide researchers as well as research institutes and repositories with approaches to improve the situation of data sharing.",5
Evaluating Language Models for Computer Graphics Code Completion,"{""Evaluation benchmarks are essential for developing and training language models, providing both comparison and optimization targets. Existing code completion benchmarks, often based on standalone Python functions and unit tests, are overly simplistic, contaminated, and fail to reflect real-world scenarios. In this paper we present ShaderMatch, a novel benchmark for code completion in computer graphics programming. The benchmark is derived from real-world fragment shaders in OpenGL Shading Language (GLSL) from the Shadertoy platform, forming a zero-shot function completion task with 467 function headers and user-written comments as input. Besides, we propose a two-step evaluation metric: static code comparison followed by frame rendering comparison. Additionally, ShaderMatch introduces eight fine-grained labels for deeper insights. We evaluate over 20 open-source code-specific models and highlight notable performance outliers. Results show that even top models fail to generate working code in 31% of cases, highlighting the challenge posed by GLSL, a low-resource language rarely found in pretraining datasets. ShaderMatch provides a well-annotated, extendable dataset for future research. Data, code, leaderboard, and discussions are available at: https://hf.co/spaces/Vipitis/shadermatch.""}",2025,"Proceedings - 2025 IEEE/ACM International Workshop on Large Language Models for Code, LLM4Code 2025","{benchmarking,""code completion"",""fragment shaders"",glsl,""language models"",""opengl shading language""}",Information Systems,Computer Science,Physical Sciences,CS,2025,"evaluation benchmarks are essential for developing and training language models, providing both comparison and optimization targets. existing code completion benchmarks, often based on standalone python functions and unit tests, are overly simplistic, contaminated, and fail to reflect real-world scenarios. in this paper we present shadermatch, a novel benchmark for code completion in computer graphics programming. the benchmark is derived from real-world fragment shaders in opengl shading language (glsl) from the shadertoy platform, forming a zero-shot function completion task with 467 function headers and user-written comments as input. besides, we propose a two-step evaluation metric: static code comparison followed by frame rendering comparison. additionally, shadermatch introduces eight fine-grained labels for deeper insights. we evaluate over 20 open-source code-specific models and highlight notable performance outliers. results show that even top models fail to generate working code in 31% of cases, highlighting the challenge posed by glsl, a low-resource language rarely found in pretraining datasets. shadermatch provides a well-annotated, extendable dataset for future research. data, code, leaderboard, and discussions are available at: https://hf.co/spaces/vipitis/shadermatch.",0
Enhancing autonomous vehicle acceptance with age and education sensitive simulation interventions: an experimental trial,"{""The familiarity principle posits that acceptance increases with exposure, which has previously been shown with in vivo and simulated experiences with connected and autonomous vehicles (CAVs). We investigate the impact of a simulated video-based first-person drive on CAV acceptance, as well as the impact of information customization, with a particular focus on acceptance by older individuals and those with lower education. Findings from an online experiment with N = 799 German residents reveal that the simulated experience improved acceptance across response variables such as intention to use and ease of use, particularly among older individuals. However, the opportunity to customize navigation information decreased acceptance of older individuals and those with university degrees and increased acceptance for younger individuals and those with lower educational levels.""}",2025,Transportation Research Part A: Policy and Practice,"{acceptance,""autonomous vehicles"",education,""older adults"",""self-driving cars"",""simulated autonomous driving"",transportation}",Civil and Structural Engineering,Engineering,Physical Sciences,CS,2025,"the familiarity principle posits that acceptance increases with exposure, which has previously been shown with in vivo and simulated experiences with connected and autonomous vehicles (cavs). we investigate the impact of a simulated video-based first-person drive on cav acceptance, as well as the impact of information customization, with a particular focus on acceptance by older individuals and those with lower education. findings from an online experiment with n = 799 german residents reveal that the simulated experience improved acceptance across response variables such as intention to use and ease of use, particularly among older individuals. however, the opportunity to customize navigation information decreased acceptance of older individuals and those with university degrees and increased acceptance for younger individuals and those with lower educational levels.",1
TJMN: Target-enhanced joint meta network with contrastive learning for cross-domain recommendation,"{""Cross-domain recommendation (CDR) provides a promising solution to mitigate the sparsity issue in the target domain by exploiting auxiliary information from the source domain. Recently, meta learning based methods have been proposed and achieved the state-of-the-art performance. However, these methods learn the transfer bridge solely relying on the source domain while the rich information from the target domain are ignored. Moreover, they leverage either a common transfer bridge or a personalized transfer bridge to transform user representations, without considering the multi-grained characteristics of user preference. In this paper, we propose a target-enhanced joint meta network with contrastive learning (JTMN) for cross-domain recommendation. To be specific, we develop a target bridge to incorporate information from the target domain to guide the learning process of user preference transfer. In addition, we introduce multi-grained transfer bridges to model the complex transfer patterns of user preference across different domains. At last, a target-aware contrastive learning layer is designed to obtain better user representations. The experimental results on six CDR tasks demonstrate that our proposed TJMN model significantly outperforms all strong baselines, especially when the training data become more sparse.""}",2025,Knowledge-Based Systems,"{""contrastive learning"",""cross-domain recommendation"",""meta network"",""recommender systems""}",Software,Computer Science,Physical Sciences,CS,2025,"cross-domain recommendation (cdr) provides a promising solution to mitigate the sparsity issue in the target domain by exploiting auxiliary information from the source domain. recently, meta learning based methods have been proposed and achieved the state-of-the-art performance. however, these methods learn the transfer bridge solely relying on the source domain while the rich information from the target domain are ignored. moreover, they leverage either a common transfer bridge or a personalized transfer bridge to transform user representations, without considering the multi-grained characteristics of user preference. in this paper, we propose a target-enhanced joint meta network with contrastive learning (jtmn) for cross-domain recommendation. to be specific, we develop a target bridge to incorporate information from the target domain to guide the learning process of user preference transfer. in addition, we introduce multi-grained transfer bridges to model the complex transfer patterns of user preference across different domains. at last, a target-aware contrastive learning layer is designed to obtain better user representations. the experimental results on six cdr tasks demonstrate that our proposed tjmn model significantly outperforms all strong baselines, especially when the training data become more sparse.",0
Extracting and Modeling Tabular Data from Marine Geology Publications into a Heterogeneous Information Network,"{""Scientific publications serve as a source of disseminating information across research communities, often containing diverse data elements such as plain-text, tables, and figures. Tables in particular offer a structured presentation of essential research data, enabling efficient information access. Automatic extraction of tabular data alongside contextual information from scientific publications can significantly enhance research workflows and integrate more research data into scholarly research cycle, particularly supporting Research Data Management (RDM). In marine geology, the researchers conduct expeditions at oceanographic locations and accumulate substantial amounts of valuable data such as Sedimentation Rate (SR), Mass Accumulation Rate (MAR) alongside relevant contextual information, often enriched with spatio-temporal context in tables of publications. These expeditions are costly and time intensive, emphasizing on the value of making such data more accessible and reusable. This paper introduces an end to end approach to extract and model heterogeneous tabular data from marine geology publications. Our approach extracts metadata and tabular content from publications, modeling them into a Heterogeneous Information Network (HIN). The network uncovers hidden relationships and patterns across multiple documents, offering new insights and facilitating enhanced data referencing. Experimental results and exploration on marine geology datasets demonstrate the effectiveness of our approach, showcasing its potential to support research data management and data driven scientific exploration.""}",2025,International Conference on Pattern Recognition Applications and Methods,"{""data modeling"",""heterogeneous information network"",""information extraction"",""marine science publication"",""research data management"",""tabular data""}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2025,"scientific publications serve as a source of disseminating information across research communities, often containing diverse data elements such as plain-text, tables, and figures. tables in particular offer a structured presentation of essential research data, enabling efficient information access. automatic extraction of tabular data alongside contextual information from scientific publications can significantly enhance research workflows and integrate more research data into scholarly research cycle, particularly supporting research data management (rdm). in marine geology, the researchers conduct expeditions at oceanographic locations and accumulate substantial amounts of valuable data such as sedimentation rate (sr), mass accumulation rate (mar) alongside relevant contextual information, often enriched with spatio-temporal context in tables of publications. these expeditions are costly and time intensive, emphasizing on the value of making such data more accessible and reusable. this paper introduces an end to end approach to extract and model heterogeneous tabular data from marine geology publications. our approach extracts metadata and tabular content from publications, modeling them into a heterogeneous information network (hin). the network uncovers hidden relationships and patterns across multiple documents, offering new insights and facilitating enhanced data referencing. experimental results and exploration on marine geology datasets demonstrate the effectiveness of our approach, showcasing its potential to support research data management and data driven scientific exploration.",2
Similarity of Neural Network Models: A Survey of Functional and Representational Measures,"{""Measuring similarity of neural networks to understand and improve their behavior has become an issue of great importance and research interest. In this survey, we provide a comprehensive overview of two complementary perspectives of measuring neural network similarity: (i) representational similarity, which considers how activations of intermediate layers differ, and (ii) functional similarity, which considers how models differ in their outputs. In addition to providing detailed descriptions of existing measures, we summarize and discuss results on the properties of and relationships between these measures, and point to open research problems. We hope our work lays a foundation for more systematic research on the properties and applicability of similarity measures for neural network models.""}",2025,ACM Computing Surveys,"{""deep learning"",""functional similarity"",""representational similarity""}",Computer Science (all),,,CS,2025,"measuring similarity of neural networks to understand and improve their behavior has become an issue of great importance and research interest. in this survey, we provide a comprehensive overview of two complementary perspectives of measuring neural network similarity: (i) representational similarity, which considers how activations of intermediate layers differ, and (ii) functional similarity, which considers how models differ in their outputs. in addition to providing detailed descriptions of existing measures, we summarize and discuss results on the properties of and relationships between these measures, and point to open research problems. we hope our work lays a foundation for more systematic research on the properties and applicability of similarity measures for neural network models.",3
TJMN: Target-enhanced joint meta network with contrastive learning for cross-domain recommendation,"{""Cross-domain recommendation (CDR) provides a promising solution to mitigate the sparsity issue in the target domain by exploiting auxiliary information from the source domain. Recently, meta learning based methods have been proposed and achieved the state-of-the-art performance. However, these methods learn the transfer bridge solely relying on the source domain while the rich information from the target domain are ignored. Moreover, they leverage either a common transfer bridge or a personalized transfer bridge to transform user representations, without considering the multi-grained characteristics of user preference. In this paper, we propose a target-enhanced joint meta network with contrastive learning (JTMN) for cross-domain recommendation. To be specific, we develop a target bridge to incorporate information from the target domain to guide the learning process of user preference transfer. In addition, we introduce multi-grained transfer bridges to model the complex transfer patterns of user preference across different domains. At last, a target-aware contrastive learning layer is designed to obtain better user representations. The experimental results on six CDR tasks demonstrate that our proposed TJMN model significantly outperforms all strong baselines, especially when the training data become more sparse.""}",2025,Knowledge-Based Systems,"{""contrastive learning"",""cross-domain recommendation"",""meta network"",""recommender systems""}",Artificial Intelligence,Computer Science,Physical Sciences,CS,2025,"cross-domain recommendation (cdr) provides a promising solution to mitigate the sparsity issue in the target domain by exploiting auxiliary information from the source domain. recently, meta learning based methods have been proposed and achieved the state-of-the-art performance. however, these methods learn the transfer bridge solely relying on the source domain while the rich information from the target domain are ignored. moreover, they leverage either a common transfer bridge or a personalized transfer bridge to transform user representations, without considering the multi-grained characteristics of user preference. in this paper, we propose a target-enhanced joint meta network with contrastive learning (jtmn) for cross-domain recommendation. to be specific, we develop a target bridge to incorporate information from the target domain to guide the learning process of user preference transfer. in addition, we introduce multi-grained transfer bridges to model the complex transfer patterns of user preference across different domains. at last, a target-aware contrastive learning layer is designed to obtain better user representations. the experimental results on six cdr tasks demonstrate that our proposed tjmn model significantly outperforms all strong baselines, especially when the training data become more sparse.",0
Graph colouring using evolutionary computation: A case study of blind naked mole-rat algorithm,"{""Graph colouring problem (GCP) is an NP-complete optimization problem. It is famous for its applications in scheduling, register allocation, and map colouring. In recent years, biological inspired and especially Swarm intelligence (SI) techniques have gained popularity for solving complex optimization problems. In this article, we have proposed blind naked mole rat-based colouring (BNMR-Col) for graphs. BNMR-Col uses both exploitation and exploration to find the best solution in search space. Exploitation uses both local moves and global moves to find a better solution in the surroundings of an existing solution. On the other hand, exploration generates new solution by combining various solutions from the search space. BNMR-Col shows better convergence rate and approaches the lowest colour value in 83% of the cases when tested on standard benchmark graph instances.""}",2025,Expert Systems,"{""intelligent system"",""meta heuristic"",optimization,""swarm intelligence""}",Computational Theory and Mathematics,Computer Science,Physical Sciences,CS,2025,"graph colouring problem (gcp) is an np-complete optimization problem. it is famous for its applications in scheduling, register allocation, and map colouring. in recent years, biological inspired and especially swarm intelligence (si) techniques have gained popularity for solving complex optimization problems. in this article, we have proposed blind naked mole rat-based colouring (bnmr-col) for graphs. bnmr-col uses both exploitation and exploration to find the best solution in search space. exploitation uses both local moves and global moves to find a better solution in the surroundings of an existing solution. on the other hand, exploration generates new solution by combining various solutions from the search space. bnmr-col shows better convergence rate and approaches the lowest colour value in 83% of the cases when tested on standard benchmark graph instances.",4
Enhancing autonomous vehicle acceptance with age and education sensitive simulation interventions: an experimental trial,"{""The familiarity principle posits that acceptance increases with exposure, which has previously been shown with in vivo and simulated experiences with connected and autonomous vehicles (CAVs). We investigate the impact of a simulated video-based first-person drive on CAV acceptance, as well as the impact of information customization, with a particular focus on acceptance by older individuals and those with lower education. Findings from an online experiment with N = 799 German residents reveal that the simulated experience improved acceptance across response variables such as intention to use and ease of use, particularly among older individuals. However, the opportunity to customize navigation information decreased acceptance of older individuals and those with university degrees and increased acceptance for younger individuals and those with lower educational levels.""}",2025,Transportation Research Part A: Policy and Practice,"{acceptance,""autonomous vehicles"",education,""older adults"",""self-driving cars"",""simulated autonomous driving"",transportation}",Management Science and Operations Research,Decision Sciences,Social Sciences & Humanities,CS,2025,"the familiarity principle posits that acceptance increases with exposure, which has previously been shown with in vivo and simulated experiences with connected and autonomous vehicles (cavs). we investigate the impact of a simulated video-based first-person drive on cav acceptance, as well as the impact of information customization, with a particular focus on acceptance by older individuals and those with lower education. findings from an online experiment with n = 799 german residents reveal that the simulated experience improved acceptance across response variables such as intention to use and ease of use, particularly among older individuals. however, the opportunity to customize navigation information decreased acceptance of older individuals and those with university degrees and increased acceptance for younger individuals and those with lower educational levels.",1
The First Workshop on Scholarly Information Access (SCOLIA),"{""The first workshop on Scholarly Information Access (SCOLIA) will take place at ECIR 2025 as a half-day workshop. The workshop is building upon and following up on the long series of the Bibliometric-enhanced Information Retrieval (BIR) workshops at ECIR. SCOLIA addresses research topics related to academic search and recommendation, at the intersection of Information Retrieval, Natural Language Processing (including generative AI), and Bibliometrics. As an interdisciplinary and intersectoral scientific event, addressing an ever-growing topic investigated by both academia and industry, SCOLIA brings together researchers and practitioners from the aforementioned communities. The interactive format fosters engagement of all participants and fruitful discussions. The outcome of the workshop will reflect the current state and identify future research questions.""}",2025,Lecture Notes in Computer Science,"{""academic search"",bibliometrics,""digital libraries"",""information access"",""information retrieval"",""natural language processing""}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2025,"the first workshop on scholarly information access (scolia) will take place at ecir 2025 as a half-day workshop. the workshop is building upon and following up on the long series of the bibliometric-enhanced information retrieval (bir) workshops at ecir. scolia addresses research topics related to academic search and recommendation, at the intersection of information retrieval, natural language processing (including generative ai), and bibliometrics. as an interdisciplinary and intersectoral scientific event, addressing an ever-growing topic investigated by both academia and industry, scolia brings together researchers and practitioners from the aforementioned communities. the interactive format fosters engagement of all participants and fruitful discussions. the outcome of the workshop will reflect the current state and identify future research questions.",2
Sharing digital trace data: Researchers’ challenges and needs,"{""Over the past decade, research has made rapid progress in the collection and analysis of digital trace data. However, when it comes to sharing data, researchers still face major barriers that often limit or prevent the reproducibility of research results and the reuse of data. Against this backdrop, we identify three broader categories of user challenges, namely researchers’ capacities & incentives, legal & ethical challenges, and technical hurdles. We describe in detail the problems researchers face in each category and why these often prevent researchers from sharing data, thus limiting both the reproducibility of research outputs and data reuse in other research projects. We conclude each category with specific needs of researchers for sharing digital trace data and making it reusable for others. These are intended to provide researchers as well as research institutes and repositories with approaches to improve the situation of data sharing.""}",2025,Big Data and Society,"{""data access"",""data archives"",""digital trace data"",""open science"",reproducibility,""research data management""}",Information Systems,Computer Science,Physical Sciences,CS,2025,"over the past decade, research has made rapid progress in the collection and analysis of digital trace data. however, when it comes to sharing data, researchers still face major barriers that often limit or prevent the reproducibility of research results and the reuse of data. against this backdrop, we identify three broader categories of user challenges, namely researchers’ capacities & incentives, legal & ethical challenges, and technical hurdles. we describe in detail the problems researchers face in each category and why these often prevent researchers from sharing data, thus limiting both the reproducibility of research outputs and data reuse in other research projects. we conclude each category with specific needs of researchers for sharing digital trace data and making it reusable for others. these are intended to provide researchers as well as research institutes and repositories with approaches to improve the situation of data sharing.",5
Understanding the Impact of Entity Linking on the Topology of Entity Co-occurrence Networks for Social Media Analysis,"{""A common form of analysis of textual data is entity co-occurrence, where networks of entities and their connections within the text are constructed and their topology analysed. As the analysis is focused on the entities and their relations, the tools used to extract them can have a potentially large effect on the results. A frequently used method as part of these analyses is entity linking, where extracted entities are mapped to a knowledge graph. Many established entity linking tools have been created for long text following standard spelling and grammar rules. As a result, the tools struggle on short, unstructured text such as tweets. On such text, it can be difficult to choose between tools and parameter settings, especially since ground truth is often unavailable. Given these challenges in entity linking on text and the direct influence of extracted entities on subsequent network analysis, we propose the need to apply multiple tools to create a more holistic set of results. We verify this assertion through a set of experiments. Using a dataset of approximately 21 million English-language tweets, we construct multiple entity co-occurrence networks using two tools (Fast Entity Linker and DBpedia Spotlight) and numerous confidence thresholds for each. We find that standard network analysis metrics, such as size, connectivity, and centrality are all heavily influenced by the choice of entity linking tool.""}",2025,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"{""co-occurrence networks"",""entity linking"",""network analysis"",""social media""}",Theoretical Computer Science,Mathematics,Physical Sciences,CS,2025,"a common form of analysis of textual data is entity co-occurrence, where networks of entities and their connections within the text are constructed and their topology analysed. as the analysis is focused on the entities and their relations, the tools used to extract them can have a potentially large effect on the results. a frequently used method as part of these analyses is entity linking, where extracted entities are mapped to a knowledge graph. many established entity linking tools have been created for long text following standard spelling and grammar rules. as a result, the tools struggle on short, unstructured text such as tweets. on such text, it can be difficult to choose between tools and parameter settings, especially since ground truth is often unavailable. given these challenges in entity linking on text and the direct influence of extracted entities on subsequent network analysis, we propose the need to apply multiple tools to create a more holistic set of results. we verify this assertion through a set of experiments. using a dataset of approximately 21 million english-language tweets, we construct multiple entity co-occurrence networks using two tools (fast entity linker and dbpedia spotlight) and numerous confidence thresholds for each. we find that standard network analysis metrics, such as size, connectivity, and centrality are all heavily influenced by the choice of entity linking tool.",0
Out-of-Vocabulary Pashto Spell Checker using Morphological Operations,"{""A spell checking model detects spelling errors in the input text, generates possible corrections for each error, and organizes them based on their relevance. Such tools are essential for writing, editing, and publishing in a language. In literature, different variants of edit distance and language models are used for dealing with spelling errors. The existing approaches have improved for efficient use of computational and memory resources without compromising accuracy. Exploiting the efforts made for other low-resource languages, we have proposed the first spell checking model for Pashto language. A common issue with the existing spell checker models of low-resourced languages is having many false positives due to limited vocabulary. Our proposed approach makes use of morphological operations to generate out-of-vocabulary words and is coupled with a filtering mechanism to retain the words with more repeating syllable patterns only. The n-gram probabilities are combined through linear interpolation to detect and correct spelling errors. The test samples with spelling errors are generated by randomizing characters for non-word errors and words in higher-order n-gram for real word errors. The proposed approach achieves detection accuracy of 85.5% and correction accuracy of 75.0%, respectively.""}",2025,"2025 International Conference on Communication Technologies, ComTech 2025","{""edit distance"",""language models"",""linear interpolation"",""spell checker""}",Information Systems,Computer Science,Physical Sciences,CS,2025,"a spell checking model detects spelling errors in the input text, generates possible corrections for each error, and organizes them based on their relevance. such tools are essential for writing, editing, and publishing in a language. in literature, different variants of edit distance and language models are used for dealing with spelling errors. the existing approaches have improved for efficient use of computational and memory resources without compromising accuracy. exploiting the efforts made for other low-resource languages, we have proposed the first spell checking model for pashto language. a common issue with the existing spell checker models of low-resourced languages is having many false positives due to limited vocabulary. our proposed approach makes use of morphological operations to generate out-of-vocabulary words and is coupled with a filtering mechanism to retain the words with more repeating syllable patterns only. the n-gram probabilities are combined through linear interpolation to detect and correct spelling errors. the test samples with spelling errors are generated by randomizing characters for non-word errors and words in higher-order n-gram for real word errors. the proposed approach achieves detection accuracy of 85.5% and correction accuracy of 75.0%, respectively.",0
Exploration of Hugging Face Models by Heterogeneous Information Network and Linking Across Scholarly Repositories,"{""With the pervasive integration of Machine Learning (ML) focusing complex tasks across various domains, generally respective models and datasets are made available in numerous scientific repositories such as Hugging Face, GitHub for recognition and understanding within the research communities hence supporting open science initiative. However, the adaptability of these repositories is increasing among users hence raising a concern about the usability of these models and datasets effectively. Therefore, it is necessary to explore these repositories and compile comprehensive information that could facilitate users as well as repositories itself. Hugging Face, a leading repository, aims to furnish a platform that organizes and presents detailed information on models and datasets employed in research. As its adoption escalates within the research communities, the necessity to delve into such repositories becomes crucial to offer researchers valuable insights and promote efficient knowledge dissemination. This study focuses on exploring Hugging Face, particularly its machine learning models by exploiting various relevant features and the insights are presented in a Heterogeneous Information Network (HIN). Our research not only demonstrates the effectiveness of the available models on Hugging Face but also highlights potential links to relevant scholarly repositories by highlighting the significance of exploration which could contribute to the future integration of repositories, facilitating a more unified and accessible framework for scientific research.""}",2025,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"{""data exploration"",""heterogeneous information network"",""hugging face"",""machine learning models"",""repository exploration""}",Computer Science (all),,,CS,2025,"with the pervasive integration of machine learning (ml) focusing complex tasks across various domains, generally respective models and datasets are made available in numerous scientific repositories such as hugging face, github for recognition and understanding within the research communities hence supporting open science initiative. however, the adaptability of these repositories is increasing among users hence raising a concern about the usability of these models and datasets effectively. therefore, it is necessary to explore these repositories and compile comprehensive information that could facilitate users as well as repositories itself. hugging face, a leading repository, aims to furnish a platform that organizes and presents detailed information on models and datasets employed in research. as its adoption escalates within the research communities, the necessity to delve into such repositories becomes crucial to offer researchers valuable insights and promote efficient knowledge dissemination. this study focuses on exploring hugging face, particularly its machine learning models by exploiting various relevant features and the insights are presented in a heterogeneous information network (hin). our research not only demonstrates the effectiveness of the available models on hugging face but also highlights potential links to relevant scholarly repositories by highlighting the significance of exploration which could contribute to the future integration of repositories, facilitating a more unified and accessible framework for scientific research.",3
Escaping the Filter Bubble: Evaluating Electroencephalographic Theta Band Synchronization as Indicator for Selective Exposure in Online News Reading,"{""Selective exposure to online news occurs when users favor information that confirms their beliefs, creating filter bubbles and limiting diverse perspectives. Interactive systems can counter this by recommending different perspectives, but to achieve this, they need a real-time metric for selective exposure. We present an experiment where we evaluate Electroencephalography (EEG) and eyetracking as indicators for selective exposure by using eye tracking to recognize which textual parts participants read and using EEG to quantify the magnitude of selective exposure. Participants read online news while we collected EEG and eye movements with their agreement towards the news. We show that the agreement with news correlates positively with the theta band power in the parietal area. Our results indicate that future interactive systems can sense selective exposure using EEG and eye tracking to propose a more balanced information diet. This work presents an integrated experimental setup that identifies selective exposure using gaze and EEG-based metrics.""}",2025,Conference on Human Factors in Computing Systems - Proceedings,"{co-registration,electroencephalography,""eye tracking"",news,reading,""selective exposure""}",Software,Computer Science,Physical Sciences,CS,2025,"selective exposure to online news occurs when users favor information that confirms their beliefs, creating filter bubbles and limiting diverse perspectives. interactive systems can counter this by recommending different perspectives, but to achieve this, they need a real-time metric for selective exposure. we present an experiment where we evaluate electroencephalography (eeg) and eyetracking as indicators for selective exposure by using eye tracking to recognize which textual parts participants read and using eeg to quantify the magnitude of selective exposure. participants read online news while we collected eeg and eye movements with their agreement towards the news. we show that the agreement with news correlates positively with the theta band power in the parietal area. our results indicate that future interactive systems can sense selective exposure using eeg and eye tracking to propose a more balanced information diet. this work presents an integrated experimental setup that identifies selective exposure using gaze and eeg-based metrics.",1
