Topic,Count,Name,Representation,Representative_Docs
-1,21,-1_detection_task_key_framework,"['detection', 'task', 'key', 'framework', 'online']","['anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. however, within the field of semantic web and knowledge graphs, anomaly detection has been relatively overlooked. additionally, the existing literature on anomaly detection over knowledge graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. in light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over knowledge graphs. in this overview, we review the quality metrics of kgs and discuss the possible errors which may occur in different parts of the rdf data. additionally, we outline a generic conceptual framework for the execution pipeline of anomaly detection over kgs. moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for kgs.', 'anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. however, within the field of semantic web and knowledge graphs, anomaly detection has been relatively overlooked. additionally, the existing literature on anomaly detection over knowledge graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. in light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over knowledge graphs. in this overview, we review the quality metrics of kgs and discuss the possible errors which may occur in different parts of the rdf data. additionally, we outline a generic conceptual framework for the execution pipeline of anomaly detection over kgs. moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for kgs.', 'anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. however, within the field of semantic web and knowledge graphs, anomaly detection has been relatively overlooked. additionally, the existing literature on anomaly detection over knowledge graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. in light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over knowledge graphs. in this overview, we review the quality metrics of kgs and discuss the possible errors which may occur in different parts of the rdf data. additionally, we outline a generic conceptual framework for the execution pipeline of anomaly detection over kgs. moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for kgs.']"
0,62,0_training_tasks_domain_accuracy,"['training', 'tasks', 'domain', 'accuracy', 'context']","['language models can serve as a valuable tool for software developers to increase productivity. large generative models can be used for code generation and code completion, while smaller encoder-only models are capable of performing code search tasks using natural language queries. these capabilities are heavily influenced by the quality and diversity of the available training data. source code datasets used for training usually focus on the most popular languages and testing is mostly conducted on the same distributions, often overlooking low-resource programming languages. motivated by the nlp generalization taxonomy proposed by hupkes et. al., we propose a new benchmark dataset called gencodesearchnet (gecs) which builds upon existing natural language code search datasets to systemically evaluate the programming language understanding generalization capabilities of language models. as part of the full dataset, we introduce a new, manually curated subset statcodesearch that focuses on r, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science. for evaluation and comparison, we collect several baseline results using fine-tuned bert-style models and gpt-style large language models in a zero-shot setting.', 'language models can serve as a valuable tool for software developers to increase productivity. large generative models can be used for code generation and code completion, while smaller encoder-only models are capable of performing code search tasks using natural language queries. these capabilities are heavily influenced by the quality and diversity of the available training data. source code datasets used for training usually focus on the most popular languages and testing is mostly conducted on the same distributions, often overlooking low-resource programming languages. motivated by the nlp generalization taxonomy proposed by hupkes et. al., we propose a new benchmark dataset called gencodesearchnet (gecs) which builds upon existing natural language code search datasets to systemically evaluate the programming language understanding generalization capabilities of language models. as part of the full dataset, we introduce a new, manually curated subset statcodesearch that focuses on r, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science. for evaluation and comparison, we collect several baseline results using fine-tuned bert-style models and gpt-style large language models in a zero-shot setting.', 'language models can serve as a valuable tool for software developers to increase productivity. large generative models can be used for code generation and code completion, while smaller encoder-only models are capable of performing code search tasks using natural language queries. these capabilities are heavily influenced by the quality and diversity of the available training data. source code datasets used for training usually focus on the most popular languages and testing is mostly conducted on the same distributions, often overlooking low-resource programming languages. motivated by the nlp generalization taxonomy proposed by hupkes et. al., we propose a new benchmark dataset called gencodesearchnet (gecs) which builds upon existing natural language code search datasets to systemically evaluate the programming language understanding generalization capabilities of language models. as part of the full dataset, we introduce a new, manually curated subset statcodesearch that focuses on r, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science. for evaluation and comparison, we collect several baseline results using fine-tuned bert-style models and gpt-style large language models in a zero-shot setting.']"
1,28,1_online_training_topics_time,"['online', 'training', 'topics', 'time', 'artificial intelligence']","['given the recent proliferation of fake news online, fact-checking has emerged as a critical defence against misinformation. several fact-checking organisations are currently employed in the initiative to assess the truthfulness of online claims. verified claims serve as foundational data for various cross-domain research, including fields of social science and natural language processing, where they are used to study misinformation and several downstream tasks such as automated fact-verification. however, these fact-checking websites inherently harbour biases, posing challenges for academic endeavours aiming to discern truth from misinformation. in this study, we aim to explore the evolving landscape of online claims verified by multiple fact-checking organisations and analyse the underlying biases of individual fact-checking websites. leveraging claimskg, the largest available corpus of fact-checked claims, we analyse the temporal evolution of claims, focusing on topics, veracity levels, and entities to offer insights into the complex dimensions of online information. we utilise data and dimensions available from claimskg for our analysis and for dimensions such as topics which are not present in claimskg, we create a topic taxonomy and implement a transformer-based model, for multi-label classification of claims. we also observe how similar claims are co-occurant amongst different websites. our work serves as a standardised framework for categorising claims sourced from diverse fact-checking organisations, laying the foundation for coherent and interpretable fact-checking datasets. the analysis conducted in this work sheds light on the dynamic landscape of online claims verified by several fact-checking organisations and dives into biases and distributions of several fact-checking websites.', 'given the recent proliferation of fake news online, fact-checking has emerged as a critical defence against misinformation. several fact-checking organisations are currently employed in the initiative to assess the truthfulness of online claims. verified claims serve as foundational data for various cross-domain research, including fields of social science and natural language processing, where they are used to study misinformation and several downstream tasks such as automated fact-verification. however, these fact-checking websites inherently harbour biases, posing challenges for academic endeavours aiming to discern truth from misinformation. in this study, we aim to explore the evolving landscape of online claims verified by multiple fact-checking organisations and analyse the underlying biases of individual fact-checking websites. leveraging claimskg, the largest available corpus of fact-checked claims, we analyse the temporal evolution of claims, focusing on topics, veracity levels, and entities to offer insights into the complex dimensions of online information. we utilise data and dimensions available from claimskg for our analysis and for dimensions such as topics which are not present in claimskg, we create a topic taxonomy and implement a transformer-based model, for multi-label classification of claims. we also observe how similar claims are co-occurant amongst different websites. our work serves as a standardised framework for categorising claims sourced from diverse fact-checking organisations, laying the foundation for coherent and interpretable fact-checking datasets. the analysis conducted in this work sheds light on the dynamic landscape of online claims verified by several fact-checking organisations and dives into biases and distributions of several fact-checking websites.', 'given the recent proliferation of fake news online, fact-checking has emerged as a critical defence against misinformation. several fact-checking organisations are currently employed in the initiative to assess the truthfulness of online claims. verified claims serve as foundational data for various cross-domain research, including fields of social science and natural language processing, where they are used to study misinformation and several downstream tasks such as automated fact-verification. however, these fact-checking websites inherently harbour biases, posing challenges for academic endeavours aiming to discern truth from misinformation. in this study, we aim to explore the evolving landscape of online claims verified by multiple fact-checking organisations and analyse the underlying biases of individual fact-checking websites. leveraging claimskg, the largest available corpus of fact-checked claims, we analyse the temporal evolution of claims, focusing on topics, veracity levels, and entities to offer insights into the complex dimensions of online information. we utilise data and dimensions available from claimskg for our analysis and for dimensions such as topics which are not present in claimskg, we create a topic taxonomy and implement a transformer-based model, for multi-label classification of claims. we also observe how similar claims are co-occurant amongst different websites. our work serves as a standardised framework for categorising claims sourced from diverse fact-checking organisations, laying the foundation for coherent and interpretable fact-checking datasets. the analysis conducted in this work sheds light on the dynamic landscape of online claims verified by several fact-checking organisations and dives into biases and distributions of several fact-checking websites.']"
2,24,2_reproducibility_framework_domain_level,"['reproducibility', 'framework', 'domain', 'level', 'recent years']","['in recent years, exciting sources of data have been modeled as knowledge graphs (kgs). this modeling represents both structural relationships and the entity-specific multi-modal data in kgs. in various data analytics pipelines and machine learning (ml), the task of semantic similarity estimation plays a significant role. assigning similarity values to entity pairs is needed in recommendation systems, clustering, classification, entity matching/disambiguation and many others. efficient and scalable frameworks are needed to handle the quadratic complexity of all-pair semantic similarity on big data kgs. moreover, heterogeneous kgs demand multi-modal semantic similarity estimation to cover the versatile contents like categorical relations between classes or their attribute literals like strings, timestamps or numeric data. in this paper, we propose the sime4kg framework as a resource providing generic open-source modules that perform semantic similarity estimation in multi-modal kgs. to justify the computational costs of similarity estimation, the sime4kg generates reproducible, reusable and explainable results. the pipeline results are a native semantic rdf kg, including the experiment results, hyper-parameter setup and explanation of the results, like the most influential features. for fast and scalable execution in memory, we implemented the distributed approach using apache spark. the entire development of this framework is integrated into the holistic distributed semantic analytics stack (sansa).', 'in recent years, exciting sources of data have been modeled as knowledge graphs (kgs). this modeling represents both structural relationships and the entity-specific multi-modal data in kgs. in various data analytics pipelines and machine learning (ml), the task of semantic similarity estimation plays a significant role. assigning similarity values to entity pairs is needed in recommendation systems, clustering, classification, entity matching/disambiguation and many others. efficient and scalable frameworks are needed to handle the quadratic complexity of all-pair semantic similarity on big data kgs. moreover, heterogeneous kgs demand multi-modal semantic similarity estimation to cover the versatile contents like categorical relations between classes or their attribute literals like strings, timestamps or numeric data. in this paper, we propose the sime4kg framework as a resource providing generic open-source modules that perform semantic similarity estimation in multi-modal kgs. to justify the computational costs of similarity estimation, the sime4kg generates reproducible, reusable and explainable results. the pipeline results are a native semantic rdf kg, including the experiment results, hyper-parameter setup and explanation of the results, like the most influential features. for fast and scalable execution in memory, we implemented the distributed approach using apache spark. the entire development of this framework is integrated into the holistic distributed semantic analytics stack (sansa).', 'in recent years, exciting sources of data have been modeled as knowledge graphs (kgs). this modeling represents both structural relationships and the entity-specific multi-modal data in kgs. in various data analytics pipelines and machine learning (ml), the task of semantic similarity estimation plays a significant role. assigning similarity values to entity pairs is needed in recommendation systems, clustering, classification, entity matching/disambiguation and many others. efficient and scalable frameworks are needed to handle the quadratic complexity of all-pair semantic similarity on big data kgs. moreover, heterogeneous kgs demand multi-modal semantic similarity estimation to cover the versatile contents like categorical relations between classes or their attribute literals like strings, timestamps or numeric data. in this paper, we propose the sime4kg framework as a resource providing generic open-source modules that perform semantic similarity estimation in multi-modal kgs. to justify the computational costs of similarity estimation, the sime4kg generates reproducible, reusable and explainable results. the pipeline results are a native semantic rdf kg, including the experiment results, hyper-parameter setup and explanation of the results, like the most influential features. for fast and scalable execution in memory, we implemented the distributed approach using apache spark. the entire development of this framework is integrated into the holistic distributed semantic analytics stack (sansa).']"
3,21,3_graph_network_state_best,"['graph', 'network', 'state', 'best', 'design']","[""session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. meanwhile, previous works usually apply gnn to capture the transformation relationship between items, however the graph used in gnn is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. in this paper, we propose a novel method called dynamic global structure enhanced multi-channel graph neural network (dgs-mgnn) to learn accurate representations of items from multiple perspectives. in dgs-mgnn, we propose a novel gnn model named multi-channel graph neural network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. we conduct extensive experiments on three widely used datasets, and the results demonstrate that dgs-mgnn is consistently superior to the state-of-the-art baseline models."", ""session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. meanwhile, previous works usually apply gnn to capture the transformation relationship between items, however the graph used in gnn is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. in this paper, we propose a novel method called dynamic global structure enhanced multi-channel graph neural network (dgs-mgnn) to learn accurate representations of items from multiple perspectives. in dgs-mgnn, we propose a novel gnn model named multi-channel graph neural network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. we conduct extensive experiments on three widely used datasets, and the results demonstrate that dgs-mgnn is consistently superior to the state-of-the-art baseline models."", ""session-based recommendation is a challenging task, which aims at making recommendation for anonymous users based on in-session data, i.e. short-term interaction data. most session-based recommendation methods only model user's preferences with the current session sequence, which ignore rich information from a global perspective. meanwhile, previous works usually apply gnn to capture the transformation relationship between items, however the graph used in gnn is built through a static mode, which may introduce noise to the graph structure if user's preferences shift. in this paper, we propose a novel method called dynamic global structure enhanced multi-channel graph neural network (dgs-mgnn) to learn accurate representations of items from multiple perspectives. in dgs-mgnn, we propose a novel gnn model named multi-channel graph neural network to generate the local, global and consensus graphs dynamically and learn more informative representations of items based on the corresponding graph. meanwhile, in order to reduce the noise information within sessions, we utilize the graph structure to assist the attention mechanism to filter noisy information within each session, so as to generate an accurate intention representation for the user. finally, combined with a repeat and explore module, a more accurate prediction probability distribution is generated. we conduct extensive experiments on three widely used datasets, and the results demonstrate that dgs-mgnn is consistently superior to the state-of-the-art baseline models.""]"
4,16,4_domains_community_artificial intelligence_impact,"['domains', 'community', 'artificial intelligence', 'impact', 'framework']","[""artificial intelligence (ai) has emerged as a transformative technology with applications across multiple domains. the corpus of work related to the field of ai has grown significantly in volume as well as in terms of the application of ai in wider domains. however, given the wide application of ai in diverse areas, the measurement and characterization of the span of ai research is often a challenging task. bibliometrics is a well-established method in the scientific community to measure the patterns and impact of research. it however has also received significant criticism for its overemphasis on the macroscopic picture and the inability to provide a deep understanding of growth and thematic structure of knowledge-creation activities. therefore, this study presents a framework comprising of two techniques, namely, bradford's distribution and path analysis to characterize the growth and thematic evolution of the discipline. while the bradford distribution provides a macroscopic view of artificial intelligence research in terms of patterns of growth, the path analysis method presents a microscopic analysis of the thematic evolutionary trajectories, thereby completing the analytical framework. detailed insights into the evolution of each subdomain are drawn, major techniques employed in various ai applications are identified, and some relevant implications are discussed to demonstrate the usefulness of the analyses."", ""artificial intelligence (ai) has emerged as a transformative technology with applications across multiple domains. the corpus of work related to the field of ai has grown significantly in volume as well as in terms of the application of ai in wider domains. however, given the wide application of ai in diverse areas, the measurement and characterization of the span of ai research is often a challenging task. bibliometrics is a well-established method in the scientific community to measure the patterns and impact of research. it however has also received significant criticism for its overemphasis on the macroscopic picture and the inability to provide a deep understanding of growth and thematic structure of knowledge-creation activities. therefore, this study presents a framework comprising of two techniques, namely, bradford's distribution and path analysis to characterize the growth and thematic evolution of the discipline. while the bradford distribution provides a macroscopic view of artificial intelligence research in terms of patterns of growth, the path analysis method presents a microscopic analysis of the thematic evolutionary trajectories, thereby completing the analytical framework. detailed insights into the evolution of each subdomain are drawn, major techniques employed in various ai applications are identified, and some relevant implications are discussed to demonstrate the usefulness of the analyses."", ""artificial intelligence (ai) has emerged as a transformative technology with applications across multiple domains. the corpus of work related to the field of ai has grown significantly in volume as well as in terms of the application of ai in wider domains. however, given the wide application of ai in diverse areas, the measurement and characterization of the span of ai research is often a challenging task. bibliometrics is a well-established method in the scientific community to measure the patterns and impact of research. it however has also received significant criticism for its overemphasis on the macroscopic picture and the inability to provide a deep understanding of growth and thematic structure of knowledge-creation activities. therefore, this study presents a framework comprising of two techniques, namely, bradford's distribution and path analysis to characterize the growth and thematic evolution of the discipline. while the bradford distribution provides a macroscopic view of artificial intelligence research in terms of patterns of growth, the path analysis method presents a microscopic analysis of the thematic evolutionary trajectories, thereby completing the analytical framework. detailed insights into the evolution of each subdomain are drawn, major techniques employed in various ai applications are identified, and some relevant implications are discussed to demonstrate the usefulness of the analyses.""]"
5,16,5_relationships_performance_extraction_state art,"['relationships', 'performance', 'extraction', 'state art', 'graph']","[""relation extraction, is a pivotal task in nlp, impacts information retrieval, natural language understanding (nlu) and knowledge generation. machine learning model has coined itself as the most influential term in this era of deep learning and llm. in scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. in this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. we attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. we report our findings with four state of the art baseline models. the findings report here exemplary performance with luke model as winner. the presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement."", ""relation extraction, is a pivotal task in nlp, impacts information retrieval, natural language understanding (nlu) and knowledge generation. machine learning model has coined itself as the most influential term in this era of deep learning and llm. in scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. in this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. we attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. we report our findings with four state of the art baseline models. the findings report here exemplary performance with luke model as winner. the presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement."", ""relation extraction, is a pivotal task in nlp, impacts information retrieval, natural language understanding (nlu) and knowledge generation. machine learning model has coined itself as the most influential term in this era of deep learning and llm. in scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. in this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. we attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. we report our findings with four state of the art baseline models. the findings report here exemplary performance with luke model as winner. the presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.""]"
6,15,6_access_topics_search_artificial intelligence,"['access', 'topics', 'search', 'artificial intelligence', 'evidence']","['the 14th iteration of the bibliometric-enhanced information retrieval (bir) workshop series takes place at ecir 2024 as a full-day workshop. bir addresses research topics related to academic search and recommendation, at the intersection of information retrieval, natural language processing, and bibliometrics. as an interdisciplinary scientific event, bir brings together researchers and practitioners from the scientometrics/bibliometrics community on the one hand, and the information retrieval and nlp communities on the other hand. bir is an ever-growing topic investigated by both academia and the industry.', 'the first workshop on scholarly information access (scolia) will take place at ecir 2025 as a half-day workshop. the workshop is building upon and following up on the long series of the bibliometric-enhanced information retrieval (bir) workshops at ecir. scolia addresses research topics related to academic search and recommendation, at the intersection of information retrieval, natural language processing (including generative ai), and bibliometrics. as an interdisciplinary and intersectoral scientific event, addressing an ever-growing topic investigated by both academia and industry, scolia brings together researchers and practitioners from the aforementioned communities. the interactive format fosters engagement of all participants and fruitful discussions. the outcome of the workshop will reflect the current state and identify future research questions.', 'the first workshop on scholarly information access (scolia) will take place at ecir 2025 as a half-day workshop. the workshop is building upon and following up on the long series of the bibliometric-enhanced information retrieval (bir) workshops at ecir. scolia addresses research topics related to academic search and recommendation, at the intersection of information retrieval, natural language processing (including generative ai), and bibliometrics. as an interdisciplinary and intersectoral scientific event, addressing an ever-growing topic investigated by both academia and industry, scolia brings together researchers and practitioners from the aforementioned communities. the interactive format fosters engagement of all participants and fruitful discussions. the outcome of the workshop will reflect the current state and identify future research questions.']"
7,11,7_publications_framework_access_extraction,"['publications', 'framework', 'access', 'extraction', 'real world']","['rdf data is playing an important role in publishing and integrating heterogeneous data in data lakes. however, since the data is generally created utilizing liberal curation approaches such as crowd-sourcing and automatic extraction tools with no cross-validation on input data, the data is prone to errors that can be hidden in several dimensions. the types of errors which can be considered as outliers may occur in any part of rdf statements, especially in literal objects. although some scientific studies have revealed anomalies in knowledge graphs, none of the current approaches has the ability to explain the anomalies that have been discovered. in this paper, we present expad, a scalable and distributed framework for explainable numeric anomaly detection on very large rdf knowledge graphs. inspired by outliertree, expad generates human-readable explanations for a given numeric result being an outlier by following and assessing supervised decision tree splits. the proposed framework expad is open-source, well-documented, and fully integrated into the semantic analytics stack (sansa). experiments on real-world use cases and synthetic datasets disclose that the framework can not only handle high volumes of rdf data but also efficiently generate explanations for hidden anomalies discovered in kgs.', 'scientific publications serve as a source of disseminating information across research communities, often containing diverse data elements such as plain-text, tables, and figures. tables in particular offer a structured presentation of essential research data, enabling efficient information access. automatic extraction of tabular data alongside contextual information from scientific publications can significantly enhance research workflows and integrate more research data into scholarly research cycle, particularly supporting research data management (rdm). in marine geology, the researchers conduct expeditions at oceanographic locations and accumulate substantial amounts of valuable data such as sedimentation rate (sr), mass accumulation rate (mar) alongside relevant contextual information, often enriched with spatio-temporal context in tables of publications. these expeditions are costly and time intensive, emphasizing on the value of making such data more accessible and reusable. this paper introduces an end to end approach to extract and model heterogeneous tabular data from marine geology publications. our approach extracts metadata and tabular content from publications, modeling them into a heterogeneous information network (hin). the network uncovers hidden relationships and patterns across multiple documents, offering new insights and facilitating enhanced data referencing. experimental results and exploration on marine geology datasets demonstrate the effectiveness of our approach, showcasing its potential to support research data management and data driven scientific exploration.', 'scientific publications serve as a source of disseminating information across research communities, often containing diverse data elements such as plain-text, tables, and figures. tables in particular offer a structured presentation of essential research data, enabling efficient information access. automatic extraction of tabular data alongside contextual information from scientific publications can significantly enhance research workflows and integrate more research data into scholarly research cycle, particularly supporting research data management (rdm). in marine geology, the researchers conduct expeditions at oceanographic locations and accumulate substantial amounts of valuable data such as sedimentation rate (sr), mass accumulation rate (mar) alongside relevant contextual information, often enriched with spatio-temporal context in tables of publications. these expeditions are costly and time intensive, emphasizing on the value of making such data more accessible and reusable. this paper introduces an end to end approach to extract and model heterogeneous tabular data from marine geology publications. our approach extracts metadata and tabular content from publications, modeling them into a heterogeneous information network (hin). the network uncovers hidden relationships and patterns across multiple documents, offering new insights and facilitating enhanced data referencing. experimental results and exploration on marine geology datasets demonstrate the effectiveness of our approach, showcasing its potential to support research data management and data driven scientific exploration.']"
8,11,8_impact_corpus_training_introduces,"['impact', 'corpus', 'training', 'introduces', 'accuracy']","['evaluation of researchers’ output is vital for hiring committees and funding bodies, and it is usually measured via their scientific productivity, citations, or a combined metric such as the h-index. assessing young researchers is more critical because it takes a while to get citations and increment of h-index. hence, predicting the h-index can help to discover the researchers’ scientific impact. in addition, identifying the influential factors to predict the scientific impact is helpful for researchers and their organizations seeking solutions to improve it. this study investigates the effect of the author, paper/venue-specific features on the future h-index. for this purpose, we used a machine learning approach to predict the h-index and feature analysis techniques to advance the understanding of feature impact. utilizing the bibliometric data in scopus, we defined and extracted two main groups of features. the first relates to prior scientific impact, and we name it ‘prior impact-based features’ and includes the number of publications, received citations, and h-index. the second group is ‘non-prior impact-based features’ and contains the features related to author, co-authorship, paper, and venue characteristics. we explored their importance in predicting researchers’ h-index in three career phases. also, we examined the temporal dimension of predicting performance for different feature categories to find out which features are more reliable for long- and short-term prediction. we referred to the gender of the authors to examine the role of this author’s characteristics in the prediction task. our findings showed that gender has a very slight effect in predicting the h-index. although the results demonstrate better performance for the models containing prior impact-based features for all researchers’ groups in the near future, we found that non-prior impact-based features are more robust predictors for younger scholars in the long term. also, prior impact-based features lose their power to predict more than other features in the long term.', 'evaluation of researchers’ output is vital for hiring committees and funding bodies, and it is usually measured via their scientific productivity, citations, or a combined metric such as the h-index. assessing young researchers is more critical because it takes a while to get citations and increment of h-index. hence, predicting the h-index can help to discover the researchers’ scientific impact. in addition, identifying the influential factors to predict the scientific impact is helpful for researchers and their organizations seeking solutions to improve it. this study investigates the effect of the author, paper/venue-specific features on the future h-index. for this purpose, we used a machine learning approach to predict the h-index and feature analysis techniques to advance the understanding of feature impact. utilizing the bibliometric data in scopus, we defined and extracted two main groups of features. the first relates to prior scientific impact, and we name it ‘prior impact-based features’ and includes the number of publications, received citations, and h-index. the second group is ‘non-prior impact-based features’ and contains the features related to author, co-authorship, paper, and venue characteristics. we explored their importance in predicting researchers’ h-index in three career phases. also, we examined the temporal dimension of predicting performance for different feature categories to find out which features are more reliable for long- and short-term prediction. we referred to the gender of the authors to examine the role of this author’s characteristics in the prediction task. our findings showed that gender has a very slight effect in predicting the h-index. although the results demonstrate better performance for the models containing prior impact-based features for all researchers’ groups in the near future, we found that non-prior impact-based features are more robust predictors for younger scholars in the long term. also, prior impact-based features lose their power to predict more than other features in the long term.', 'evaluation of researchers’ output is vital for hiring committees and funding bodies, and it is usually measured via their scientific productivity, citations, or a combined metric such as the h-index. assessing young researchers is more critical because it takes a while to get citations and increment of h-index. hence, predicting the h-index can help to discover the researchers’ scientific impact. in addition, identifying the influential factors to predict the scientific impact is helpful for researchers and their organizations seeking solutions to improve it. this study investigates the effect of the author, paper/venue-specific features on the future h-index. for this purpose, we used a machine learning approach to predict the h-index and feature analysis techniques to advance the understanding of feature impact. utilizing the bibliometric data in scopus, we defined and extracted two main groups of features. the first relates to prior scientific impact, and we name it ‘prior impact-based features’ and includes the number of publications, received citations, and h-index. the second group is ‘non-prior impact-based features’ and contains the features related to author, co-authorship, paper, and venue characteristics. we explored their importance in predicting researchers’ h-index in three career phases. also, we examined the temporal dimension of predicting performance for different feature categories to find out which features are more reliable for long- and short-term prediction. we referred to the gender of the authors to examine the role of this author’s characteristics in the prediction task. our findings showed that gender has a very slight effect in predicting the h-index. although the results demonstrate better performance for the models containing prior impact-based features for all researchers’ groups in the near future, we found that non-prior impact-based features are more robust predictors for younger scholars in the long term. also, prior impact-based features lose their power to predict more than other features in the long term.']"
9,11,9_search_reproducibility_topics_real world,"['search', 'reproducibility', 'topics', 'real world', 'score']","['human feedback is often used, either directly or indirectly, as input to algorithmic decision making. however, humans are biased: if the algorithm that takes as input the human feedback does not control for potential biases, this might result in biased algorithmic decision making, which can have a tangible impact on people’s lives. in this paper, we study how to detect and correct for evaluators’ bias in the task of ranking people (or items) from pairwise comparisons. specifically, we assume we are given pairwise comparisons of the items to be ranked produced by a set of evaluators. while the pairwise assessments of the evaluators should reflect to a certain extent the latent (unobservable) true quality scores of the items, they might be affected by each evaluator’s own bias against, or in favor, of some groups of items. by detecting and amending evaluators’ biases, we aim to produce a ranking of the items that is, as much as possible, in accordance with the ranking one would produce by having access to the latent quality scores. our proposal is a novel method that extends the classic bradley-terry model by having a bias parameter for each evaluator which distorts the true quality score of each item, depending on the group the item belongs to. thanks to the simplicity of the model, we are able to write explicitly its log-likelihood w.r.t. the parameters (i.e., items’ latent scores and evaluators’ bias) and optimize by means of the alternating approach. our experiments on synthetic and real-world data confirm that our method is able to reconstruct the bias of each single evaluator extremely well and thus to outperform several non-trivial competitors in the task of producing a ranking which is as much as possible close to the unbiased ranking.', 'human feedback is often used, either directly or indirectly, as input to algorithmic decision making. however, humans are biased: if the algorithm that takes as input the human feedback does not control for potential biases, this might result in biased algorithmic decision making, which can have a tangible impact on people’s lives. in this paper, we study how to detect and correct for evaluators’ bias in the task of ranking people (or items) from pairwise comparisons. specifically, we assume we are given pairwise comparisons of the items to be ranked produced by a set of evaluators. while the pairwise assessments of the evaluators should reflect to a certain extent the latent (unobservable) true quality scores of the items, they might be affected by each evaluator’s own bias against, or in favor, of some groups of items. by detecting and amending evaluators’ biases, we aim to produce a ranking of the items that is, as much as possible, in accordance with the ranking one would produce by having access to the latent quality scores. our proposal is a novel method that extends the classic bradley-terry model by having a bias parameter for each evaluator which distorts the true quality score of each item, depending on the group the item belongs to. thanks to the simplicity of the model, we are able to write explicitly its log-likelihood w.r.t. the parameters (i.e., items’ latent scores and evaluators’ bias) and optimize by means of the alternating approach. our experiments on synthetic and real-world data confirm that our method is able to reconstruct the bias of each single evaluator extremely well and thus to outperform several non-trivial competitors in the task of producing a ranking which is as much as possible close to the unbiased ranking.', 'human feedback is often used, either directly or indirectly, as input to algorithmic decision making. however, humans are biased: if the algorithm that takes as input the human feedback does not control for potential biases, this might result in biased algorithmic decision making, which can have a tangible impact on people’s lives. in this paper, we study how to detect and correct for evaluators’ bias in the task of ranking people (or items) from pairwise comparisons. specifically, we assume we are given pairwise comparisons of the items to be ranked produced by a set of evaluators. while the pairwise assessments of the evaluators should reflect to a certain extent the latent (unobservable) true quality scores of the items, they might be affected by each evaluator’s own bias against, or in favor, of some groups of items. by detecting and amending evaluators’ biases, we aim to produce a ranking of the items that is, as much as possible, in accordance with the ranking one would produce by having access to the latent quality scores. our proposal is a novel method that extends the classic bradley-terry model by having a bias parameter for each evaluator which distorts the true quality score of each item, depending on the group the item belongs to. thanks to the simplicity of the model, we are able to write explicitly its log-likelihood w.r.t. the parameters (i.e., items’ latent scores and evaluators’ bias) and optimize by means of the alternating approach. our experiments on synthetic and real-world data confirm that our method is able to reconstruct the bias of each single evaluator extremely well and thus to outperform several non-trivial competitors in the task of producing a ranking which is as much as possible close to the unbiased ranking.']"
