Topic,Count,Name,Representation,Representative_Docs
-1,87,-1_retrieval_graph_tasks_artificial intelligence,"['retrieval', 'graph', 'tasks', 'artificial intelligence', 'llms']","[""tracing architecture of machine learning models through their mentions in scholarly articles {relation extraction, is a pivotal task in nlp, impacts information retrieval, natural language understanding (nlu) and knowledge generation. machine learning model has coined itself as the most influential term in this era of deep learning and llm. in scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. in this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. we attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. we report our findings with four state of the art baseline models. the findings report here exemplary performance with luke model as winner. the presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.} {baseline,extraction,machine learning model,machine learning model architecture,relation} {baseline,extraction,machine learning model,machine learning model architecture,relation}"", ""tracing architecture of machine learning models through their mentions in scholarly articles {relation extraction, is a pivotal task in nlp, impacts information retrieval, natural language understanding (nlu) and knowledge generation. machine learning model has coined itself as the most influential term in this era of deep learning and llm. in scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. in this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. we attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. we report our findings with four state of the art baseline models. the findings report here exemplary performance with luke model as winner. the presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.} {baseline,extraction,machine learning model,machine learning model architecture,relation} {baseline,extraction,machine learning model,machine learning model architecture,relation}"", ""tracing architecture of machine learning models through their mentions in scholarly articles {relation extraction, is a pivotal task in nlp, impacts information retrieval, natural language understanding (nlu) and knowledge generation. machine learning model has coined itself as the most influential term in this era of deep learning and llm. in scientific text how machine learning models relate with other key entities, holds always a quintessentially interesting topic. knowing the origins of machine learning model in terms of their architecture open a crucial tunnel of understanding towards its characteristics. in this paper we experiment on tracing the machine learning model architecture of the machine learning models from their mentions in scholarly texts. we attack this problem in supervised approach; first we identify multiple machine learning model oriented entities present in a sentence and then figure out if each of such entities are based on another such entity through binary ('based on' and other relation) classification task. we report our findings with four state of the art baseline models. the findings report here exemplary performance with luke model as winner. the presence of 'based on' relation has quite low evidence support, which effected the performance result of the models, which inspire for further explorations for improvement.} {baseline,extraction,machine learning model,machine learning model architecture,relation} {baseline,extraction,machine learning model,machine learning model architecture,relation}""]"
0,40,0_tasks_community_impact_retrieval,"['tasks', 'community', 'impact', 'retrieval', 'social media']","['shall androids dream of genocides? how generative ai can change the future of memorialization of mass atrocities {the memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. at the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. the emergence of generative forms of artificial intelligence (ai), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. ai can identify patterns in training data to create new narratives for representing and interpreting mass atrocities—and do so in a fraction of the time it takes for humans. the use of generative ai in this context raises numerous questions: for example, can the paucity of training data on mass atrocities distort how ai interprets some atrocity-related inquiries? how important is the ability to differentiate between human- and ai-made content concerning mass atrocities? can ai-made content be used to promote false information concerning atrocities? this article addresses these and other questions by examining the opportunities and risks associated with using generative ais for memorializing mass atrocities. it also discusses recommendations for ais integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.}', 'shall androids dream of genocides? how generative ai can change the future of memorialization of mass atrocities {the memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. at the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. the emergence of generative forms of artificial intelligence (ai), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. ai can identify patterns in training data to create new narratives for representing and interpreting mass atrocities—and do so in a fraction of the time it takes for humans. the use of generative ai in this context raises numerous questions: for example, can the paucity of training data on mass atrocities distort how ai interprets some atrocity-related inquiries? how important is the ability to differentiate between human- and ai-made content concerning mass atrocities? can ai-made content be used to promote false information concerning atrocities? this article addresses these and other questions by examining the opportunities and risks associated with using generative ais for memorializing mass atrocities. it also discusses recommendations for ais integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.}', 'shall androids dream of genocides? how generative ai can change the future of memorialization of mass atrocities {the memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. at the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. the emergence of generative forms of artificial intelligence (ai), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. ai can identify patterns in training data to create new narratives for representing and interpreting mass atrocities—and do so in a fraction of the time it takes for humans. the use of generative ai in this context raises numerous questions: for example, can the paucity of training data on mass atrocities distort how ai interprets some atrocity-related inquiries? how important is the ability to differentiate between human- and ai-made content concerning mass atrocities? can ai-made content be used to promote false information concerning atrocities? this article addresses these and other questions by examining the opportunities and risks associated with using generative ais for memorializing mass atrocities. it also discusses recommendations for ais integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.}']"
1,37,1_detection_knowledge graphs_domain_social media,"['detection', 'knowledge graphs', 'domain', 'social media', 'pipeline']","['anomaly detection for numerical literals in knowledge graphs: a short review of approaches {anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. however, within the field of semantic web and knowledge graphs, anomaly detection has been relatively overlooked. additionally, the existing literature on anomaly detection over knowledge graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. in light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over knowledge graphs. in this overview, we review the quality metrics of kgs and discuss the possible errors which may occur in different parts of the rdf data. additionally, we outline a generic conceptual framework for the execution pipeline of anomaly detection over kgs. moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for kgs.} {anomaly detection,knowledge graphs,linked open data,outlier detection,rdf data,semantic web} {anomaly detection,knowledge graphs,linked open data,outlier detection,rdf data,semantic web}', 'anomaly detection for numerical literals in knowledge graphs: a short review of approaches {anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. however, within the field of semantic web and knowledge graphs, anomaly detection has been relatively overlooked. additionally, the existing literature on anomaly detection over knowledge graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. in light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over knowledge graphs. in this overview, we review the quality metrics of kgs and discuss the possible errors which may occur in different parts of the rdf data. additionally, we outline a generic conceptual framework for the execution pipeline of anomaly detection over kgs. moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for kgs.} {anomaly detection,knowledge graphs,linked open data,outlier detection,rdf data,semantic web} {anomaly detection,knowledge graphs,linked open data,outlier detection,rdf data,semantic web}', 'anomaly detection for numerical literals in knowledge graphs: a short review of approaches {anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. however, within the field of semantic web and knowledge graphs, anomaly detection has been relatively overlooked. additionally, the existing literature on anomaly detection over knowledge graphs lacks proper organization and poses challenges for new researchers seeking a comprehensive understanding. in light of these gaps, this paper aims to offer a well-structured and comprehensive overview of the existing research conducted on anomaly detection over knowledge graphs. in this overview, we review the quality metrics of kgs and discuss the possible errors which may occur in different parts of the rdf data. additionally, we outline a generic conceptual framework for the execution pipeline of anomaly detection over kgs. moreover, we study the anomaly detection techniques, along with their variants, and present key assumptions, to differentiate between normal and anomalous behavior. finally, we outline open issues in research and challenges encountered while adopting anomaly detection techniques for kgs.} {anomaly detection,knowledge graphs,linked open data,outlier detection,rdf data,semantic web} {anomaly detection,knowledge graphs,linked open data,outlier detection,rdf data,semantic web}']"
2,20,2_vocabulary_accuracy_automated_training,"['vocabulary', 'accuracy', 'automated', 'training', 'llms']","['out-of-vocabulary pashto spell checker using morphological operations {a spell checking model detects spelling errors in the input text, generates possible corrections for each error, and organizes them based on their relevance. such tools are essential for writing, editing, and publishing in a language. in literature, different variants of edit distance and language models are used for dealing with spelling errors. the existing approaches have improved for efficient use of computational and memory resources without compromising accuracy. exploiting the efforts made for other low-resource languages, we have proposed the first spell checking model for pashto language. a common issue with the existing spell checker models of low-resourced languages is having many false positives due to limited vocabulary. our proposed approach makes use of morphological operations to generate out-of-vocabulary words and is coupled with a filtering mechanism to retain the words with more repeating syllable patterns only. the n-gram probabilities are combined through linear interpolation to detect and correct spelling errors. the test samples with spelling errors are generated by randomizing characters for non-word errors and words in higher-order n-gram for real word errors. the proposed approach achieves detection accuracy of 85.5% and correction accuracy of 75.0%, respectively.} {edit distance,language models,linear interpolation,spell checker} {edit distance,language models,linear interpolation,spell checker}', 'out-of-vocabulary pashto spell checker using morphological operations {a spell checking model detects spelling errors in the input text, generates possible corrections for each error, and organizes them based on their relevance. such tools are essential for writing, editing, and publishing in a language. in literature, different variants of edit distance and language models are used for dealing with spelling errors. the existing approaches have improved for efficient use of computational and memory resources without compromising accuracy. exploiting the efforts made for other low-resource languages, we have proposed the first spell checking model for pashto language. a common issue with the existing spell checker models of low-resourced languages is having many false positives due to limited vocabulary. our proposed approach makes use of morphological operations to generate out-of-vocabulary words and is coupled with a filtering mechanism to retain the words with more repeating syllable patterns only. the n-gram probabilities are combined through linear interpolation to detect and correct spelling errors. the test samples with spelling errors are generated by randomizing characters for non-word errors and words in higher-order n-gram for real word errors. the proposed approach achieves detection accuracy of 85.5% and correction accuracy of 75.0%, respectively.} {edit distance,language models,linear interpolation,spell checker} {edit distance,language models,linear interpolation,spell checker}', 'out-of-vocabulary pashto spell checker using morphological operations {a spell checking model detects spelling errors in the input text, generates possible corrections for each error, and organizes them based on their relevance. such tools are essential for writing, editing, and publishing in a language. in literature, different variants of edit distance and language models are used for dealing with spelling errors. the existing approaches have improved for efficient use of computational and memory resources without compromising accuracy. exploiting the efforts made for other low-resource languages, we have proposed the first spell checking model for pashto language. a common issue with the existing spell checker models of low-resourced languages is having many false positives due to limited vocabulary. our proposed approach makes use of morphological operations to generate out-of-vocabulary words and is coupled with a filtering mechanism to retain the words with more repeating syllable patterns only. the n-gram probabilities are combined through linear interpolation to detect and correct spelling errors. the test samples with spelling errors are generated by randomizing characters for non-word errors and words in higher-order n-gram for real word errors. the proposed approach achieves detection accuracy of 85.5% and correction accuracy of 75.0%, respectively.} {edit distance,language models,linear interpolation,spell checker} {edit distance,language models,linear interpolation,spell checker}']"
3,19,3_domain_dependency_state art_words,"['domain', 'dependency', 'state art', 'words', 'graphs']","['exploring rich structure information for aspect-based sentiment classification {graph convolutional network (gcn) for aspect-based sentiment classification has attracted a lot of attention recently due to their promising performance in handling complex structure information. however, previous methods based on gcn focused mainly on examining the structure of syntactic dependency relationships, which were subject to the noise and sparsity problem. furthermore, these methods tend to focus on one kind of structural information (namely syntactic dependency) while ignoring many other kinds of rich structures between words. to tackle these problems, we propose a novel gcn based model, named structure-enhanced dual-channel graph convolutional network (sedc-gcn). specifically, we first exploit the rich structure information by constructing a text sequence graph and an enhanced dependency graph, then design a dual-channel graph encoder to model the structure information from the two graphs. after that, we propose two kinds of aspect-specific attention, i.e., aspect-specific semantic attention and aspect-specific structure attention, to learn sentence representation from two different perspectives, i.e., the semantic perspective based on the text encoder, and the structure perspective based on the dual-channel graph encoder. finally, we merge the sentence representations from the above two perspectives and obtain the final sentence representation. we experimentally validate our proposed model sedc-gcn by comparing with seven strong baseline methods. in terms of the metric accuracy, sedc-gcn achieves performance gains of 74.42%, 77.74%, 83.30%, 81.73% and 90.75% on twitter, laptop, rest14, rest15, and rest16, respectively, which are 0.35%, 4.22%, 1.62%, 0.70% and 2.01% better than the best performing baseline bigcn. similar performance improvements are also observed in terms of the metric macro-averaged f1 score. the ablation study further demonstrates the effectiveness of each component of sedc-gcn.} {aspect-based sentiment classification,attention mechanism,graph convolutional networks,sentiment analysis} {aspect-based sentiment classification,attention mechanism,graph convolutional networks,sentiment analysis}', 'exploring rich structure information for aspect-based sentiment classification {graph convolutional network (gcn) for aspect-based sentiment classification has attracted a lot of attention recently due to their promising performance in handling complex structure information. however, previous methods based on gcn focused mainly on examining the structure of syntactic dependency relationships, which were subject to the noise and sparsity problem. furthermore, these methods tend to focus on one kind of structural information (namely syntactic dependency) while ignoring many other kinds of rich structures between words. to tackle these problems, we propose a novel gcn based model, named structure-enhanced dual-channel graph convolutional network (sedc-gcn). specifically, we first exploit the rich structure information by constructing a text sequence graph and an enhanced dependency graph, then design a dual-channel graph encoder to model the structure information from the two graphs. after that, we propose two kinds of aspect-specific attention, i.e., aspect-specific semantic attention and aspect-specific structure attention, to learn sentence representation from two different perspectives, i.e., the semantic perspective based on the text encoder, and the structure perspective based on the dual-channel graph encoder. finally, we merge the sentence representations from the above two perspectives and obtain the final sentence representation. we experimentally validate our proposed model sedc-gcn by comparing with seven strong baseline methods. in terms of the metric accuracy, sedc-gcn achieves performance gains of 74.42%, 77.74%, 83.30%, 81.73% and 90.75% on twitter, laptop, rest14, rest15, and rest16, respectively, which are 0.35%, 4.22%, 1.62%, 0.70% and 2.01% better than the best performing baseline bigcn. similar performance improvements are also observed in terms of the metric macro-averaged f1 score. the ablation study further demonstrates the effectiveness of each component of sedc-gcn.} {aspect-based sentiment classification,attention mechanism,graph convolutional networks,sentiment analysis} {aspect-based sentiment classification,attention mechanism,graph convolutional networks,sentiment analysis}', 'exploring rich structure information for aspect-based sentiment classification {graph convolutional network (gcn) for aspect-based sentiment classification has attracted a lot of attention recently due to their promising performance in handling complex structure information. however, previous methods based on gcn focused mainly on examining the structure of syntactic dependency relationships, which were subject to the noise and sparsity problem. furthermore, these methods tend to focus on one kind of structural information (namely syntactic dependency) while ignoring many other kinds of rich structures between words. to tackle these problems, we propose a novel gcn based model, named structure-enhanced dual-channel graph convolutional network (sedc-gcn). specifically, we first exploit the rich structure information by constructing a text sequence graph and an enhanced dependency graph, then design a dual-channel graph encoder to model the structure information from the two graphs. after that, we propose two kinds of aspect-specific attention, i.e., aspect-specific semantic attention and aspect-specific structure attention, to learn sentence representation from two different perspectives, i.e., the semantic perspective based on the text encoder, and the structure perspective based on the dual-channel graph encoder. finally, we merge the sentence representations from the above two perspectives and obtain the final sentence representation. we experimentally validate our proposed model sedc-gcn by comparing with seven strong baseline methods. in terms of the metric accuracy, sedc-gcn achieves performance gains of 74.42%, 77.74%, 83.30%, 81.73% and 90.75% on twitter, laptop, rest14, rest15, and rest16, respectively, which are 0.35%, 4.22%, 1.62%, 0.70% and 2.01% better than the best performing baseline bigcn. similar performance improvements are also observed in terms of the metric macro-averaged f1 score. the ablation study further demonstrates the effectiveness of each component of sedc-gcn.} {aspect-based sentiment classification,attention mechanism,graph convolutional networks,sentiment analysis} {aspect-based sentiment classification,attention mechanism,graph convolutional networks,sentiment analysis}']"
4,18,4_retrieval_impact_trained_context,"['retrieval', 'impact', 'trained', 'context', 'large language llms']","['dissecting paraphrases: the impact of prompt syntax and supplementary information on knowledge retrieval from pretrained language models {pre-trained language models (plms) are known to contain various kinds of knowledge. one method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. we designed conparelama – a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. these paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. conparelama enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of plms. extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying plms with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. in addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.}', 'decoding prompt syntax: analysing its impact on knowledge retrieval in large language models {large language models (llms), with their advanced architectures and training on massive language datasets, contain unexplored knowledge. one method to infer this knowledge is through the use of cloze-style prompts. typically, these prompts are manually designed because the phrasing of these prompts impacts the knowledge retrieval performance, even if the llm encodes the desired information. in this paper, we study the impact of prompt syntax on the knowledge retrieval capacity of llms. we use a template-based approach to paraphrase simple prompts into prompts with a more complex grammatical structure. we then analyse the llm performance for these structurally different but semantically equivalent prompts. our study reveals that simple prompts work better than complex forms of sentences. the performance across the syntactical variations for simple relations (1:1) remains best, with a marginal decrease across different typologies. these results reinforce that simple prompt structures are more effective for knowledge retrieval in llms and motivate future research into the impact of prompt syntax on various tasks.} {bert,knowledge retrieval,large language models,syntax aware prompt} {bert,knowledge retrieval,large language models,syntax aware prompt}', 'decoding prompt syntax: analysing its impact on knowledge retrieval in large language models {large language models (llms), with their advanced architectures and training on massive language datasets, contain unexplored knowledge. one method to infer this knowledge is through the use of cloze-style prompts. typically, these prompts are manually designed because the phrasing of these prompts impacts the knowledge retrieval performance, even if the llm encodes the desired information. in this paper, we study the impact of prompt syntax on the knowledge retrieval capacity of llms. we use a template-based approach to paraphrase simple prompts into prompts with a more complex grammatical structure. we then analyse the llm performance for these structurally different but semantically equivalent prompts. our study reveals that simple prompts work better than complex forms of sentences. the performance across the syntactical variations for simple relations (1:1) remains best, with a marginal decrease across different typologies. these results reinforce that simple prompt structures are more effective for knowledge retrieval in llms and motivate future research into the impact of prompt syntax on various tasks.} {bert,knowledge retrieval,large language models,syntax aware prompt} {bert,knowledge retrieval,large language models,syntax aware prompt}']"
5,16,5_training_f1 score_large language_llms,"['training', 'f1 score', 'large language', 'llms', 'retrieval']","[""question answering versus named entity recognition for extracting unknown datasets {dataset mention extraction is a difficult problem due to the unstructured nature of text, the sparsity of dataset mentions, and the various ways the same dataset can be mentioned. extracting unknown dataset mentions which are not part of the training data of the model is even harder. we address this challenge in two ways. first, we consider a two-step approach where a binary classifier filters out positive contexts, i.e., detects sentences with a dataset mention. we consider multiple transformer-based models and strong baselines for this task. subsequently, the dataset is extracted from the positive context. second, we consider a one-step approach and directly aim to detect and extract a possible dataset mention. for the extraction of datasets, we consider transformer models in named entity recognition (ner) mode. we contrast ner with the transformers' capabilities for question answering (qa). we use the coleridge initiative 'show us the data' dataset consisting of 14.3k scientific papers with about 35k mentions of datasets. we found that using transformers in qa mode is a better choice than ner for extracting unknown datasets. the rationale is that detecting new datasets is an out-of-vocabulary task, i.e., the dataset name has not been seen once during training. comparing the two-step versus the one-step approach, we found contrasting strengths. a two-step dataset extraction using an mlp for filtering and roberta in qa mode extracts more dataset mentions than a one-step system, but at the cost of a lower f1-score of 62.7%. a one-step extraction with deberta in qa achieves the highest f1-score of 92.88% at the cost of missing dataset mentions. we recommend the one-step approach for the case when accuracy is more important, and the two-step approach when there is a postprocessing mechanism for the extracted dataset mentions, e.g., a manual check. the source code is available at https://github.com/yousef-younes/dataset-mention-extraction.} {binary text classification,dataset mentions,named entity recognition,question answering} {binary text classification,dataset mentions,named entity recognition,question answering}"", ""question answering versus named entity recognition for extracting unknown datasets {dataset mention extraction is a difficult problem due to the unstructured nature of text, the sparsity of dataset mentions, and the various ways the same dataset can be mentioned. extracting unknown dataset mentions which are not part of the training data of the model is even harder. we address this challenge in two ways. first, we consider a two-step approach where a binary classifier filters out positive contexts, i.e., detects sentences with a dataset mention. we consider multiple transformer-based models and strong baselines for this task. subsequently, the dataset is extracted from the positive context. second, we consider a one-step approach and directly aim to detect and extract a possible dataset mention. for the extraction of datasets, we consider transformer models in named entity recognition (ner) mode. we contrast ner with the transformers' capabilities for question answering (qa). we use the coleridge initiative 'show us the data' dataset consisting of 14.3k scientific papers with about 35k mentions of datasets. we found that using transformers in qa mode is a better choice than ner for extracting unknown datasets. the rationale is that detecting new datasets is an out-of-vocabulary task, i.e., the dataset name has not been seen once during training. comparing the two-step versus the one-step approach, we found contrasting strengths. a two-step dataset extraction using an mlp for filtering and roberta in qa mode extracts more dataset mentions than a one-step system, but at the cost of a lower f1-score of 62.7%. a one-step extraction with deberta in qa achieves the highest f1-score of 92.88% at the cost of missing dataset mentions. we recommend the one-step approach for the case when accuracy is more important, and the two-step approach when there is a postprocessing mechanism for the extracted dataset mentions, e.g., a manual check. the source code is available at https://github.com/yousef-younes/dataset-mention-extraction.} {binary text classification,dataset mentions,named entity recognition,question answering} {binary text classification,dataset mentions,named entity recognition,question answering}"", ""question answering versus named entity recognition for extracting unknown datasets {dataset mention extraction is a difficult problem due to the unstructured nature of text, the sparsity of dataset mentions, and the various ways the same dataset can be mentioned. extracting unknown dataset mentions which are not part of the training data of the model is even harder. we address this challenge in two ways. first, we consider a two-step approach where a binary classifier filters out positive contexts, i.e., detects sentences with a dataset mention. we consider multiple transformer-based models and strong baselines for this task. subsequently, the dataset is extracted from the positive context. second, we consider a one-step approach and directly aim to detect and extract a possible dataset mention. for the extraction of datasets, we consider transformer models in named entity recognition (ner) mode. we contrast ner with the transformers' capabilities for question answering (qa). we use the coleridge initiative 'show us the data' dataset consisting of 14.3k scientific papers with about 35k mentions of datasets. we found that using transformers in qa mode is a better choice than ner for extracting unknown datasets. the rationale is that detecting new datasets is an out-of-vocabulary task, i.e., the dataset name has not been seen once during training. comparing the two-step versus the one-step approach, we found contrasting strengths. a two-step dataset extraction using an mlp for filtering and roberta in qa mode extracts more dataset mentions than a one-step system, but at the cost of a lower f1-score of 62.7%. a one-step extraction with deberta in qa achieves the highest f1-score of 92.88% at the cost of missing dataset mentions. we recommend the one-step approach for the case when accuracy is more important, and the two-step approach when there is a postprocessing mechanism for the extracted dataset mentions, e.g., a manual check. the source code is available at https://github.com/yousef-younes/dataset-mention-extraction.} {binary text classification,dataset mentions,named entity recognition,question answering} {binary text classification,dataset mentions,named entity recognition,question answering}""]"
6,13,6_impact_reference_sentence_account,"['impact', 'reference', 'sentence', 'account', 'community']","['investigating the contribution of author- and publication-specific features to scholars’ h-index prediction {evaluation of researchers’ output is vital for hiring committees and funding bodies, and it is usually measured via their scientific productivity, citations, or a combined metric such as the h-index. assessing young researchers is more critical because it takes a while to get citations and increment of h-index. hence, predicting the h-index can help to discover the researchers’ scientific impact. in addition, identifying the influential factors to predict the scientific impact is helpful for researchers and their organizations seeking solutions to improve it. this study investigates the effect of the author, paper/venue-specific features on the future h-index. for this purpose, we used a machine learning approach to predict the h-index and feature analysis techniques to advance the understanding of feature impact. utilizing the bibliometric data in scopus, we defined and extracted two main groups of features. the first relates to prior scientific impact, and we name it ‘prior impact-based features’ and includes the number of publications, received citations, and h-index. the second group is ‘non-prior impact-based features’ and contains the features related to author, co-authorship, paper, and venue characteristics. we explored their importance in predicting researchers’ h-index in three career phases. also, we examined the temporal dimension of predicting performance for different feature categories to find out which features are more reliable for long- and short-term prediction. we referred to the gender of the authors to examine the role of this author’s characteristics in the prediction task. our findings showed that gender has a very slight effect in predicting the h-index. although the results demonstrate better performance for the models containing prior impact-based features for all researchers’ groups in the near future, we found that non-prior impact-based features are more robust predictors for younger scholars in the long term. also, prior impact-based features lose their power to predict more than other features in the long term.} {academic mobility,feature importance,h-index prediction,machine learning,open access publishing} {academic mobility,feature importance,h-index prediction,machine learning,open access publishing}', 'investigating the contribution of author- and publication-specific features to scholars’ h-index prediction {evaluation of researchers’ output is vital for hiring committees and funding bodies, and it is usually measured via their scientific productivity, citations, or a combined metric such as the h-index. assessing young researchers is more critical because it takes a while to get citations and increment of h-index. hence, predicting the h-index can help to discover the researchers’ scientific impact. in addition, identifying the influential factors to predict the scientific impact is helpful for researchers and their organizations seeking solutions to improve it. this study investigates the effect of the author, paper/venue-specific features on the future h-index. for this purpose, we used a machine learning approach to predict the h-index and feature analysis techniques to advance the understanding of feature impact. utilizing the bibliometric data in scopus, we defined and extracted two main groups of features. the first relates to prior scientific impact, and we name it ‘prior impact-based features’ and includes the number of publications, received citations, and h-index. the second group is ‘non-prior impact-based features’ and contains the features related to author, co-authorship, paper, and venue characteristics. we explored their importance in predicting researchers’ h-index in three career phases. also, we examined the temporal dimension of predicting performance for different feature categories to find out which features are more reliable for long- and short-term prediction. we referred to the gender of the authors to examine the role of this author’s characteristics in the prediction task. our findings showed that gender has a very slight effect in predicting the h-index. although the results demonstrate better performance for the models containing prior impact-based features for all researchers’ groups in the near future, we found that non-prior impact-based features are more robust predictors for younger scholars in the long term. also, prior impact-based features lose their power to predict more than other features in the long term.} {academic mobility,feature importance,h-index prediction,machine learning,open access publishing} {academic mobility,feature importance,h-index prediction,machine learning,open access publishing}', 'investigating the contribution of author- and publication-specific features to scholars’ h-index prediction {evaluation of researchers’ output is vital for hiring committees and funding bodies, and it is usually measured via their scientific productivity, citations, or a combined metric such as the h-index. assessing young researchers is more critical because it takes a while to get citations and increment of h-index. hence, predicting the h-index can help to discover the researchers’ scientific impact. in addition, identifying the influential factors to predict the scientific impact is helpful for researchers and their organizations seeking solutions to improve it. this study investigates the effect of the author, paper/venue-specific features on the future h-index. for this purpose, we used a machine learning approach to predict the h-index and feature analysis techniques to advance the understanding of feature impact. utilizing the bibliometric data in scopus, we defined and extracted two main groups of features. the first relates to prior scientific impact, and we name it ‘prior impact-based features’ and includes the number of publications, received citations, and h-index. the second group is ‘non-prior impact-based features’ and contains the features related to author, co-authorship, paper, and venue characteristics. we explored their importance in predicting researchers’ h-index in three career phases. also, we examined the temporal dimension of predicting performance for different feature categories to find out which features are more reliable for long- and short-term prediction. we referred to the gender of the authors to examine the role of this author’s characteristics in the prediction task. our findings showed that gender has a very slight effect in predicting the h-index. although the results demonstrate better performance for the models containing prior impact-based features for all researchers’ groups in the near future, we found that non-prior impact-based features are more robust predictors for younger scholars in the long term. also, prior impact-based features lose their power to predict more than other features in the long term.} {academic mobility,feature importance,h-index prediction,machine learning,open access publishing} {academic mobility,feature importance,h-index prediction,machine learning,open access publishing}']"
